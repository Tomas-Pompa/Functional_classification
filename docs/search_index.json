[["index.html", "Support Vector Machine Classification of Functional Data with Random Vertical Shifts SVM for Functional Data", " Support Vector Machine Classification of Functional Data with Random Vertical Shifts Tomáš Pompa 2025-03-23 SVM for Functional Data This document provides supplementary material for the article Support Vector Machine Classification of Functional Data with Random Vertical Shifts. It includes additional details, supporting analyses, and further explanations that complement the main findings presented in the paper. The goal will be to apply knowledge of the Support Vector Machine (SVM) method for multivariate data to functional-type data, that is, infinite-dimensional objects. To do this, we will use both a transformation (reduction) of objects from infinite to finite dimension, followed by the use of established procedures, as well as a modification of SVM specifically for functional data, leveraging knowledge of Hilbert spaces and inner products. Another objective will be to compare different methods for classifying functional data on real and simulated datasets. It would be beneficial to design an interesting simulation study that demonstrates the various behaviors of the considered methods. The classification methods considered include: \\(K\\) nearest neighbors (KNN), logistic regression (both standard (LR) and its functional modification (LR_fda)), linear (LDA) and quadratic (QDA) discriminant analysis, decision trees (DT), random forests (RF), and Support Vector Machines. We will go through each method, starting with simulated data, and then move on to constructing a support vector machine method for functional data (SVM_fda). The primary package in R for working with functional objects is fda. Other useful packages include MASS, e1071, fda.usc, refund, and more. "],["simulace3.html", "Chapter 1 Classification of simulated curves 1.1 Simulation of Functional Data 1.2 Smoothing Observed Curves 1.3 Classification of Curves 1.4 Simulation Study", " Chapter 1 Classification of simulated curves 1.1 Simulation of Functional Data First, we simulate functions that we will subsequently classify. For simplicity, we will consider two classification classes. For the simulation, we will first: choose appropriate functions, generate points from a selected interval, which contain, for example, Gaussian noise, smooth these obtained discrete points into a functional object form using a suitable basis system. Through this process, we obtain functional objects along with the value of a categorical variable \\(Y\\), which distinguishes membership in the classification class. Code library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Let us consider two classification classes, \\(Y \\in \\{0, 1\\}\\), with the same number n of generated functions for each class. First, let’s define two functions, each corresponding to one class. We will consider the functions on the interval \\(I = [0, 6]\\). Now, we will create the functions using interpolation polynomials. First, we define the points through which our curve should pass, and then we fit an interpolation polynomial through them, which we use to generate curves for classification. Code # defining points for class 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # defining points for class 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) Figure 1.1: Points defining interpolation polynomials for both classification classes. To compute the interpolation polynomials, we will use the poly.calc() function from the polynom library. Next, we define the functions poly.0() and poly.1(), which will calculate the polynomial values at a given point in the interval. We use the predict() function to create them, where we input the respective polynomial and the point at which we want to evaluate the polynomial. Code polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) Figure 1.2: Visualization of two functions on the interval \\(I = [0, 6]\\), from which we generate observations for classes 0 and 1. Code funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Now we will create a function to generate random functions with added noise (i.e., points on a predefined grid) from a chosen generating function. The argument t represents the vector of values at which we want to evaluate the given functions, fun denotes the generating function, n is the number of functions, and sigma is the standard deviation \\(\\sigma\\) of the normal distribution \\(\\text{N}(\\mu, \\sigma^2)\\), from which we randomly generate Gaussian white noise with \\(\\mu = 0\\). To demonstrate the advantage of using methods that work with functional data, we will add a random term to each simulated observation during generation, representing a vertical shift of the entire function (parameter sigma_shift). This shift will be generated from a normal distribution with the parameter \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Now we can generate the functions. In each of the two classes, we will consider 100 observations, so n = 100. Code n &lt;- 100 t &lt;- seq(0, 6, length = 51) # Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (not yet smoothed) functions in color according to their class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.3: The first 10 generated observations from each of the two classification classes. The observed data are not smoothed. 1.2 Smoothing Observed Curves Now we will convert the observed discrete values (vectors of values) into functional objects that we will work with subsequently. We will again use a B-spline basis for smoothing. We take the entire vector t as the knots, and since we are considering cubic splines, we choose (the implicit choice in R) norder = 4. We will penalize the second derivative of the functions. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) We will find a suitable value for the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), which stands for generalized cross-validation. We will consider the same value of \\(\\lambda\\) for both classification groups, as we would not know in advance which value of \\(\\lambda\\) to choose for test observations in the case of different selections for each class. Code XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 1, length.out = 50) gcv &lt;- rep(NA, length = length(lambda.vect)) for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) gcv[index] &lt;- mean(BSmooth$gcv) } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) lambda.opt &lt;- lambda.vect[which.min(gcv)] To better illustrate, we will plot the course of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.4: The course of \\(GCV(\\lambda)\\) for the chosen vector \\(\\boldsymbol\\lambda\\). The values on the \\(x\\)-axis are plotted on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is indicated in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically represent the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.5: The first 10 smoothed curves from each classification class. Let’s also illustrate all curves, including the average, separately for each class. Code abs.labs &lt;- paste(&quot;Classification class:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, y = &quot;$x_i(t)$&quot;, colour = &#39;Class&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Figure 1.6: Plot of all smoothed observed curves, distinguished by color according to their classification class. The average for each class is indicated by a black line, and for the second class, it is indicated by a dashed line. Code # ggsave(&quot;figures/kap6_sim_03_curves.tex&quot;, device = tikz, width = 8, height = 4) Now let’s take a look at the comparison of average courses from a detailed perspective. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 1.7: Plot of all smoothed observed curves, distinguished by color according to their classification class. The average for each class is indicated by a black line. A closer view. 1.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) library(caret) library(fda.usc) library(MASS) library(fdapace) library(pracma) library(refund) library(nnet) library(caret) library(rpart) library(rattle) library(e1071) library(randomForest) To compare the individual classifiers, we will split the set of generated observations into two parts in a ratio of 70:30, designating them as the training and test (validation) sets. The training set will be used for constructing the classifier, while the test set will be used to calculate the classification error and potentially other characteristics of our model. We can then compare the resulting classifiers based on these calculated characteristics in terms of their classification success. Code # Splitting into test and training sets split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, let’s look at the representation of individual groups in the test and training portions of the data. Code # Absolute representation table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # Relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 1.3.1 \\(K\\)-Nearest Neighbors Let’s start with the non-parametric classification method, specifically the \\(K\\)-nearest neighbors method. First, we will create the necessary objects so that we can work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and examine its classification success. However, the last question remains: how do we choose the optimal number of neighbors \\(K\\)? We could select \\(K\\) as the value that results in the minimum error on the training data. However, this could lead to overfitting, so we will use cross-validation. Given the computational intensity and the size of the dataset, we will choose \\(k\\)-fold cross-validation; for example, we will set \\(k = 10\\). Code # Model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # neighb.model$max.prob # Maximum accuracy # (K.opt &lt;- neighb.model$h.opt) # Optimal value of K Let’s perform the previous procedure for the training data, which we will split into \\(k\\) parts and repeat this part of the code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # Number of neighbors # Splitting training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # Empty matrix to store individual results # Columns will have accuracy values for the given part of the training set # Rows will have values for the given neighbor count K CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # Define the current index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # Iterate through each part... repeat k times for(neighbour in neighbours) { # Model for the specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # Prediction on the validation part model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # Accuracy on the validation part accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store the accuracy in the position for the given K and fold CV.results[neighbour, index] &lt;- accuracy } } # Calculate average accuracies for each K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) accuracy.opt.cv &lt;- max(CV.results) # CV.results We can see that the optimal value of the parameter \\(K\\) is 5 with an error rate calculated using 10-fold CV of 0.3429. For clarity, let’s plot the validation error as a function of the number of neighbors \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation error&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 24 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.8: Dependence of validation error on the value of \\(K\\), i.e., on the number of neighbors. Now that we know the optimal value of the parameter \\(K\\), we can construct the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # Prediction model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # Accuracy on the test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we can see that the error rate of the model constructed using the \\(K\\)-nearest neighbors method with the optimal choice \\(K_{optimal}\\) equal to 5, which we determined via cross-validation, is 0.3571 on the training data and 0.3833 on the test data. To compare the different models, we can use both types of errors. For clarity, we will store them in a table. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 1.3.2 Linear Discriminant Analysis As the second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied to functional data, we must first discretize it, which we will do using Functional Principal Component Analysis. We will then perform the classification algorithm on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components explain at least 90% of the variability in the data. Let’s first perform Functional Principal Component Analysis and determine the number \\(p\\). Code # Principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of principal components nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determine p if(nharm == 1) nharm &lt;- 2 # to plot graphs, we need at least 2 principal components data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p principal components data.PCA.train$Y &lt;- factor(Y.train) # class membership In this specific case, we took the number of principal components $p = $ 2, which together explain 98.72 % of the variability in the data. The first principal component explains 98.2 % and the second 0.52 % of the variability. We can graphically display the scores of the first two principal components, color-coded according to class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Figure 1.9: Scores of the first two principal components for training data. Points are color-coded according to class membership. To determine the classification accuracy on the test data, we need to compute the scores for the first 2 principal components for the test data. These scores are calculated using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] where \\(\\mu(t)\\) is the mean function and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # Calculate scores for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of the residual and eigenfunctions rho (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training data. Code # Model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # Accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (41.43 %) and on the test data (40 %). For graphical representation of the method, we can indicate the decision boundary in the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function. Code # Add decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # if p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # if p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # if p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # Add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.10: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line in the plane of the first two principal components) between the classes constructed using LDA is marked in black. We see that the decision boundary is a line, a linear function in 2D space, which we expected from LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.3 Quadratic Discriminant Analysis Next, let’s construct a classifier using Quadratic Discriminant Analysis (QDA). This is an analogous case to LDA, with the difference that we now allow a different covariance matrix for the normal distribution from which the respective scores originate for each of the classes. This relaxed assumption of equal covariance matrices leads to a quadratic boundary between the classes. In R, we perform QDA analogously to LDA in the previous section, meaning we would again calculate scores for the training and test functions using functional principal component analysis, construct the classifier based on the scores of the first \\(p\\) principal components, and use it to predict the class membership of the test curves \\(Y^* \\in \\{0, 1\\}\\). We do not need to perform functional PCA; we will use the results from the LDA section. We can now proceed directly to constructing the classifier, which we will do using the qda() function. We will then calculate the accuracy of the classifier on the test and training data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we have calculated the error rate of the classifier on the training (35.71 %) and test data (40 %). For a graphical representation of the method, we can mark the decision boundary in the graph of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, just as we did in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.11: Scores of the first two principal components, colored according to class membership. The decision boundary (a parabola in the plane of the first two principal components) between the classes constructed using QDA is indicated in black. Note that the decision boundary between the classification classes is now a parabola. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4 Logistic Regression Logistic regression can be conducted in two ways. One way is to use the functional equivalent of classical logistic regression, and the other is the classical multivariate logistic regression applied to scores of the first \\(p\\) principal components. 1.3.4.1 Functional Logistic Regression Similarly to the case of finite-dimensional input data, we consider the logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is a linear predictor taking values from the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function, which in the case of logistic regression is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\), and \\(\\pi(x)\\) is the conditional probability \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is the parametric function. Our goal is to estimate this parametric function. For functional logistic regression, we use the function fregre.glm() from the fda.usc package. First, we create suitable objects for constructing the classifier. Code # create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # points where the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train$basis To estimate the parametric function \\(\\beta(t)\\), we need to express it in some basis representation, in our case a B-spline basis. However, we need to find a suitable number of basis functions for this. This could be determined based on the error on the training data, but such data would favor selecting a large number of bases, leading to model overfitting. Let’s illustrate this with the following example. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\), we train the model on the training data, determine the error on them, and also calculate the error on the test data. Let us recall that we cannot use the same data for selecting the optimal number of bases as for estimating the test error, as this would lead to an underestimated error. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # relationship f &lt;- Y ~ x # basis for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into matrix pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s plot the trend of both types of errors in a graph depending on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 47 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.12: Dependency of test and training error on the number of basis functions for \\(\\beta\\). The optimal number \\(n_{optimal}\\) selected as the minimum test error is marked with a red point, the test error is drawn in black, and the training error is shown with a blue dashed line. We see that as the number of bases for \\(\\beta(t)\\) increases, the training error (blue line) tends to decrease, and thus we would select large values of \\(n_{basis}\\) based on it. However, the optimal choice based on the test error is \\(n\\) equal to 29, which is significantly smaller than 50. On the other hand, as \\(n\\) increases, the test error rises, indicating model overfitting. For these reasons, we use 10-fold cross-validation to determine the optimal number of basis functions for \\(\\beta(t)\\). We consider a maximum number of 35 basis functions, as we saw above that beyond this value, the model starts overfitting. Code ### 10-fold cross-validation n.basis.max &lt;- 35 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # split training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that do not change during the loop # points where functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline basis basis1 &lt;- X.train$basis # relationship f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to store individual results # columns contain accuracy values for a given part of the training set # rows contain values for a given number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) We prepared everything necessary to calculate the error rate for each of the ten subsets of the training set. We then determine the mean error and select the optimal \\(n\\) as the argument of the minimum validation error. Code for (index in 1:k_cv) { # Define the current index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # Basis for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # Binomial model... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Accuracy on the validation set newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # Calculate the average accuracy for each n across folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Next, let’s plot the validation error rate, highlighting the optimal value \\(n_{optimal}\\) equal to 11 with validation error 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 32 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.13: Relationship of validation error rate with \\(n_{basis}\\), i.e., the number of bases. Now we can define the final model using functional logistic regression, choosing a B-spline basis for \\(\\beta(t)\\) with 11 bases. Code # Optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # Bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # Input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # Binomial model... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Training accuracy predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Test accuracy newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We calculated the training error (equal to 3.57 %) and test error (equal to 8.33 %). For better illustration, we can plot the estimated probabilities of class \\(Y = 1\\) for the training data in relation to the values of the linear predictor. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probabilities Pr(Y = 1|X = x)&#39;, colour = &#39;Class&#39;) Figure 1.14: Relationship of estimated probabilities with values of the linear predictor. Colors differentiate points by class membership. For further information, we can also visualize the estimated parametric function \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 1.15: Plot of the estimated parametric function \\(\\beta(t), t \\in [0, 6]\\). We observe that the values of \\(\\hat\\beta(t)\\) hover around zero for times \\(t\\) in the middle and beginning of the interval \\([0, 6]\\), while the values increase for later times. This suggests a distinction between functions of different classes at the beginning and end of the interval, with high similarity in the middle. Finally, we add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.4.2 Logistic Regression with Principal Component Analysis To construct this classifier, we need to perform functional principal component analysis, determine an appropriate number of components, and calculate the score values for the test data. We already performed this in the linear discriminant analysis section, so we will use those results in the following section. We can now construct the logistic regression model directly using the function glm(, family = binomial). Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # accuracy on training data predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we calculated the classifier error rate on the training data (40.71 %) and on the test data (40 %). To illustrate the method graphically, we can plot the decision boundary on a graph of the scores of the first two principal components. This boundary will be calculated on a dense grid of points and displayed using the geom_contour() function, just as in the cases of LDA and QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.16: Scores of the first two principal components, color-coded by classification class membership. The separating boundary (line in the plane of the first two principal components) between classes, constructed using logistic regression, is shown in black. Note that the separating boundary between the classification classes is now a line, as in the case of LDA. Finally, we add the error rates to a summary table. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5 Decision Trees In this section, we will look at a very different approach to constructing a classifier compared to methods like LDA or logistic regression. Decision trees are a popular tool for classification; however, as with some previous methods, they are not directly designed for functional data. There are ways, though, to transform functional objects into multivariate form and then apply the decision tree algorithm. We can consider the following approaches: using the algorithm on basis coefficients, using principal component scores, using interval discretization and evaluating the function only on a finite grid of points. We will first focus on interval discretization and then compare the results with the remaining two approaches for constructing the decision tree. 1.3.5.1 Interval Discretization First, we need to define points from the interval \\(I = [0, 6]\\) at which we will evaluate the functions. Then, we create an object in which the rows represent individual (discretized) functions, and the columns represent time points. Finally, we add a column \\(Y\\) with the classification class information and repeat the same steps for the test data. Code # sequence of points at which the function is evaluated t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can construct a decision tree where all time points from the t.seq vector serve as predictors. This classifier is not susceptible to multicollinearity, so we don’t need to consider it. Accuracy will be used as the metric. Code # model construction clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The classifier error rate on the test data is 46.67 % and on the training data 33.57 %. We can visualize the decision tree using the fancyRpartPlot() function. We set the node colors to reflect the previous color coding. This is an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.17: Graphic representation of the unpruned decision tree. Nodes belonging to classification class 1 are shown in blue shades, and those for class 0 are in red shades. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # displaying desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 1.18: Final pruned decision tree. Finally, we again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - discret.&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.2 Principal Component Scores Another option for constructing a decision tree is to use principal component scores. Since we have already calculated scores for previous classification methods, we will use these results and construct a decision tree based on the scores of the first 2 principal components. Code # model construction clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the test data is 40 % and on the training data 37.86 %. We can visualize the decision tree based on principal component scores using the fancyRpartPlot() function. We set the node colors to reflect the previous color coding. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.19: Graphic representation of the unpruned decision tree based on principal component scores. Nodes belonging to classification class 1 are shown in blue shades, and those for class 0 are in red shades. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # final model extra = 104, # displaying desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 1.20: Final pruned decision tree. Finally, we add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.5.3 Basis Coefficients The final method we will use to construct a decision tree is by employing coefficients derived from the B-spline basis representation of functions. First, let us define the required datasets with the basis coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testing dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now we can proceed to build the classifier. Code # model construction clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on testing data predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the training data is 33.57 % and on the testing data 45 %. We can visualize the decision tree built on the B-spline coefficients using the fancyRpartPlot() function. We set the node colors to reflect the previous color coding. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.21: Graphic representation of the unpruned decision tree based on basis coefficients. Nodes belonging to classification class 1 are shown in blue shades, and those for class 0 are in red shades. We can also display the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # final model extra = 104, # displaying desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 1.22: Final pruned decision tree. Finally, let us add the training and testing error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6 Random Forests A classifier constructed with the random forest method consists of building multiple individual decision trees that are then combined to form a unified classifier (by “voting” among the trees). Similar to decision trees, we have several options for which type of (finite-dimensional) data to use in constructing the model. We will once again consider the three approaches discussed above. The datasets with the respective variables for all three approaches have been prepared in the previous section, so we can directly proceed to construct the models, calculate classifier characteristics, and add the results to the summary table. 1.3.6.1 Interval Discretization In the first case, we evaluate the functions on a specified grid of points in the interval $ I = [0, 6] $. Code # model construction clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on testing data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is 0.71 % and on the testing data 40 %. Code Res &lt;- data.frame(model = &#39;RForest - discr&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.2 Principal Component Scores In this case, we use the scores of the first $ p = $ 2 principal components. Code # model construction clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on testing data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate on the training data is therefore 4.29 % and on the testing data 40 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.6.3 Basis Coefficients Finally, we use the function representations through the B-spline basis. Code # model construction clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on testing data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 0.71 % and on the testing data 36.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7 Support Vector Machines Now let’s look at the classification of our simulated curves using the Support Vector Machines (SVM) method. The advantage of this classification method is its computational efficiency, as it uses only a few (often very few) observations to define the boundary curve between classes. The main advantage of SVM is the use of the so-called kernel trick, which allows us to replace the ordinary scalar product with another scalar product of transformed data without having to directly define this transformation. This gives us a generally nonlinear decision boundary between classification classes. The kernel (kernel function) \\(K\\) is a function that satisfies \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] where \\(\\phi\\) is some (unknown) transformation (feature map), \\(\\mathcal H\\) is a Hilbert space, and \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) is some scalar product in this Hilbert space. In practice, three types of kernel functions are most commonly chosen: linear kernel – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), polynomial kernel – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), radial (Gaussian) kernel – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). For all the above-mentioned kernels, we must choose a constant \\(C &gt; 0\\), which indicates the degree of penalty for exceeding the decision boundary between classes (inverse regularization parameter). As the value of \\(C\\) increases, the method will penalize poorly classified data more and pay less attention to the shape of the boundary; conversely, for small values of \\(C\\), the method does not give much importance to poorly classified data but focuses more on penalizing the shape of the boundary. This constant \\(C\\) is typically set to 1 by default, but we can also determine it directly, for example, using cross-validation. By using cross-validation, we can also determine the optimal values of other hyperparameters, which now depend on our choice of kernel function. In the case of a linear kernel, we do not choose any other parameters aside from the constant \\(C\\); with polynomial and radial kernels, we must specify the hyperparameter values \\(\\alpha_0, \\gamma, \\text{ and } d\\), whose default values in R are sequentially \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{\\text{dim}(\\texttt{data})} \\text{ and } d^{default} = 3\\). Again, we could determine the optimal hyperparameter values for our data, but due to the relatively high computational cost, we will leave the values of the respective hyperparameters at the values estimated using CV on a generated dataset, choosing \\(\\alpha_0 = 1\\). In the case of functional data, we have several options for applying the SVM method. The simplest variant is to apply this classification method directly to the discretized function (section 1.3.7.2). Another option is to again use the principal component scores and classify the curves based on their representation 1.3.7.3. Another straightforward variant is to utilize the expression of curves using a B-spline basis and classify the curves based on the coefficients of their representation in this basis (section 1.3.7.4). Through more complex reasoning, we can arrive at several other options that utilize the functional nature of the data. On the one hand, instead of classifying the original curve, we can use its derivative (or second derivative, third, etc.); on the other hand, we can utilize the projection of functions onto the subspace generated by, for example, B-spline functions (section 1.3.7.5). The last method we will use for classifying functional data involves a combination of projecting onto a certain subspace generated by functions (Reproducing Kernel Hilbert Space, RKHS) and classifying the corresponding representation. This method utilizes both classical SVM and SVM for regression; more details can be found in the section RKHS + SVM 1.3.7.6. 1.3.7.1 SVM for Functional Data In the fda.usc library, we will use the function classif.svm() to apply the SVM method directly to functional data. First, we will create suitable objects for constructing the classifier. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # splitting into test and training parts X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) Code # create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train_norm$basis Code # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # SVM model model.svm.f &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, degree = 5, coef0 = 1, cost = 1e4) # accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.train &lt;- mean(factor(Y.train_norm) == predictions.train) # accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.test &lt;- mean(factor(Y.test_norm) == predictions.test) We calculated the training error (which is 0 %) and the test error (which is 11.67 %). Now let’s attempt, unlike the procedure in the previous chapters, to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, acknowledging that its optimal value may differ between kernels. For all three kernels, we will explore the values of the hyperparameter \\(C\\) in the range \\([10^{0}, 10^{7}]\\), while for the polynomial kernel, we will consider the value of the hyperparameter \\(p\\) to be 3, 4, or 5, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use \\(r k_cv\\)-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the range \\([10^{-5}, 10^{0}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # We split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Which values of gamma do we want to consider gamma.cv &lt;- 10^seq(-5, 0, length = 6) C.cv &lt;- 10^seq(0, 7, length = 8) p.cv &lt;- c(3, 4, 5) coef0 &lt;- 1 # A list with three components... an array for each kernel -&gt; linear, poly, radial # An empty matrix where we will place individual results # The columns will contain the accuracy values for each # The rows will correspond to the values for a given gamma and the layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (cost in C.cv) { # We go through the individual folds for (index_cv in 1:k_cv) { # Definition of the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # Points at which the functions are evaluated tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEAR KERNEL # SVM model clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gam.cv in gamma.cv) { # Model construction clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- accuracy.test.r } } } Now we will average the results of 10-fold CV so that we have one estimate of validation error for one value of the hyperparameter (or one combination of values). At the same time, we will determine the optimal values of the individual hyperparameters. Code # We calculate the average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s take a look at how the optimal values turned out. For linear kernel, we have the optimal value \\(C\\) equal to 100, for polynomial kernel \\(C\\) is equal to 1, and for radial kernel, we have two optimal values, for \\(C\\) the optimal value is 10^{6} and for \\(\\gamma\\) it is 10^{-4}. The validation error rates are 0.0935256 for linear, 0.1731822 for polynomial, and 0.0787637 for radial kernel. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # Create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # Points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) Code model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # Accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # Accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) The error rate of the SVM method on the training data is thus 5 % for the linear kernel, 11.4286 % for the polynomial kernel, and 2.8571 % for the Gaussian kernel. On the test data, the error rate of the method is 23.3333 % for the linear kernel, 26.6667 % for the polynomial kernel, and 13.3333 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.2 Discretization of the Interval Let’s start by applying the support vector method directly to the discretized data (evaluating the function on a given grid of points over the interval \\(I = [0, 6]\\)), considering all three aforementioned kernel functions. Code # Set norms equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = TRUE) # Split into test and training parts X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Now, let’s attempt to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, and we allow its optimal value to differ among kernels. For all three kernels, we will go through the values of the hyperparameter \\(C\\) in the interval \\([10^{0}, 10^{6}]\\), while for the polynomial kernel we will fix the hyperparameter \\(p\\) at a value of 3, since other integer values do not yield nearly as good results. On the other hand, for the radial kernel, we will use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-5}, 10^{-2}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Values of gamma to consider gamma.cv &lt;- 10^seq(-5, -2, length = 7) C.cv &lt;- 10^seq(0, 6, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components ... arrays for individual kernels -&gt; linear, poly, radial # Empty matrices to store the results # Columns will contain accuracy values for given C # Rows will contain values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (C in C.cv) { # Go through individual folds for (index_cv in 1:k_cv) { # Definition of test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEAR KERNEL # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) accuracy.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) accuracy.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Model construction clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) accuracy.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- accuracy.test.r } } } Now, let’s average the results of the 10-fold CV so that for each value of the hyperparameter (or one combination of values), we have one estimate of the validation error. In this process, we will also determine the optimal values for the individual hyperparameters. Code # Calculate average accuracies for each C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For the linear kernel, the optimal value of \\(C\\) is 100, for the polynomial kernel \\(C\\) is 10, and for the radial kernel, we have two optimal values: for \\(C\\), the optimal value is 10^{6}, and for \\(\\gamma\\), it is 0. The validation errors are 0.0875733 for linear, 0.160815 for polynomial, and 0.0859066 for radial kernels. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined by 10-fold CV. We will also determine the errors on both test and training datasets. Code # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) accuracy.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) accuracy.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) accuracy.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) accuracy.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) accuracy.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) accuracy.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is 5.7143 % for the linear kernel, 9.2857 % for the polynomial kernel, and 2.1429 % for the Gaussian kernel. On the test data, the error rate is 16.6667 % for the linear kernel, 21.6667 % for the polynomial kernel, and 11.6667 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.3 Principal Component Scores In this case, we use the scores of the first \\(p =\\) 2 principal components. Now, let’s try, unlike the approach in previous chapters, to estimate the classifier hyperparameters from the data using 10-fold cross-validation. Given that each kernel has different hyperparameters in its definition, we will treat each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, although we allow that its optimal value may differ between kernels. For all three kernels, we will test values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{4}]\\). For the polynomial kernel, we fix the hyperparameter \\(p\\) at the value of 3, as for other integer values, the method does not give nearly as good results. In contrast, for the radial kernel, we will use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-4}, 10^{-1}]\\). We set coef0 \\(= 1\\). Code set.seed(42) # gamma values to consider gamma.cv &lt;- 10^seq(-4, -1, length = 7) C.cv &lt;- 10^seq(-3, 4, length = 8) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components ... array for individual kernels -&gt; linear, poly, radial # empty matrix to store results # columns will have accuracy values for a given # rows will have values for given gamma and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first, go through C values for (C in C.cv) { # iterate over each fold for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEAR KERNEL # build model clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # build model clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # build model clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we average the results of the 10-fold CV to obtain a single estimate of validation error for each hyperparameter value (or combination of values). We also determine the optimal values of each hyperparameter. Code # calculate average accuracies for individual C over folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For the linear kernel, the optimal value of \\(C\\) is 10, for the polynomial kernel \\(C\\) is 0.01, and for the radial kernel, there are two optimal values: \\(C\\) is 10^{4} and \\(\\gamma\\) is 0.001. The validation errors are 0.4097253 for linear, 0.4287454 for polynomial, and 0.3906502 for the radial kernel. Finally, we can construct the final classifiers on the entire training dataset with hyperparameter values determined using 10-fold CV. We also calculate errors on the test and training datasets. Code # build model clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The training accuracies are 0.6 for linear, 0.5928571 for polynomial, and 0.6357143 for radial kernels, and the test accuracies are 0.6 for linear, 0.6 for polynomial, and 0.5833333 for radial. To visualize the method, we can plot the decision boundary on a graph of the scores for the first two principal components. We compute this boundary on a dense grid of points and display it using the geom_contour() function, just as in previous cases where we also plotted the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;, linetype = &#39;Kernel&#39;) + scale_colour_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 1.23: Scores of the first two principal components, color-coded by classification group. The decision boundary (either a line or curves in the plane of the first two principal components) between classes is displayed in black, created using the SVM method. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.4 Basis Coefficients Finally, we use a B-spline basis to express the functions. For all three kernels, we examine the values of hyperparameter \\(C\\) in the interval \\([10^{1}, 10^{4}]\\). For the polynomial kernel, we fix the hyperparameter \\(p\\) at 3, as other integer values do not yield nearly as good results. On the other hand, for the radial kernel, we again use 10-fold CV to select the optimal value of hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-4}, 10^{-1}]\\). We set coef0 \\(= 1\\). Code set.seed(42) # gamma values to consider gamma.cv &lt;- 10^seq(-4, -1, length = 7) C.cv &lt;- 10^seq(1, 4, length = 10) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components...array for each kernel -&gt; linear, poly, radial # empty matrix to store individual results # columns hold accuracy values for given # rows represent values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first iterate through values of C for (C in C.cv) { # iterate over individual folds for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEAR KERNEL # model creation clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # model creation clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # model creation clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we average the results from 10-fold CV so that we have a single estimate of validation error for each hyperparameter value (or combination of values). We also determine the optimal values of each hyperparameter. Code # calculate average accuracies for each C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s see how the optimal values turned out. For the linear kernel, the optimal \\(C\\) value is 100, for the polynomial kernel, \\(C\\) is 215.4435, and for the radial kernel, we have two optimal values: \\(C\\) is 4641.5888, and \\(\\gamma\\) is 10^{-4}. The validation error rates are 0.0929762 for linear, 0.1542537 for polynomial, and 0.1319048 for radial kernels. Finally, we can construct the final classifiers on the entire training dataset using the hyperparameter values determined by 10-fold CV. We will also calculate the error rates on both the test and training data. Code # Model construction clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the basis coefficients on the training data is 6.43% for the linear kernel, 4.29% for the polynomial kernel, and 13.57% for the Gaussian kernel. On the test data, the error rate is 6.6667% for the linear kernel, 13.3333% for the polynomial kernel, and 20% for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.5 Projection onto the B-spline Basis Another approach for using the classical SVM method for functional data is to project the original data onto a \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal H\\), which we denote as \\(V_d\\). Assume this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), which can be written as \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] For classification, we can then use the coefficients from the orthogonal projection, applying the standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By using this transformation, we define a new, so-called adapted kernel, composed of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector method. Thus, we have an (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This method of dimensionality reduction is often referred to as filtering. For the actual projection, we use the project.basis() function in R from the fda package. Its input includes a matrix of original discrete (unsmoothed) data, the values at which we measure in the original data matrix, and the basis object onto which we want to project the data. We choose to project onto the B-spline basis because the Fourier basis is not suitable for our non-periodic data. Another option would be to use the wavelet basis. The dimension \\(d\\) can be chosen based on prior expert knowledge or by cross-validation. In our case, we determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (selecting \\(k \\ll n\\) due to the computational intensity of the method, often with \\(k = 5\\) or \\(k = 10\\)). We require 4th-order B-splines, with the number of basis functions given by the relation \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). In R, however, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and large values of \\(n_{basis}\\) lead to overfitting, so we choose a smaller maximum for \\(n_{basis}\\). Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # All dimensions to test # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components ... matrices for each kernel -&gt; linear, poly, radial # Empty matrix for storing individual results # Columns contain accuracy values for each part of the training set # Rows contain values for each dimension size CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Project discrete data onto B-spline basis of dimension d Projection &lt;- project.basis(y = XX, # Matrix of discrete data argvals = t, # Vector of arguments basisobj = bbasis) # Basis object # Training and test split within CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Define training and test parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Model construction clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## Linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies in positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.05863095 ## poly 11 0.08244048 ## radial 10 0.07440476 We see that the optimal value of the parameter \\(d\\) is 11 for the linear kernel with an error rate computed via 10-fold CV of 0.0586, 11 for the polynomial kernel with an error rate of 0.0824, and 10 for the radial kernel with an error rate of 0.0744. To provide a clearer overview, let’s plot the progression of validation error rates as a function of dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation Error Rate&#39;, colour = &#39;Kernel&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 1.24: Dependence of validation error rate on subspace dimension \\(V_d\\), for each of the three kernels considered in the SVM method. Optimal values of dimension \\(V_d\\) for each kernel function are marked by black points. Now, we can train individual classifiers on all training data and evaluate their accuracy on test data. For each kernel function, we select the dimension of the subspace onto which we project based on the cross-validation results. In the variable Projection, we store the matrix of orthogonal projection coefficients: \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare data table for storing results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Iterate over kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # projection of discrete data onto B-spline basis Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # split into training and testing data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # construct model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # training data accuracy predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # test data accuracy predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to basis coefficients on the training data is 4.29% for the linear kernel, 3.57% for the polynomial kernel, and 5% for the Gaussian kernel. On the test data, the error rate is 10% for the linear kernel, 8.33% for the polynomial kernel, and 10% for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.6 RKHS + SVM In this section, we will look at another way to utilize support vector methods for classifying functional data. This time, we will again apply the well-known principle of first expressing functional data as some finite-dimensional objects, and then applying the classic SVM method to these objects. However, in this case, we will also use the SVM method for the representation of functional data using a certain finite-dimensional object. As the name suggests, this involves a combination of two concepts: the support vector method and a space known in English literature as Reproducing Kernel Hilbert Space (RKHS). The key concept for this space is the kernel. Definition 1.1 (Kernel) A kernel is a function \\(K : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\) such that for every pair \\(\\boldsymbol{x}, \\tilde{\\boldsymbol{x}} \\in \\mathcal{X}\\) it holds that \\[ K(\\boldsymbol{x}, \\tilde{\\boldsymbol{x}}) = \\big\\langle \\boldsymbol{\\phi}(\\boldsymbol{x}), \\boldsymbol{\\phi}(\\tilde{\\boldsymbol{x}}) \\big\\rangle_{\\mathcal{H}}, \\] where \\(\\boldsymbol{\\phi} : \\mathcal{X} \\rightarrow \\mathcal{H}\\) is a mapping from the space \\(\\mathcal{X}\\) to the space \\(\\mathcal{H}\\). For a function to be a kernel, it must satisfy certain conditions. Lemma 1.1 Let \\(\\mathcal{X}\\) be a Hilbert space. Then a symmetric function \\(K : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\) is a kernel if for all \\(k \\geq 1, \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_k \\in \\mathcal{X}\\) and \\(c_1, \\dots, c_k \\in \\mathbb{R}\\), it holds that \\[ \\sum_{i, j = 1}^k c_i c_j K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0. \\] The property above is called positive semidefiniteness. The following statement also holds. Theorem 1.1 A function \\(K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\) is a kernel if and only if there exists a Hilbert space \\(\\mathcal{H}\\) and a mapping \\(\\boldsymbol{\\phi} : \\mathcal{X} \\rightarrow \\mathcal{H}\\) such that \\[ K(\\boldsymbol{x}, \\tilde{\\boldsymbol{x}}) = \\big\\langle \\boldsymbol{\\phi}(\\boldsymbol{x}), \\boldsymbol{\\phi}(\\tilde{\\boldsymbol{x}}) \\big\\rangle_{\\mathcal{H}} \\quad \\forall \\boldsymbol{x}, \\tilde{\\boldsymbol{x}} \\in \\mathcal{X}. \\] Now we are ready to introduce the concept of Reproducing Kernel Hilbert Space. 1.3.7.6.1 Reproducing Kernel Hilbert Space (RKHS) Consider a Hilbert space \\(\\mathcal{H}\\) as a space of functions. Our goal is to define the space \\(\\mathcal{H}\\) and the mapping \\(\\phi\\) such that \\(\\phi(x) \\in \\mathcal{H}, \\ \\forall x \\in \\mathcal{X}\\). Let \\(\\phi(x) = k_x\\). We assign to each function \\(x \\in \\mathcal{X}\\) the function \\(x \\mapsto k_x \\in \\mathcal{H}, k_x := K(x, \\cdot)\\). Then \\(\\phi: \\mathcal{X} \\rightarrow \\mathbb{R}^{\\mathcal{X}}\\), so we can summarize it as \\[ x \\in \\mathcal{X} \\mapsto \\phi(x) = k_x = K(x, \\cdot) \\in \\mathcal{H}, \\] The point (function) \\(x \\in \\mathcal{X}\\) is mapped to the function \\(k_x: \\mathcal{X} \\rightarrow \\mathbb{R}, k_x(y) = K(x, y)\\). Consider the set of all images \\(\\{k_x | x \\in \\mathcal{X}\\}\\) and define the linear span of this set of vectors as \\[ \\mathcal{G} := \\text{span}\\{k_x | x \\in \\mathcal{X}\\} = \\left\\{\\sum_{i = 1}^r \\alpha_i K(x_i, \\cdot) \\ \\Big|\\ \\alpha_i \\in \\mathbb{R}, r \\in \\mathbb{N}, x_i \\in \\mathcal{X}\\right\\}. \\] Then the inner product is given by \\[ \\langle k_x, k_y \\rangle = \\langle K(x, \\cdot), K(y, \\cdot) \\rangle = K(x, y),\\quad x, y \\in \\mathcal{X} \\] and generally \\[ f, g \\in \\mathcal{G}, f = \\sum_i \\alpha_i K(x_i, \\cdot), g = \\sum_j \\beta_j K(y_j, \\cdot), \\\\ \\langle f, g \\rangle_{\\mathcal{G}} = \\Big\\langle \\sum_i \\alpha_i K(x_i, \\cdot), \\sum_j \\beta_j K(y_j, \\cdot) \\Big\\rangle = \\sum_i\\sum_j \\alpha_i \\beta_j \\langle K(x_i, \\cdot), K(y_j, \\cdot) \\rangle = \\sum_i\\sum_j \\alpha_i \\beta_j K(x_i, y_j). \\] The space \\(\\mathcal{H} := \\overline{\\mathcal{G}}\\), which is the closure of the space \\(\\mathcal{G}\\), is called the Reproducing Kernel Hilbert Space (RKHS). A significant property of this space is \\[ K(x, y) = \\Big\\langle \\phi(x), \\phi(y) \\Big\\rangle_{\\mathcal{H}}. \\] Note: The name Reproducing comes from the following fact. Let \\(f = \\sum_i \\alpha_i K(x_i, \\cdot)\\) be any function. Then \\[\\begin{align*} \\langle K(x, \\cdot), f \\rangle &amp;= \\langle K(x, \\cdot), \\sum_i \\alpha_i K(x_i, \\cdot) \\rangle =\\\\ &amp;= \\sum_i \\alpha_i \\langle K(x, \\cdot), K(x_i, \\cdot) \\rangle = \\sum_i \\alpha_i K(x_i, x) = \\\\ &amp;= f(x) \\end{align*}\\] Properties: Let \\(\\mathcal{H}\\) be a Hilbert space of functions \\(g: \\mathcal{X} \\rightarrow \\mathbb{R}\\). Then \\(\\mathcal{H}\\) is an RKHS if and only if all evaluation functionals \\(\\delta_x: \\mathcal{H} \\rightarrow \\mathbb{R}, g \\mapsto g(x)\\) are continuous. For a given kernel \\(K\\), there exists exactly one RKHS (up to isometric isomorphism). For a given RKHS, the kernel \\(K\\) is uniquely determined. Functions in RKHS are pointwise well-defined. RKHS is generally an infinite-dimensional vector space, but in practice, we only work with its finite-dimensional subspace. At the end of this section, let us state an important theorem. Theorem 1.2 (The representer theorem) Let \\(K\\) be a kernel and \\(\\mathcal H\\) be the corresponding RKHS with norm and inner product \\(\\|\\cdot\\|_{\\mathcal H}\\) and \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\). Suppose we want to find a linear function \\(f: \\mathcal H \\rightarrow \\mathbb R\\) in the Hilbert space \\(\\mathcal H\\) defined by the kernel \\(K\\). The function \\(f\\) has the form \\(f(x) = \\langle \\omega, x \\rangle_{\\mathcal H}\\) for some \\(\\omega \\in \\mathcal H\\). Consider the regularized minimization problem: \\[\\begin{equation} \\min_{\\omega \\in \\mathcal H} R_n(\\omega) + \\lambda \\Omega(\\|\\omega\\|_{\\mathcal H}), \\tag{1.1} \\end{equation}\\] where \\(\\Omega: [0, \\infty) \\rightarrow \\mathbb R\\) is a strictly monotonically increasing function (regularizer), and \\(R_n(\\cdot)\\) is the empirical loss (empirical risk) of the classifier with respect to the loss function \\(\\ell\\). Then the optimization problem (1.1) always has an optimal solution, which is of the form: \\[\\begin{equation} \\omega^* = \\sum_{i = 1}^n \\alpha_i K(x_i, \\cdot), \\tag{1.2} \\end{equation}\\] where \\((x_i, y_i)_{i = 1, 2, \\dots, n} \\in \\mathcal X \\times \\mathcal Y\\) is a set of training values. \\(\\mathcal H\\) is generally an infinite-dimensional space, but for a finite dataset of size \\(n\\), \\(\\mathcal H\\) has dimension at most \\(n\\). Furthermore, every \\(n\\)-dimensional subspace of a Hilbert space is isometric to \\(\\mathbb R^n\\), so we can assume that the mapping (feature map) maps into \\(\\mathbb R^n\\). The kernel \\(K\\) is universal if the RKHS \\(\\mathcal H\\) is a dense subset of \\(\\mathcal C(\\mathcal X)\\) (the set of continuous functions). Moreover, the following facts hold: Universal kernels are good for approximation. The Gaussian kernel with a fixed value \\(\\sigma\\) is universal. Universality is a necessary condition for consistency. 1.3.7.6.2 Classification Using RKHS The basic idea is to project the original data onto a subspace of the RKHS, denoted as \\(\\mathcal H_K\\) (the index \\({}_K\\) refers to the fact that this space is defined by the kernel \\(K\\)). The goal is to transform a curve (the observed object, function) into a point in the RKHS. Let us denote the set of observed curves as \\(\\{\\hat c_1, \\dots, \\hat c_n\\}\\), where each curve \\(\\hat c_l\\) is defined by the data \\(\\{(\\boldsymbol x_i, \\boldsymbol y_{il}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), with \\(\\mathcal X\\) being the space of input variables and typically \\(\\mathcal Y = \\mathbb R\\). Assume that for each function \\(\\hat c_l\\), there exists a continuous function \\(c_l:\\mathcal X \\rightarrow \\mathcal Y\\) such that \\(\\mathbb E[y_l|\\boldsymbol x] = c_l(\\boldsymbol x)\\). We also assume that \\(\\boldsymbol x_i\\) are common to all curves. Muñoz and González in their paper1 propose the following procedure. The curve \\(c_l^*\\) can be expressed as \\[ c_l^*(\\boldsymbol x) = \\sum_{i = 1}^m \\alpha_{il} K(\\boldsymbol x_i, \\boldsymbol x), \\quad \\forall \\boldsymbol x \\in \\mathcal X, \\] where \\(\\alpha_{il} \\in \\mathbb R\\). These coefficients can be obtained in practice by solving the optimization problem \\[ \\text{argmin}_{c \\in \\mathcal H_K} \\frac{1}{m} \\sum_{i = 1}^m \\big[|c(\\boldsymbol x_i) - y_i| - \\varepsilon\\big]_+ + \\gamma \\|c\\|_{K}^2, \\quad \\gamma &gt; 0, \\varepsilon \\geq 0, \\] specifically, for example, using the SVM method. Due to the well-known property of this method, many coefficients will be \\(\\alpha_{il} = 0\\). By minimizing the above expression, we obtain functions \\(c_1^*, \\dots, c_n^*\\) corresponding to the original curves \\(\\hat c_1, \\dots, \\hat c_n\\). Thus, the SVM method provides a meaningful representation of the original curves using the coefficient vector \\(\\boldsymbol \\alpha_l = (\\alpha_{1l}, \\dots, \\alpha_{ml})^\\top\\) for \\(\\hat c_l\\). However, this representation is very unstable, as even a small change in the original values can lead to a change in the set of support vectors for the given function, resulting in a significant change in the entire representation of this curve (the representation is not continuous in the input values). Therefore, we define a new representation of the original curves that will not suffer from this drawback. Theorem 1.3 (Muñoz-González) Let \\(c\\) be a function whose observed version is \\(\\hat c = \\{(\\boldsymbol x_i, y_{i}) \\in \\mathcal X \\times \\mathcal Y\\}_{i = 1}^m\\), and let \\(K\\) be a kernel with eigenfunctions \\(\\{\\phi_1, \\dots, \\phi_d, \\dots\\}\\) (basis of \\(\\mathcal H_K\\)). Then the function \\(c^*(\\boldsymbol x)\\) can be expressed in the form \\[\\begin{equation*} c^*(\\boldsymbol x) = \\sum_{j = 1}^d \\lambda_j^* \\phi_j(\\boldsymbol x), \\end{equation*}\\] where \\(\\lambda_j^*\\) are the weights of the projection of \\(c^*(\\boldsymbol x)\\) onto the space of functions generated by the eigenfunctions of the kernel \\(K\\), and \\(d\\) is the dimension of the space \\(\\mathcal H\\). In practice, when we only have a finite number of observations, \\(\\lambda_j^*\\) can be estimated using \\[\\begin{equation*} \\hat\\lambda_j^* = \\hat\\lambda_j \\sum_{i = 1}^m \\alpha_i\\hat\\phi_{ji}, \\quad j = 1, 2, \\dots, \\hat d, \\end{equation*}\\] where \\(\\hat\\lambda_j\\) is the \\(j\\)-th eigenvalue corresponding to the \\(j\\)-th eigenvector \\(\\hat\\phi_j\\) of the matrix \\(K_S = \\big(K(\\boldsymbol x_i, \\boldsymbol x_j)\\big)_{i, j = 1}^m\\), \\(\\hat d = \\text{rank}(K_S)\\), and \\(\\alpha_i\\) are the solutions to the optimization problem. 1.3.7.6.3 Implementation of the Method in R From the last part of Theorem 1.3, it follows how we should compute the representations of the curves in practice. We will work with discretized data after smoothing the curves. First, we define the kernel for the RKHS space. We will use the Gaussian kernel with parameter \\(\\gamma\\). The value of this hyperparameter significantly affects the behavior and thus the success of the method, so we must pay special attention to its selection (we choose it using cross-validation). 1.3.7.6.3.1 Gaussian Kernel Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s calculate the matrix \\(K_S\\) and its eigenvalues and corresponding eigenvectors. Code # Calculate the matrix K gamma &lt;- 0.1 # Fixed value of gamma; we will determine the optimal value using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To calculate the coefficients in the representation of the curves, i.e., the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat d l}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we also need the coefficients from the SVM. Unlike the classification problem, we are now solving a regression problem, as we are trying to express our observed curves in some basis (chosen by us via the kernel \\(K\\)). Therefore, we will use the Support Vector Regression method, from which we will subsequently obtain the coefficients \\(\\alpha_{il}\\). Code # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } Now we can calculate the representations of the individual curves. First, let’s choose \\(\\hat d\\) as the entire dimension, i.e., \\(\\hat d = m ={}\\) 101, and then we will determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create an empty object # Calculate the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) stored in the columns of the matrix Lambda.RKHS for each curve. We will now use these vectors as the representation of the given curves and classify the data according to this discretization. Code # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # Loop through each kernel for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 1.1: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 We see that the model performs very well on the training data for all three kernels, while its performance on the testing data is not good at all. It is clear that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values of \\(\\gamma\\) and \\(d\\). Code # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:40 # Reasonable range of values for d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to insert individual results # Columns will contain accuracy values for given # Rows will contain values for given gamma and layers will correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate over folds for (index_cv in 1:k_cv) { # Define testing and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # Insert accuracies for the given d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 1.2: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) the testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 10 8.4834 0.0728 linear poly 17 8.4834 0.0870 polynomial radial 38 0.4394 0.1054 radial We see that the best parameter values are \\(d={}\\) 10 and \\(\\gamma={}\\) 8.4834 for the linear kernel, with the error calculated using 10-fold CV being 0.0728. Similarly, \\(d={}\\) 17 and \\(\\gamma={}\\) 8.4834 for the polynomial kernel, with an error of 0.087, and \\(d={}\\) 38 and \\(\\gamma={}\\) 0.4394 for the radial kernel, yielding an error of 0.1054. For interest, let’s plot the validation error as a function of dimension \\(d\\) and the hyperparameter \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 1.25: Dependence of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. In the above plots, we can see how the validation error changed with the values of hyperparameters \\(d\\) and \\(\\gamma\\). Notably, in all three plots for the individual kernels, significant horizontal patterns are visible. From this, we can draw a significant theoretical and practical conclusion: the considered classification method (projection onto RKHS using SVM + SVM classification) is robust to the choice of the hyperparameter \\(d\\) (i.e., a small change in this parameter’s value does not lead to a significant deterioration in validation error), while we must be very cautious with the choice of the hyperparameter \\(\\gamma\\) (even a small change in its value can lead to a large change in validation error). This behavior is most apparent with the Gaussian kernel. Since we have already found the optimal values of the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Loop through the individual kernels for (kernel_number in 1:3) { # Calculate the K matrix gamma &lt;- gamma.opt[kernel_number] # gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine the coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Model construction clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 1.3: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimated training error and \\(\\widehat{Err}_{test}\\) the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0500 0.1333 SVM poly - RKHS - radial 0.0143 0.2000 SVM rbf - RKHS - radial 0.0143 0.1333 The error rate of the SVM method combined with the projection onto the Reproducing Kernel Hilbert Space is thus equal to 5 % for the linear kernel, 1.43 % for the polynomial kernel, and 1.43 % for the Gaussian kernel on the training data. For the test data, the error rate of the method is 13.33 % for the linear kernel, 20 % for the polynomial kernel, and 13.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.6.3.2 Polynomial Kernel Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix... polynomial with parameter p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Values of hyperparameters to iterate over dimensions &lt;- 3:40 # reasonable range of values d poly.cv &lt;- 2:5 # List with three components... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store individual results # Columns will have accuracy values for the given # Rows will have values for the given p, and layers correspond to folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate over folds for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies for the given d, gamma, and fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate the average accuracy for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 1.4: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 17 4 0.1412 linear poly 10 4 0.1429 polynomial radial 38 4 0.1550 radial We see that the best values for the parameter \\(d={}\\) 17 and \\(p={}\\) 4 are for the linear kernel, with an error value computed using 10-fold CV of 0.1412. The values \\(d={}\\) 10 and \\(p={}\\) 4 are for the polynomial kernel, with an error value computed using 10-fold CV of 0.1429, and \\(d={}\\) 38 and \\(p={}\\) 4 are for the radial kernel, with an error value of 0.155. Since we have found the optimal values of the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # Loop through individual kernels for (kernel_number in 1:3) { # Calculate matrix K p &lt;- poly.opt[kernel_number] # value of gamma from CV K &lt;- Kernel.RKHS(t.seq, p = p) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Model construction clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 1.5: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.1071 0.2167 SVM poly - RKHS - poly 0.0643 0.2000 SVM rbf - RKHS - poly 0.1000 0.2333 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 10.71 % for the linear kernel, 6.43 % for the polynomial kernel, and 10 % for the Gaussian kernel. On the test data, the error rate of the method is 21.67 % for the linear kernel, 20 % for the polynomial kernel, and 23.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.7.6.3.3 Linear Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate through dimensions &lt;- 3:40 # reasonable range of values for d # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store individual results # Columns will hold accuracy values for given d # Rows will hold values for layers corresponding to folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation itself K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Constructing the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store the results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # Insert accuracies into positions for given d, gamma, and fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 1.6: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 9 0.4158 linear poly 30 0.3753 polynomial radial 17 0.3962 radial We see that the optimal parameter value \\(d={}\\) 9 for the linear kernel corresponds to a classification error rate estimated using 10-fold CV of 0.4158, \\(d={}\\) 30 for the polynomial kernel has a classification error rate estimated using 10-fold CV of 0.3753, and \\(d={}\\) 17 for the radial kernel has a classification error rate of 0.3962. Now that we have found the optimal hyperparameter values, we can construct the final models and determine their classification success on the test data. Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { # Calculate the K matrix K &lt;- Kernel.RKHS(t.seq) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Constructing the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 1.7: Summary of results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.3643 0.3000 SVM poly - RKHS - linear 0.2071 0.3500 SVM rbf - RKHS - linear 0.3071 0.3833 The error rate for the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus equal to 36.43 % on the training data for the linear kernel, 20.71 % for the polynomial kernel, and 30.71 % for the Gaussian kernel. On the test data, the error rate is then 30 % for the linear kernel, 35 % for the polynomial kernel, and 38.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 1.3.8 Table of results Table 1.8: Summary of the results of methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 LR score 0.4071 0.4000 Tree - discret. 0.3357 0.4667 Tree - score 0.3786 0.4000 Tree - Bbasis 0.3357 0.4500 RForest - discr 0.0071 0.4000 RForest - score 0.0429 0.4000 RForest - Bbasis 0.0071 0.3667 SVM linear - func 0.0500 0.2333 SVM poly - func 0.1143 0.2667 SVM rbf - func 0.0286 0.1333 SVM linear - discr 0.0571 0.1667 SVM poly - discr 0.0929 0.2167 SVM rbf - discr 0.0214 0.1167 SVM linear - PCA 0.4000 0.4000 SVM poly - PCA 0.4071 0.4000 SVM rbf - PCA 0.3643 0.4167 SVM linear - Bbasis 0.0643 0.0667 SVM poly - Bbasis 0.0429 0.1333 SVM rbf - Bbasis 0.1357 0.2000 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0500 0.1333 SVM poly - RKHS - radial 0.0143 0.2000 SVM rbf - RKHS - radial 0.0143 0.1333 SVM linear - RKHS - poly 0.1071 0.2167 SVM poly - RKHS - poly 0.0643 0.2000 SVM rbf - RKHS - poly 0.1000 0.2333 SVM linear - RKHS - linear 0.3643 0.3000 SVM poly - RKHS - linear 0.2071 0.3500 SVM rbf - RKHS - linear 0.3071 0.3833 1.4 Simulation Study In the entire previous section, we dealt with only one randomly generated dataset of functions from two classification classes, which we then randomly split into testing and training parts. After that, we evaluated the individual classifiers obtained using the considered methods based on testing and training errors. Since the generated data (and their division into two parts) can vary significantly with each repetition, the errors of individual classification algorithms will also vary significantly. Therefore, making any conclusions about the methods and comparing them with each other based on one generated dataset can be very misleading. For this reason, in this section, we will focus on repeating the entire previous procedure for different generated datasets. We will store the results in a table and finally calculate the average characteristics of the models across the individual repetitions. To ensure that our conclusions are sufficiently general, we will choose the number of repetitions \\(n_{sim} = 100\\). Code # Setting the random number generator set.seed(42) # Number of simulations n.sim &lt;- 100 ## List to store error values # The columns will be methods # The rows will be individual repetitions # The list has two items ... train and test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # Object to store optimal values of hyperparameters determined using CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Now we will repeat the entire previous part 100 times and store the error values in the list SIMULACE. In the data table CV_RESULTS, we will then store the values of optimal hyperparameters—for the \\(K\\) nearest neighbors method and for SVM, the dimension \\(d\\) in the case of projection onto the B-spline basis. We will also store all hyperparameter values for the SVM + RKHS method. Code # setting the seed for the random number generator set.seed(42) ## SIMULATION for(sim in 1:n.sim) { # number of generated observations for each class n &lt;- 100 # time vector equally spaced over the interval [0, 6] t &lt;- seq(0, 6, length = 51) # for Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # for Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # combining observations into one matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector to store GCV values for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the value of the minimum lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # splitting into test and training sets split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } ### 7.0) SVM for functional data # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-5, 0, length = 5) C.cv &lt;- 10^seq(0, 7, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (cost in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEARNI JADRO # model SVM clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gam.cv in gamma.cv) { # sestrojeni modelu clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # vytvorime vhodne objekty x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # presnost na trenovacich datech newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.1) Diskretizace intervalu # ktere hodnoty chceme uvazovat gamma.cv &lt;- 10^seq(-5, -2, length = 5) C.cv &lt;- 10^seq(0, 6, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent gamma.cv &lt;- 10^seq(-4, -1, length = 5) C.cv &lt;- 10^seq(-3, 4, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty gamma.cv &lt;- 10^seq(-4, -1, length = 5) C.cv &lt;- 10^seq(1, 4, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by =2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_03_cv.RData&#39;) Now we will calculate the average training and testing error rates for individual classification methods. Code # Add to the final table SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # Save the resulting values save(SIMULACE.df, file = &#39;RData/simulace_03_res_cv.RData&#39;) 1.4.1 Results Table 1.9: Summary results of the methods used on the simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error, \\(\\widehat{Err}_{test}\\) the testing error, \\(\\widehat{SD}_{train}\\) the estimate of the standard deviation of training errors, and \\(\\widehat{SD}_{test}\\) the estimate of the standard deviation of testing errors. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.3604 0.3917 0.0469 0.0686 LDA 0.3599 0.3950 0.0664 0.0935 QDA 0.3534 0.3953 0.0651 0.0897 LR_functional 0.0349 0.1083 0.0303 0.0513 LR_score 0.3595 0.3950 0.0664 0.0935 Tree_discr 0.2736 0.4250 0.0713 0.0743 Tree_score 0.3307 0.4217 0.0797 0.0978 Tree_Bbasis 0.2732 0.4222 0.0728 0.0770 RF_discr 0.0102 0.3487 0.0077 0.0663 RF_score 0.0519 0.4252 0.0176 0.0859 RF_Bbasis 0.0105 0.3395 0.0082 0.0624 SVM linear - func 0.0507 0.1218 0.0346 0.0503 SVM poly - func 0.0421 0.1910 0.0408 0.0498 SVM rbf - func 0.0508 0.1247 0.0275 0.0487 SVM linear - diskr 0.0451 0.1122 0.0305 0.0433 SVM poly - diskr 0.0483 0.1872 0.0442 0.0492 SVM rbf - diskr 0.0504 0.1278 0.0290 0.0495 SVM linear - PCA 0.3601 0.3970 0.0666 0.0982 SVM poly - PCA 0.3465 0.4177 0.0682 0.0960 SVM rbf - PCA 0.3419 0.3973 0.0637 0.0900 SVM linear - Bbasis 0.0442 0.0963 0.0255 0.0437 SVM poly - Bbasis 0.0472 0.1652 0.0360 0.0502 SVM rbf - Bbasis 0.0734 0.1443 0.0366 0.0492 SVM linear - projection 0.0395 0.0932 0.0208 0.0466 SVM poly - projection 0.0264 0.1005 0.0158 0.0441 SVM rbf - projection 0.0530 0.1035 0.0203 0.0413 SVM linear - RKHS - radial 0.0506 0.1325 0.0252 0.0460 SVM poly - RKHS - radial 0.0165 0.1598 0.0169 0.0442 SVM rbf - RKHS - radial 0.0453 0.1420 0.0184 0.0476 SVM linear - RKHS - poly 0.0759 0.1712 0.0349 0.0551 SVM poly - RKHS - poly 0.0331 0.1877 0.0204 0.0535 SVM rbf - RKHS - poly 0.0755 0.1777 0.0253 0.0581 SVM linear - RKHS - linear 0.2539 0.3873 0.0573 0.0721 SVM poly - RKHS - linear 0.1905 0.3743 0.0479 0.0714 SVM rbf - RKHS - linear 0.2516 0.3705 0.0357 0.0656 The table above lists all calculated characteristics. Standard deviations are also provided so that we can compare some stability or variability of individual methods. Finally, we can graphically display the calculated values from the simulation for individual classification methods using box plots, separately for testing and training error rates. Code # Set different names for classification methods methods_names &lt;- c( &#39;$K$ nearest neighbors&#39;, &#39;Linear discriminant analysis&#39;, &#39;Quadratic discriminant analysis&#39;, &#39;Functional logistic regression&#39;, &#39;Logistic regression with fPCA&#39;, &#39;Decision tree -- discretization&#39;, &#39;Decision tree -- fPCA&#39;, &#39;Decision tree -- basis coefficients&#39;, &#39;Random forest -- discretization&#39;, &#39;Random forest -- fPCA&#39;, &#39;Random forest -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # Alpha for box plots box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # Set different names for classification methods methods_names1 &lt;- c( &#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;functional LR&#39;, &#39;LR with fPCA&#39;, #&#39;Decision Tree -- Discretization&#39;, #&#39;Decision Tree -- fPCA&#39;, #&#39;Decision Tree -- Basis Coefficients&#39;, &#39;RF -- discretization&#39;, &#39;RF -- fPCA&#39;, &#39;RF -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;#, #&#39;RKHS (Linear SVR) $+$ SVM (linear)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (poly)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col1 &lt;- c(&#39;#4dd2ff&#39;, &#39;#00ace6&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#00bfff&#39;, rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 3)) # Alpha for box plots box_alpha1 &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, rep(c(0.9, 0.8, 0.7), 7)) #- 0.3 Code # For training data # tikz(file = &quot;figures/DP_sim_03_boxplot_train.tex&quot;, width = 10, height = 6) SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods2, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Figure 1.26: Box plots of training errors for 100 simulations for individual classification methods. Mean values are marked with black \\(+\\) symbols. Code # dev.off() Code # For test data sub_methods &lt;- methods[c(1:5, 9:32)] SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods)) |&gt; as.data.frame() |&gt; filter(method %in% sub_methods) |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_jitter(position = position_jitter(height = 0, width = 0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[test]) # y = &#39;$\\\\widehat{\\\\textnormal{Err}}_{test}$&#39; ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 45, hjust = 1)) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = 4, size = 2, color = &quot;black&quot;, alpha = 1) + scale_x_discrete(labels = methods_names1) + theme(plot.margin = unit(c(0.1, 0.1, 0.7, 0.5), &quot;cm&quot;)) + scale_fill_manual(values = box_col1) + scale_alpha_manual(values = box_alpha1) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Figure 1.27: Box plots of test errors for 100 simulations for individual classification methods. Mean values are marked with black \\(+\\) symbols. Code # ggsave(&quot;figures/sim_01_boxplot_test_final.tex&quot;, device = tikz, width = 7, height = 5) We would now like to formally test whether some classification methods are better than others based on the previous simulation on these data, or to show that we can consider them equally successful. Code # Check normality library(nortest) lillie.test(SIMULACE$test[, &#39;LR_functional&#39;])$p.value ## [1] 0.008072969 Code lillie.test(SIMULACE$test[, &#39;SVM linear - projection&#39;])$p.value ## [1] 0.0004162642 Given the violation of the normality assumption, we cannot use the classic paired t-test. Instead, we will utilize its nonparametric alternative - the paired Wilcoxon test. However, we must be careful with interpretation in this case. Code wilcox.test(SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.4297988 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.1274886 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.0001353731 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.002208014 Comparing SVM with discretization with functional SVM. Code wilcox.test(SIMULACE$test[, &#39;SVM linear - func&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.0003004829 Code wilcox.test(SIMULACE$test[, &#39;SVM poly - func&#39;], SIMULACE$test[, &#39;SVM poly - diskr&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.1656051 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - func&#39;], SIMULACE$test[, &#39;SVM rbf - diskr&#39;], alternative = &#39;two.sided&#39;, paired = TRUE)$p.value ## [1] 0.4878897 Now let’s look at the comparison between functional logistic regression and projection onto the B-spline basis. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;greater&#39;, paired = TRUE)$p.value ## [1] 0.001104007 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;greater&#39;, paired = TRUE)$p.value ## [1] 0.0471872 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - projection&#39;], alternative = &#39;greater&#39;, paired = TRUE)$p.value ## [1] 0.1611741 We see that the linear kernel has a statistically significantly lower error rate compared to functional logistic regression (\\(p\\)-value 0.001104). On the other hand, for the remaining SVM kernels, we could not demonstrate that the median of their test errors is smaller than the median of fLR. We might be interested in further comparisons. Code # wilcox.test(SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value # wilcox.test(SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value ## [1] 1.797302e-05 Code # wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - poly&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value ## [1] 9.443463e-08 Code # wilcox.test(SIMULACE$test[, &#39;SVM poly - diskr&#39;], SIMULACE$test[, &#39;SVM poly - Bbasis&#39;], alternative = &#39;greater&#39;, paired = TRUE)$p.value Now let’s compare the kernels for projection. Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value ## [1] 0.03742462 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - projection&#39;], SIMULACE$test[, &#39;SVM poly - projection&#39;], alternative = &#39;greater&#39;, paired = TRUE)$p.value ## [1] 0.2798017 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM rbf - projection&#39;], alternative = &#39;less&#39;, paired = TRUE)$p.value ## [1] 0.02379553 Finally, let’s look at which hyperparameter values were the most frequently chosen. Table 1.10: Medians of hyperparameter values for selected methods, for which some hyperparameter was determined using cross-validation. Median hyperparameter value KNN_K 4.0 nharm 1.0 LR_func_n_basis 13.0 SVM_d_Linear 15.5 SVM_d_Poly 13.0 SVM_d_Radial 11.0 SVM_RKHS_radial_gamma1 5.2 SVM_RKHS_radial_gamma2 3.2 SVM_RKHS_radial_gamma3 1.6 SVM_RKHS_radial_d1 19.0 SVM_RKHS_radial_d2 18.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 23.0 SVM_RKHS_poly_d2 31.0 SVM_RKHS_poly_d3 31.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 25.0 SVM_RKHS_linear_d3 25.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.28: Histograms of hyperparameter values for KNN, functional logistic regression, and a histogram for the number of principal components. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.29: Histograms of hyperparameter values for the SVM method with projection onto the B-spline basis. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.30: Histograms of hyperparameter values for RKHS + SVM with radial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.31: Histograms of hyperparameter values for RKHS + SVM with polynomial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.32: Histograms of hyperparameter values for RKHS + SVM with linear kernel. Muñoz, A. and González, J. (2010) Representing functional data using support vector machines, Pattern Recognition Letters, 31(6), pp. 511–516. doi:10.1016/j.patrec.2009.07.014.↩︎ "],["simulace3sigma.html", "Chapter 2 Dependency on the parameter \\(\\sigma^2\\) 2.1 Simulation of Functional Data 2.2 Smoothing the Observed Curves 2.3 Classification of Curves 2.4 Simulation Study", " Chapter 2 Dependency on the parameter \\(\\sigma^2\\) In this section, we will focus on the dependency of the results from the previous Chapter 1 on the value of \\(\\sigma^2\\), which defines the variance of the normal distribution from which we generate random errors around the generating curves (we can say that \\(\\sigma^2\\) carries information about the measurement error of a certain device, for example). We expect that with an increasing value of \\(\\sigma^2\\), the results of the individual methods will deteriorate, and thus classification will not be as successful. In the following section 3, we will then examine the dependency of the results on the value of \\(\\sigma^2_{shift}\\), that is, the variance of the normal distribution from which we generate the shift for the generated curves. 2.1 Simulation of Functional Data First, we will simulate the functions that we will later want to classify. For simplicity, we will consider two classification classes. For the simulation, we will first: choose appropriate functions, generate points from the chosen interval, which contain, for example, Gaussian noise, smooth the obtained discrete points into the form of a functional object using some suitable basis system. Through this procedure, we will obtain functional objects along with the value of the categorical variable \\(Y\\), which distinguishes the belonging to a classification class. Code # Load necessary packages library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Let us consider two classification classes, \\(Y \\in \\{0, 1\\}\\), with the same number of generated functions n for each class. First, we will define two functions, each for one class. We will consider the functions over the interval \\(I = [0, 6]\\). Now we will create functions using interpolation polynomials. First, we will define the points through which our curve should pass, and then we will interpolate them with a polynomial, which we will use to generate curves for classification. Code # Defining points for class 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # Defining points for class 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # Graph of points dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Class&#39;) Figure 1.1: Body defining interpolation polynomials. To calculate the interpolation polynomials, we will use the function poly.calc() from the polynom package. We will also define the functions poly.0() and poly.1(), which will calculate the values of the polynomials at a given point in the interval. We will use the function predict(), to which we will input the corresponding polynomial and the point at which we want to evaluate the polynomial. Code # Calculation of polynomials polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # Plotting polynomials xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Class&#39;) Figure 1.2: Illustration of two functions over the interval \\(I = [0, 6]\\), from which we generate observations from classes 0 and 1. Code # Generating functions for Y = 0 and Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Now we will create a function for generating random functions with added noise (or points on a predetermined grid) from the chosen generating function. The argument t denotes the vector of values at which we want to evaluate the functions, fun indicates the generating function, n is the number of functions, and sigma is the standard deviation \\(\\sigma\\) of the normal distribution \\(\\text{N}(\\mu, \\sigma^2)\\), from which we randomly generate Gaussian white noise with \\(\\mu = 0\\). To demonstrate the advantage of using methods that work with functional data, we will also add a random component to each simulated observation, which will represent the vertical shift of the entire function (parameter sigma_shift). This shift will be generated from a normal distribution with the parameter \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Now we can generate functions. In each of the two classes, we will consider 100 observations, thus n = 100. Code # number of generated observations for each class n &lt;- 100 # vector of time equidistant on the interval [0, 6] t &lt;- seq(0, 6, length = 51) # for Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # for Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (not smoothed) functions colored by class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 # number of curves to plot from each group DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 2.1: First 10 generated observations from each of the two classification classes. The observed data are not smoothed. 2.2 Smoothing the Observed Curves Now we will convert the observed discrete values (vectors of values) into functional objects that we will subsequently work with. We will again use B-spline basis for smoothing. We take the entire vector t as the knots, and we typically consider cubic splines, thus we choose (the implicit choice in R) norder = 4. We will penalize the second derivative of the functions. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalizing the 2nd derivative We will find an appropriate value of the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), that is, using generalized cross-validation. We will consider the value of \\(\\lambda\\) the same for both classification groups, since for test observations, we would not know in advance which value of \\(\\lambda\\), in the case of different choices for each class, we should select. Code # combining observations into one matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] For better visualization, we will plot the progression of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.5: The progression of \\(GCV(\\lambda)\\) for the chosen vector \\(\\boldsymbol\\lambda\\). The values on the x-axis are plotted on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is shown in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically represent the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.6: First 10 smoothed curves from each classification class. Let’s also visualize all curves, including the average, separately for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Figure 1.7: Plotting all smoothed observed curves, with colors distinguishing the curves by their classification class. The thick line represents the average for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 2.2: Plotting all smoothed observed curves, with colors distinguishing the curves by their classification class. The thick line represents the average for each class. A close-up view. 2.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for LR on scores library(nnet) # for LR on scores library(caret) library(rpart) # trees library(rattle) # graphics library(e1071) library(randomForest) # random forest To compare individual classifiers, we will divide the generated observations into two parts in a ratio of 70:30, specifically for training and testing (validation) purposes. The training part will be used in constructing the classifier, while the testing part will be used for calculating the classification error and potentially other characteristics of our model. The resulting classifiers can then be compared against each other based on their classification success according to these calculated characteristics. Code # division into testing and training parts split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, we will look at the representation of individual groups in the testing and training parts of the data. Code # absolute representation table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 2.3.1 \\(K\\) Nearest Neighbors Let’s start with a non-parametric classification method, the \\(K\\) nearest neighbors method. First, we will create the necessary objects so that we can further work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and look at its classification success. However, the last question remains how to choose the optimal number of neighbors \\(K\\). We could choose this number as such \\(K\\) at which the minimum error occurs on the training data. However, this could lead to model overfitting, so we will use cross-validation. Given the computational intensity and the size of the dataset, we will choose \\(k\\)-fold CV, for example, selecting the value \\(k = 10\\). Code # model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # model summary # plot(neighb.model$gcv, pch = 16) # plot of GCV dependence on the number of neighbors K # neighb.model$max.prob # maximum accuracy (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 5 Let’s repeat the previous procedure for the training data, which we will divide into \\(k\\) parts, thus repeating this part of the code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # divide training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # empty matrix where we will insert individual results # the columns will contain accuracy values for the given part of the training set # the rows will contain values for the given value of K CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # define the specific index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # go through each part ... repeat k times for(neighbour in neighbours) { # model for specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predictions on the validation part model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # accuracy on the validation part accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracy at the position for the given K and fold CV.results[neighbour, index] &lt;- accuracy } } # calculate average accuracies for individual K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) accuracy.opt.cv &lt;- max(CV.results) # CV.results We can see that the best value of the parameter \\(K\\) is 5 with an error calculated using 10-fold CV of 0.3429. For clarity, let’s also plot the validation error as a function of the number of neighbors \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation error&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 24 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.3: Dependence of validation error on the value of \\(K\\), i.e., the number of neighbors. Now that we know the optimal value of the parameter \\(K\\), we can assemble the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predictions model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # accuracy on the test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # error rate # 1 - accuracy Thus, we can see that the error rate of the model constructed using the \\(K\\) nearest neighbors method with the optimal choice of \\(K_{optimal}\\) equal to 5, which we determined through cross-validation, is 0.3571 on the training data and 0.3833 on the test data. To compare individual models, we can use both types of error rates; for clarity, we will store them in a table. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 2.3.2 Linear Discriminant Analysis As a second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied to functional data, we must first discretize it, which we will do using Functional Principal Component Analysis. We will then perform the classification algorithm on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components together explain at least 90% of the variability in the data. So let’s first perform Functional Principal Component Analysis and determine the number \\(p\\). Code # principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of principal components nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determine p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p principal components data.PCA.train$Y &lt;- factor(Y.train) # class membership In this particular case, we have taken the number of principal components to be $p = 2, which together explain 98.72% of the variability in the data. The first principal component explains 98.2% and the second 0.52% of the variability. We can graphically display the scores of the first two principal components, color-coded according to class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Figure 2.4: Scores of the first two principal components for the training data. Points are color-coded according to class membership. To determine the classification accuracy on the test data, we need to calculate the scores for the first 2 principal components for the test data. These scores can be determined using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text{ dt}, \\] where \\(\\mu(t)\\) is the mean function and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # calculate scores for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of the residual and the eigenfunctions rho (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training part of the data. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # accuracy on the training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on the test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training (41.43%) and test data (40%). To graphically illustrate the method, we can mark the decision boundary in the score plot of the first two principal components. This boundary will be calculated on a dense grid of points and displayed using the geom_contour() function. Code # add decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # case for p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # case for p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # case for p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 2.5: Scores of the first two principal components, color-coded by class membership. The decision boundary (line in the plane of the first two principal components) between classes constructed using LDA is indicated in black. We can see that the decision boundary is a line, a linear function in the 2D space, which we expected from LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.3 Quadratic Discriminant Analysis Next, we will construct a classifier using Quadratic Discriminant Analysis (QDA). This is an analogous case to LDA, with the difference that we now allow for different covariance matrices for each class of the normal distribution from which the corresponding scores are derived. This relaxed assumption of equal covariance matrices leads to a quadratic boundary between classes. In R, QDA can be performed similarly to LDA in the previous section. Therefore, we will calculate the scores for both training and test functions using Functional Principal Component Analysis, construct the classifier on the scores of the first \\(p\\) principal components, and use it to predict the class membership of the test curves to class \\(Y^* \\in \\{0, 1\\}\\). We do not need to perform functional PCA again, as we will utilize the results from the LDA section. We can proceed directly to constructing the classifier using the qda() function. Next, we will calculate the accuracy of the classifier on the test and training data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we have calculated the error rate of the classifier on the training (35.71 %) and the test data (40 %). For a graphical representation of the method, we can mark the decision boundary in the score plot of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, just as in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 2.6: Scores of the first two principal components, color-coded by classification group. The decision boundary (a parabola in the plane of the first two principal components) between classes constructed using QDA is marked in black. Note that the decision boundary between the classification classes is now a parabola. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.4 Logistic Regression Logistic regression can be performed in two ways. One can either use the functional analogue of classic logistic regression or the classic multivariate logistic regression, which we will perform on the scores of the first \\(p\\) principal components. 2.3.4.1 Functional Logistic Regression Analogous to the case of finite dimension input data, we consider the logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is the linear predictor taking values from the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function, and in the case of logistic regression, it is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\), and \\(\\pi(x)\\) is the conditional probability \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is a parametric function. Our goal is to estimate this parametric function. For functional logistic regression, we will use the fregre.glm() function from the fda.usc package. First, we will create suitable objects for constructing the classifier. Code # create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train$basis To estimate the parametric function \\(\\beta(t)\\), we need to express it in some basis representation, in our case, B-spline basis. However, we need to find a suitable number of basis functions. We could determine this based on the error rate on the training data; however, this data will favor selecting a large number of bases, leading to model overfitting. Let’s illustrate this with the following case. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\), we will train a model on the training data, determine the error rate on this data, and also calculate the error rate on the test data. Remember that we cannot use the same data for estimating the test error rate, as it would underestimate this error. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # bases for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # relationship f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into matrix pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s visualize the progression of both types of error in a graph depending on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error Rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 47 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.7: Dependence of test and training error rates on the number of basis functions for \\(\\beta\\). The red point indicates the optimal number \\(n_{optimal}\\) chosen as the minimum test error rate, the black line depicts the test error, and the blue dashed line shows the progression of training error rate. We see that as the number of bases for \\(\\beta(t)\\) increases, the training error rate (blue line) tends to decrease, leading us to select large values of \\(n_{basis}\\) based on it. Conversely, the optimal choice based on the test error rate is \\(n\\) equal to 29, which is significantly smaller than 50. Meanwhile, as \\(n\\) increases, the test error increases, indicating model overfitting. For the above reasons, to determine the optimal number of basis functions for \\(\\beta(t)\\), we will utilize 10-fold cross-validation. We will consider a maximum number of basis functions of 25, as we observed that above this value, overfitting occurs. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # split the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that do not change during the loop # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline bases basis1 &lt;- X.train$basis # relationship f &lt;- Y ~ x # bases for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to store individual results # in columns, there will be accuracy values for each part of the training set # in rows, there will be values for the given number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now we have everything prepared to calculate the error on each of the ten subsets of the training set. Next, we will determine the average and take the argument of the minimum validation error as the optimal \\(n\\). Code for (index in 1:k_cv) { # define the specific index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # bases for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on validation part newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # calculate average accuracy for each n over folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Let’s also plot the validation error course with the optimal value of \\(n_{optimal}\\) equal to 11 and validation error 0.072 highlighted. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation error&#39;) + scale_x_continuous(breaks = n.basis) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 22 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.8: Dependency of validation error on the value of \\(n_{basis}\\), i.e., on the number of bases. We can now define the final model using functional logistic regression, choosing the B-spline basis for \\(\\beta(t)\\) with 11 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We calculated the training error rate (which is 3.57 %) and the test error rate (which is 8.33 %). To get a better understanding, we can also plot the estimated probabilities of belonging to the classification class \\(Y = 1\\) based on the values of the linear predictor in the training data. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear predictor&#39;, y = &#39;Estimated probabilities Pr(Y = 1|X = x)&#39;, colour = &#39;Class&#39;) Figure 1.15: Dependence of estimated probabilities on the values of the linear predictor. Points are color-coded according to class membership. We can also visualize the estimated parametric function \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Figure 2.9: Plot of the estimate of the parametric function \\(\\beta(t), t \\in [0, 6]\\). We will again add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5 Random Forests The classifier built using the random forest method consists of constructing several individual decision trees, which are then combined to create a common classifier (by a common “vote”). Just like with decision trees, we have several options for what data (finite-dimensional) we will use to construct the model. We will again consider the three approaches discussed above. The data files with the respective quantities for all three approaches are already prepared from the previous section, so we can directly build the models, calculate the characteristics of the classifier, and add the results to the summary table. 2.3.5.1 Discretization of the Interval In the first case, we evaluate the functions at a given grid of points in the interval \\(I = [0, 6]\\). Code grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for row-wise functions grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Code # model construction clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is 0 % and on the test data is 38.33 %. Code Res &lt;- data.frame(model = &#39;RForest - discretization&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.2 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 2 principal components. Code # model construction clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate on the training data is thus 3.57 %, and on the test data, it is 41.67 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.5.3 B-Spline Coefficients Finally, we will use the representation of functions through B-spline basis. First, let’s define the necessary data files with coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # test dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Code # model construction clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 0 %, and on the test data, it is 41.67 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6 Support Vector Machines Let’s define the data for the next methods. Code # sequence of points where we evaluate the function t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for row functions grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() First, let’s define the necessary data files with coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # test dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now, let’s look at the classification of our simulated curves using Support Vector Machines (SVM). The advantage of this classification method is its computational efficiency, as it uses only a few (often very few) observations to define the boundary curve between classes. 2.3.6.1 Interval Discretization Let’s start by applying the Support Vector Machines method directly to the discretized data (evaluating the function on a grid of points on the interval \\(I = [0, 6]\\)), considering all three previously mentioned kernel functions. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = TRUE) # split into test and training parts X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() We estimated the parameters for each kernel using cross-validation on one generated dataset. We will use these values of \\(C\\), \\(d\\), and \\(\\gamma\\) for all datasets in this simulation. Code # model construction clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is 5.71 % for the linear kernel, 2.14 % for the polynomial kernel, and 3.57 % for the radial kernel. On the test data, the error rate of the method is 16.67 % for the linear kernel, 16.67 % for the polynomial kernel, and 13.33 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.2 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 2 principal components. Code # model construction clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the principal component scores on the training data is 44.29 % for the linear kernel, 37.86 % for the polynomial kernel, and 37.14 % for the radial kernel. On the test data, the error rate of the method is 48.33 % for the linear kernel, 43.33 % for the polynomial kernel, and 40 % for the radial kernel. For the graphical representation of the method, we can mark the decision boundary on the plot of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, as we did in previous cases when plotting the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 2.10: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line or curves in the plane of the first two principal components) between classes constructed using the SVM method is marked in black. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.3 B-spline Coefficients Finally, we will use the representation of functions through the B-spline basis. Code # model construction clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the basis coefficients on the training data is therefore 4.29 % for the linear kernel, 4.29 % for the polynomial kernel, and 7.14 % for the Gaussian kernel. For the test data, the error rate of the method is 10 % for the linear kernel, 11.67 % for the polynomial kernel, and 15 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.4 Projection onto B-spline Basis Another way to apply the classical SVM method to functional data is to project the original data onto a \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal{H}\\), which we denote as \\(V_d\\). Let’s assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation $ P_{V_d} $ as the orthogonal projection onto the subspace $ V_d $, which can be expressed as \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] We can then use the coefficients from the orthogonal projection for classification, applying standard SVM to the vectors $ ( x, 1 , , x, d )^$. By utilizing this transformation, we define a new, so-called adapted kernel, composed of the orthogonal projection $ P{V_d} $ and the kernel function of the standard support vector machine. Thus, we have the (adapted) kernel $ Q(x_i, x_j) = K(P{V_d}(x_i), P_{V_d}(x_j)) $. This represents a dimensionality reduction method, which we can refer to as filtering. For the projection itself, we will use the project.basis() function from the fda library in R. The input will be a matrix of original discrete (non-smoothed) data, the values over which we measure the values in the original data matrix, and the basis object onto which we want to project the data. We will choose to project onto a B-spline basis because using a Fourier basis is not suitable for our non-periodic data. Another option is to use a wavelet basis. The dimension $ d $ can be chosen based on some prior expert knowledge or through cross-validation. In our case, we will determine the optimal dimension of the subspace $ V_d $ using $ k $-fold cross-validation (we choose $ k n $ due to the computational intensity of the method, often selecting $ k = 5 $ or $ k = 10 $). We require B-splines of order 4, and the number of basis functions is determined by the relation \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where $ n_{breaks} $ is the number of knots and $ n_{order} = 4 $. The minimum dimension is therefore (for $ n_{breaks} = 1 $) $ n_{basis} = 3 $, and the maximum (for $ n_{breaks} = 51 $, corresponding to the number of original discrete data) $ n_{basis} = 53 $. However, in R, the value of $ n_{basis} $ must be at least $ n_{order} = 4 $, and for large values of $ n_{basis} $, overfitting of the model occurs, so we choose a maximum $ n_{basis} $ of a smaller number, say 43. Code k_cv &lt;- 10 # k-fold CV # values for the B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # all dimensions we want to test # split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # list with three components ... matrices for each kernel -&gt; linear, poly, radial # empty matrix to hold results # columns will have accuracy values for each part of the training set # rows will have values for each dimension value CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # projection of discrete data onto the B-spline basis of dimension d Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # split into training and test data for CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # definition of test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # model construction clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # accuracy on validation data ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for the given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.04306319 ## poly 11 0.07699176 ## radial 10 0.06449176 We see that the best value for the parameter \\(d\\) is 11 for the linear kernel, with an error rate calculated using 10-fold CV of 0.0431, 11 for the polynomial kernel with an error rate of 0.077, and 10 for the radial kernel with an error rate of 0.0645. For clarity, let’s plot the validation error rates as a function of the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation error&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 2.11: Dependency of validation error on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The optimal values of dimension \\(V_d\\) for individual kernel functions are marked with black points. Now we can train individual classifiers on all training data and look at their performance on the test data. For each kernel function, we choose the dimension of the subspace onto which we project based on the results of cross-validation. In the variable Projection, we have stored the matrix of coefficients from the orthogonal projection, that is, \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data table to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Project discrete data onto the B-spline basis Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # Split into training and test data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is therefore 4.29 % for the linear kernel, 3.57 % for the polynomial kernel, and 5 % for the Gaussian kernel. On the test data, the error rates are 10 % for the linear kernel, 8.33 % for the polynomial kernel, and 10 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.6.5 RKHS + SVM 2.3.6.5.0.1 Gaussian Kernel Code # remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # also add test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s compute the matrix \\(K_S\\) and its eigenvalues and corresponding eigenvectors. Code # compute matrix K gamma &lt;- 0.1 # fixed value of gamma, optimal determined using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # determine eigenvalues and vectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To compute the coefficients in the representation of curves, that is, to compute the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we still need the coefficients from SVM. Unlike the classification problem, we are now solving a regression problem, as we are trying to express our observed curves in some (chosen by us using the kernel \\(K\\)) basis. Therefore, we will use the Support Vector Regression method, from which we will subsequently obtain the coefficients \\(\\alpha_{il}\\). Code # determining coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # determining alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } Now we can compute the representations of the individual curves. First, let’s take \\(\\hat d\\) as the full dimension, i.e., \\(\\hat d = m ={}\\) 101, and then determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # determining the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # computation of the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have stored the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) for each curve in the columns of the matrix Lambda.RKHS. We will now use these vectors as the representation of the given curves and classify the data according to this discretization. Code # splitting into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # preparing a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # iterating over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # building models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # storing results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 2.1: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 We can see that the model classifies the training data very well for all three kernels, while its performance on the test data is not good at all. It is evident that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values of \\(\\gamma\\) and \\(d\\). Code # splitting training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hyperparameter values that we will iterate over dimensions &lt;- 3:40 # reasonable range of values for d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # list with three components ... array for individual kernels -&gt; linear, poly, radial # empty matrix to store individual results # columns will hold accuracy values for the respective gamma # rows will hold values for the respective folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # actual CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # iterating over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # iterate over folds for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] # split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # building models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # storing results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # store accuracies at positions for the respective d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each method across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 2.2: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 10 13.8950 0.0630 linear poly 19 22.7585 0.0912 polynomial radial 40 0.4394 0.1016 radial We see that the optimal parameter value is \\(d={}\\) 10 and \\(\\gamma={}\\) 13.895 for the linear kernel, with an error rate computed using 10-fold CV of 0.063. For the polynomial kernel, \\(d={}\\) 19 and \\(\\gamma={}\\) 22.7585 yield an error rate of 0.0912, and for the radial kernel, \\(d={}\\) 40 and \\(\\gamma={}\\) 0.4394 yield an error rate of 0.1016. For interest, let’s plot the validation error as a function of dimension \\(d\\) and the hyperparameter \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 2.12: Dependence of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. Since we have already found the optimal hyperparameter values, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate over the different kernels for (kernel_number in 1:3) { # Calculate the matrix K gamma &lt;- gamma.opt[kernel_number] # gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Model construction clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 2.3: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0571 0.1667 SVM rbf - RKHS - radial 0.0143 0.1500 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is therefore 5.71 % on the training data for the linear kernel, 5.71 % for the polynomial kernel, and 1.43 % for the Gaussian kernel. On the test data, the error rate of the method is 13.33 % for the linear kernel, 16.67 % for the polynomial kernel, and 15 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 2.3.7 Results Table Table 2.4: Summary of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 RForest - discretization 0.0000 0.3833 RForest - score 0.0357 0.4167 RForest - Bbasis 0.0000 0.4167 SVM linear - discr 0.0571 0.1667 SVM poly - discr 0.0214 0.1667 SVM rbf - discr 0.0357 0.1333 SVM linear - PCA 0.4429 0.4833 SVM poly - PCA 0.3786 0.4333 SVM rbf - PCA 0.3714 0.4000 SVM linear - Bbasis 0.0429 0.1000 SVM poly - Bbasis 0.0429 0.1167 SVM rbf - Bbasis 0.0714 0.1500 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0571 0.1333 SVM poly - RKHS - radial 0.0571 0.1667 SVM rbf - RKHS - radial 0.0143 0.1500 2.4 Simulation Study In the entire preceding section, we dealt with only one randomly generated dataset of functions from two classification classes, which we then randomly split into test and training parts. We evaluated the individual classifiers obtained using the considered methods based on test and training errors. Since the generated data (and their division into two parts) can vary significantly with each repetition, the errors of individual classification algorithms will also vary significantly. Therefore, drawing any conclusions about the methods and comparing them with each other based on a single generated dataset can be very misleading. For this reason, in this section, we will focus on repeating the entire previous procedure for various generated datasets. We will store the results in a table and ultimately calculate the average characteristics of the models across the individual repetitions. To make our conclusions sufficiently general, we will choose the number of repetitions to be \\(n_{sim} = 25\\). Now, we will repeat the entire previous section \\(n.sim\\) times and store the error values in the object SIMUL_params, while varying the value of the parameter \\(\\sigma\\) and observing how the results of the selected classification methods change with this value. Code # set seed for random number generation set.seed(42) # number of simulations for each value of the simulation parameter n.sim &lt;- 25 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39; ) # vector of standard deviations defining the variance around the generating curves sigma_vector &lt;- seq(0.1, 5, length = 30) # resulting object to store simulation results SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(sigma_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), sigma = paste0(sigma_vector))) for (n_sigma in 1:length(sigma_vector)) { ## list to store error values # columns will represent methods # rows will represent individual repetitions # the list has two entries ... train and test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # object to store optimal hyperparameter values determined using CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULATIONS for(sim in 1:n.sim) { # number of generated observations for each class n &lt;- 100 # time vector evenly spaced on the interval [0, 6] t &lt;- seq(0, 6, length = 51) # for Y = 0 X0 &lt;- generate_values(t, funkce_0, n, sigma_vector[n_sigma], 2) # for Y = 1 X1 &lt;- generate_values(t, funkce_1, n, sigma_vector[n_sigma], 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # combine observations into one matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -5, to = 3, length.out = 25) # lambda vector gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector to store GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # split into test and training parts split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K-Nearest Neighbors k_cv &lt;- 5 # k-fold CV neighbours &lt;- 1:15 # number of neighbors # split the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # define the index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max # k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 # length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # # jadro a jadrova matice ... Gaussovske s parametrem gamma # Gauss.kernel &lt;- function(x, y, gamma) { # return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) # } # # Kernel.RKHS &lt;- function(x, gamma) { # K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) # for(i in 1:nrow(K)) { # for(j in 1:ncol(K)) { # K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) # } # } # return(K) # } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(10, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt # CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_sigma, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_sigma] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_sigma_03_rf_cv.RData&#39;) 2.4.1 Graphical Output Let’s look at the dependence of simulated results on the value of the standard deviation parameter of the normal distribution for measurement errors. First, let’s plot the data for training errors. Code # for training data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;bottom&#39;) Figure 2.13: Graph of the dependence of training error for individual classification methods on the value of the parameter \\(\\sigma_{x}\\), which defines the standard deviation for generating random deviations around the generating curves. Next, for test errors. Code # for test data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[x]), y = expression(widehat(Err)[test]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;bottom&#39;) Figure 2.14: Graph of the dependence of test error for individual classification methods on the value of the parameter \\(\\sigma_{x}\\), which defines the standard deviation for generating random deviations around the generating curves. Since the results are affected by random deviations, and increasing the number of repetitions n.sim would be computationally intensive, let’s now smooth the curves of the average test and training errors. Since the error is a non-negative quantity, we will smooth the curves with this in mind. Additionally, to ensure that the smoothed curves closely follow the observed discrete values, we consider a different weight for small and large values of \\(\\sigma\\). Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- sigma_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.opt &lt;- 1e-4 curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, wtvec = c(rep(1000, 5), rep(8, 5), rep(1, 20))) # smooth.pos Code XXfd &lt;- BSmooth$Wfdobj # Wfdobj fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd evalarg = seq(min(sigma_vector), max(sigma_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) To enhance clarity, we will plot the graphs only for a subset of methods. Code methods_names &lt;- c( &#39;KNN&#39;, &#39;fLR&#39;, &#39;RF - coefs&#39;, &#39;SVM (linear) - discr&#39;, &#39;SVM (linear) - fPCA&#39;, &#39;SVM (linear) - coefs&#39;, &#39;SVM (linear) - projec&#39;, &#39;RKHS (radial) + SVM (linear)&#39; ) Code # for test data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;sigma&#39;, y = &#39;Test Error&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + geom_line(data = df_plot_smooth, aes(x = sigma, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) Figure 2.15: Graph of the dependence of test error for individual classification methods on the value of the parameter \\(\\sigma_{x}\\). Now we will do the same for test error. Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) # methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- sigma_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vector of lambdas # gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for storing GCV # for(index in 1:length(lambda.vect)) { # curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # smoothing # gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves # } # # GCV &lt;- data.frame( # lambda = round(log10(lambda.vect), 3), # GCV = gcv # ) # # # find the minimum value # lambda.opt &lt;- lambda.vect[which.min(gcv)] # # curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) # BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # smooth.pos # XXfd &lt;- BSmooth$fd # Wfdobj # # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd # evalarg = seq(min(sigma_vector), max(sigma_vector), # length = 101)) # df_plot_smooth &lt;- data.frame( # method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), # value = c(fdobjSmootheval), # sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) # ) |&gt; # mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) ## for positive smoothing # find the minimum value lambda.opt &lt;- 3e-4 curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.pos(breaks, Dat, curv.fdPar, dbglev = 0, # wtvec = c(rep(1000, 5), rep(100, 5), rep(1, 20)) wtvec = c(rep(1000, 5), seq(100, 10, length = 5), rep(1, 20)) ) # smooth.pos Code XXfd &lt;- BSmooth$Wfdobj # Wfdobj fdobjSmootheval &lt;- eval.posfd(Wfdobj = XXfd, # eval.posfd evalarg = seq(min(sigma_vector), max(sigma_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), sigma = rep(seq(min(sigma_vector), max(sigma_vector), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Code # for test data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(sigma_vector), names_to = &#39;sigma&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), sigma = as.numeric(sigma)) |&gt; # filter(method %in% c(&#39;KNN&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, # &#39;RF_discr&#39;, #&#39;RF_score&#39;, &#39;RF_Bbasis&#39;, # &#39;SVM linear - diskr&#39;,# &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, # # &#39;SVM linear - PCA&#39;,# &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, # &#39;SVM linear - Bbasis&#39;,# &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, # &#39;SVM linear - projection&#39;,# &#39;SVM poly - projection&#39;, # # &#39;SVM rbf - projection&#39;, # &#39;SVM linear - RKHS - radial&#39;#, &#39;SVM poly - RKHS - radial&#39;, # # &#39;SVM rbf - RKHS - radial&#39; # )) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = sigma, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + # facet_wrap(~factor(param, c(&#39;sigma&#39;, &#39;shift&#39;))) + labs(x = &#39;sigma&#39;, y = &#39;Test Error&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.5, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank(), strip.background = element_blank(), strip.text.x = element_blank()) + # guides(colour=guide_legend(direction = &#39;vertical&#39;), # linetype=guide_legend(direction = &#39;vertical&#39;), # shape=guide_legend(direction = &#39;vertical&#39;)) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = sigma, y = value, col = method, linetype = method), linewidth = 1.5) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.4, alpha = 0.6)), linetype = guide_legend(override.aes = list(linewidth = 0.7))) Figure 2.16: Graph of the dependence of test error for individual classification methods on the parameter \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/sigmashiftresults.tex&quot;, device = tikz, width = 8, height = 5) Comments on the graphs can be found in the thesis. We only note that as the value of \\(\\sigma\\) increases, the test error also increases for all methods. Functional logistic regression and projection onto the B-spline basis combined with the SVM method with a linear kernel perform the best. "],["simulace3shift.html", "Chapter 3 Dependence on parameter \\(\\sigma_{shift}\\) 3.1 Simulation of Functional Data 3.2 Smoothing the Observed Curves 3.3 Classification of Curves 3.4 Simulation Study", " Chapter 3 Dependence on parameter \\(\\sigma_{shift}\\) In this section, we will focus on the dependence of the results from section 1 on the value of \\(\\sigma^2_{shift}\\), which defines the variance of the normal distribution from which we generate the shift for the generated curves. We expect that as the value of \\(\\sigma^2_{shift}\\) increases, the results of the individual methods will deteriorate, and thus the classification will not be as successful. We assume that methods utilizing the functional nature of the data will be more successful compared to classical methods as the value of \\(\\sigma^2_{shift}\\) increases. In the previous section 2, we examined the dependence of results on the value of \\(\\sigma^2\\), that is, on the variance of the normal distribution from which we generate random errors around the generating curves. 3.1 Simulation of Functional Data First, we will simulate functions that we will subsequently want to classify. For simplicity, we will consider two classification classes. For the simulation, we will first: choose suitable functions, generate points from the chosen interval, which include, for example, Gaussian noise, smooth the obtained discrete points into the form of a functional object using some suitable basis system. With this approach, we will obtain functional objects along with the value of the categorical variable \\(Y\\), which distinguishes the membership in the classification class. Code # Load necessary packages library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Let us consider two classification classes, \\(Y \\in \\{0, 1\\}\\), with the same number of n generated functions for each class. First, we define two functions, each corresponding to one class. The functions will be considered on the interval \\(I = [0, 6]\\). Now we will create the functions using interpolation polynomials. First, we define the points through which our curve will pass and subsequently fit an interpolation polynomial through these points, which we will use for generating curves for classification. Code # Defining points for class 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # Defining points for class 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # Graph of the points dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Class&#39;) Figure 1.1: Points that define the interpolation polynomials. To calculate the interpolation polynomials, we will use the poly.calc() function from the polynom library. We will also define functions poly.0() and poly.1(), which will compute the polynomial values at a given point in the interval. To create them, we will use the predict() function, to which we will input the respective polynomial and the point at which we want to evaluate the polynomial. Code # Calculation of the polynomials polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # Plotting the polynomial xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Class&#39;) Figure 1.2: Representation of two functions over the interval \\(I = [0, 6]\\), from which we generate observations from classes 0 and 1. Code # Generating functions for Y = 0 and Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Now we will create a function to generate random functions with added noise (or points on a predetermined grid) from the chosen generating function. The argument t represents the vector of values at which we want to evaluate the given functions, fun denotes the generating function, n is the number of functions, and sigma is the standard deviation \\(\\sigma\\) of the normal distribution \\(\\text{N}(\\mu, \\sigma^2)\\), from which we randomly generate Gaussian white noise with \\(\\mu = 0\\). To demonstrate the advantage of using methods that work with functional data, we will also add a random term during generation to each simulated observation, which will serve as the vertical shift of the entire function (parameter sigma_shift). This shift will be generated from a normal distribution with the parameter \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Now we can generate the functions. In each of the two classes, we will consider 100 observations, so n = 100. Code # Number of generated observations for each class n &lt;- 100 # Vector of time evenly spaced over the interval [0, 6] t &lt;- seq(0, 6, length = 51) # For Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # For Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (not yet smoothed) functions color-coded by class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 # Number of curves we want to plot from each group DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 2.1: First 10 generated observations from each of the two classification classes. The observed data are not smoothed. 3.2 Smoothing the Observed Curves Now we will convert the observed discrete values (vectors of values) into functional objects, with which we will subsequently work. We will again use the B-spline basis for smoothing. We will take the entire vector t as the knots and typically consider cubic splines, so we choose (the implicit choice in R) norder = 4. We will penalize the second derivative of the functions. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # Penalizing the 2nd derivative We will find a suitable value for the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), that is, through generalized cross-validation. We will consider the value of \\(\\lambda\\) to be the same for both classification groups, as for test observations we would not know in advance which value of $, in the case of different choices for each class, we should choose. Code # Combining observations into one matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # Vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # Empty vector to store GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # Smoothing gcv[index] &lt;- mean(BSmooth$gcv) # Average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # Finding the value of the minimum lambda.opt &lt;- lambda.vect[which.min(gcv)] For better visualization, we will plot the progression of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.5: The progression of \\(GCV(\\lambda)\\) for the selected vector \\(\\boldsymbol\\lambda\\). The values on the x-axis are plotted on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is shown in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically represent the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.6: The first 10 smoothed curves from each classification class. Let’s also display all curves including the average separately for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Figure 1.7: Plot of all smoothed observed curves, colored according to their classification class. The thick line represents the mean for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 2.2: Plot of all smoothed observed curves, colored according to their classification class. The thick line represents the mean for each class. Zoomed-in view. 3.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training sets library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for LR on scores library(nnet) # for LR on scores library(caret) library(rpart) # for decision trees library(rattle) # for visualization library(e1071) library(randomForest) # for random forests To compare the individual classifiers, we will split the generated observation set into two parts in a 70:30 ratio, specifically for training and test (validation) sets. The training set will be used for constructing the classifier, while the test set will be used for calculating classification error and potentially other characteristics of our model. The resulting classifiers can then be compared based on these calculated characteristics in terms of their classification success. Code # Splitting into test and training sets split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, we will examine the representation of individual groups in the test and training parts of the data. Code # Absolute representation table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # Relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 3.3.1 \\(K\\) Nearest Neighbors We will start with a non-parametric classification method, the \\(K\\) nearest neighbors method. First, we will create the necessary objects so that we can further work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and examine its classification success. The last question remains how to choose the optimal number of neighbors \\(K\\). We could select \\(K\\) that minimizes the error rate on the training data. However, this could lead to overfitting the model; therefore, we will use cross-validation. Given the computational complexity and the size of the dataset, we will choose \\(k\\)-fold CV, for instance, with \\(k = 10\\). Code # Model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # summary of the model # plot(neighb.model$gcv, pch = 16) # plot the dependence of GCV on the number of neighbors K # neighb.model$max.prob # maximum accuracy (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 5 Let’s follow the previous procedure for the training data, which we will split into \\(k\\) parts and repeat this code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # Split training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # Empty matrix to store individual results # Columns will contain accuracy values for the given part of the training set # Rows will contain values for each neighbor value K CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # Define the specific index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # Loop through each part ... repeat k times for(neighbour in neighbours) { # Model for a specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # Prediction on the validation part model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # Accuracy on the validation part accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy at the position for given K and fold CV.results[neighbour, index] &lt;- accuracy } } # Calculate average accuracy for each K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) accuracy.opt.cv &lt;- max(CV.results) # CV.results We can see that the optimal parameter \\(K\\) is 5 with an error rate calculated using 10-fold CV of 0.3429. For clarity, let’s also plot the validation error as a function of the number of neighbors \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation Error&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 24 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.3: Dependence of validation error on the value of \\(K\\), i.e., on the number of neighbors. Now that we know the optimal value of the parameter \\(K\\), we can build the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # Predictions model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # Accuracy on test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # Error rate # 1 - accuracy Thus, we see that the error rate of the model constructed using the \\(K\\) nearest neighbors method with the optimal choice \\(K_{optimal}\\) equal to 5, which we determined through cross-validation, is 0.3571 on the training data and 0.3833 on the test data. To compare the individual models, we can use both types of error rates. For clarity, we will store them in a table. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 3.3.2 Linear Discriminant Analysis As the second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied directly to functional data, we first need to discretize it using Functional Principal Component Analysis (FPCA). The classification algorithm will then be performed on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components together explain at least 90% of the variability in the data. Let’s first perform the functional principal component analysis and determine the number \\(p\\). Code # Principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of principal components nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determine p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p principal components data.PCA.train$Y &lt;- factor(Y.train) # membership to classes In this specific case, we have taken the number of principal components as \\(p\\) = 2, which together explain 98.72 % of the variability in the data. The first principal component explains 98.2 % and the second 0.52 % of the variability. We can graphically display the scores of the first two principal components, color-coded by classification class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Figure 2.4: Scores of the first two principal components for the training data. Points are color-coded based on class membership. To determine the classification accuracy on the test data, we need to compute the scores for the first 2 principal components for the test data. These scores can be calculated using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] where \\(\\mu(t)\\) is the mean function (average function) and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # Calculate scores for the test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of the residual and eigenfunctions rho (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct a classifier on the training portion of the data. Code # Model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # Accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (41.43 %) as well as on the test data (40 %). To visually illustrate the method, we can mark the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function. Code # Add decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # if p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # if p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # if p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # Add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 2.5: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line in the plane of the first two principal components) between the classes constructed using LDA is shown in black. We observe that the decision boundary is a line, a linear function in 2D space, which we expected from LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.3 Quadratic Discriminant Analysis Next, let’s construct a classifier using Quadratic Discriminant Analysis (QDA). This is an analogous case to LDA, with the difference that we now allow for different covariance matrices of the normal distribution for each class from which the corresponding scores originate. This dropped assumption of equal covariance matrices leads to a quadratic boundary between classes. In R, QDA is performed similarly to LDA in the previous section, meaning we would again calculate scores for both training and test functions using functional principal component analysis. We will construct the classifier on the scores of the first \\(p\\) principal components and use it to predict the class membership of the test curves as \\(Y^* \\in \\{0, 1\\}\\). We do not need to perform functional PCA; we will use the results from the LDA section. We can now proceed directly to constructing the classifier using the qda() function. Subsequently, we will calculate the accuracy of the classifier on the test and training data. Code # Model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # Accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we have calculated the error rate of the classifier on the training data (35.71 %) as well as on the test data (40 %). To visually illustrate the method, we can mark the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, just like in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 2.6: Scores of the first two principal components, color-coded according to class membership. The decision boundary (parabola in the plane of the first two principal components) between the classes constructed using QDA is shown in black. Note that the decision boundary between classification classes is now a parabola. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.4 Logistic Regression Logistic regression can be performed in two ways: 1. Using the functional analogue of classical logistic regression, 2. Using classical multidimensional logistic regression applied to the scores of the first \\(p\\) principal components. 3.3.4.1 Functional Logistic Regression Analogous to the case of finite-dimensional input data, we consider a logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text{ d} t, \\] where \\(\\eta(x)\\) is a linear predictor taking values in the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is a link function (in the case of logistic regression, this is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\)), and \\(\\pi(x)\\) is the conditional probability: \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text{e}^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text{ d} t}}{1 + \\text{e}^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text{ d} t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is a parameterized function. Our goal is to estimate this parameterized function. For functional logistic regression, we will use the fregre.glm() function from the fda.usc package. First, we will create appropriate objects for the construction of the classifier. Code # Create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # Points at which functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train$basis To estimate the parameterized function \\(\\beta(t)\\), we need to express it in some basis representation, in our case, a B-spline basis. However, we need to find an appropriate number of basis functions. This could be determined based on the error rate on the training data, but this data would favor the selection of a large number of bases, leading to model overfitting. Let’s illustrate this with the following case. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\), we will train a model on the training data, determine the error rate on them, and also calculate the error rate on the test data. Remember, we cannot use the same data to estimate the test error rate, as this would underestimate it. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # Basis for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # Relationship f &lt;- Y ~ x # Basis for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # Binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store in matrix pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s illustrate the relationship between both types of error rates in a graph, depending on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error Rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 47 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.7: Dependence of test and training error rates on the number of basis functions for \\(\\beta\\). The red point indicates the optimal number \\(n_{optimal}\\) chosen as the minimum of the test error rate, the black line represents the test error rate, and the blue dashed line shows the trend of the training error rate. As we can see, as the number of basis functions for \\(\\beta(t)\\) increases, the training error rate (blue line) tends to decrease, suggesting that we would choose large values of \\(n_{basis}\\) based on this. Conversely, the optimal choice based on the test error rate is \\(n = 29\\), which is significantly smaller than 50. In contrast, as \\(n\\) increases, the test error rate rises, indicating model overfitting. For these reasons, we will use 10-fold cross-validation to determine the optimal number of basis functions for \\(\\beta(t)\\). The maximum number of basis functions considered will be 25, as we observed earlier that exceeding this number leads to model overfitting. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # Split the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # Unchanging elements during the loop # Points at which functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline basis basis1 &lt;- X.train$basis # Relationship f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Empty matrix to store individual results # Columns will contain accuracy values for each part of the training set # Rows will contain values for the given number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now we are ready to calculate the error rate on each of the ten subsets of the training set. Next, we will determine the average and take the argument of the minimum validation error rate as optimal \\(n\\). Code for (index in 1:k_cv) { # Define the given index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # Basis for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # Binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Accuracy on validation part newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store in matrix CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # Calculate the average accuracies for each n over folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Let’s also plot the validation error rate along with the highlighted optimal value \\(n_{optimal}\\) as 11 with validation error rate 0.072. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = n.basis) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 22 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 2.8: Dependence of validation error rate on the value of \\(n_{basis}\\), that is, on the number of bases. Now we can define the final model using functional logistic regression, choosing a B-spline basis for \\(\\beta(t)\\) with 11 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # Bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # Input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # Binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the training error rate (which is 3.57 %) and the test error rate (which is 8.33 %). To better visualize the estimated probabilities of belonging to the classification class \\(Y = 1\\) on the training data in relation to the values of the linear predictor, we can plot: Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probabilities Pr(Y = 1|X = x)&#39;, colour = &#39;Class&#39;) Figure 1.15: Dependence of estimated probabilities on the values of the linear predictor. Points are color-coded according to class membership. We can also visualize the estimated parametric function \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Figure 2.9: Plot of the estimated parametric function \\(\\beta(t), t \\in [0, 6]\\). Finally, we will summarize the results in a summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 3.3.5 Support Vector Machines We will define data for the next methods. Code # Sequence of points where we will evaluate the function t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # Transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() First, let’s define the necessary datasets with coefficients. Code # Training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # Test dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now let’s look at the classification of our simulated curves using Support Vector Machines (SVM). The advantage of this classification method is its computational efficiency, as it uses only a few observations (often very few) to define the decision boundary between classes. 3.3.5.1 Discretization of the Interval Let’s start by applying the SVM method directly to the discretized data (evaluating the function on a given grid of points in the interval \\(I = [0, 6]\\)), considering all three aforementioned kernel functions. Code # Set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # Split into training and test sets X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # Build models clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is thus 5.71 % for the linear kernel, 2.14 % for the polynomial kernel, and 3.57 % for the radial kernel. On the test data, the error rate is 16.67 % for the linear kernel, 16.67 % for the polynomial kernel, and 13.33 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discretized&#39;, &#39;SVM polynomial - discretized&#39;, &#39;SVM RBF - discretized&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.6 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 2 principal components. Code # Model building clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the principal component scores on the training data is thus 44.29 % for the linear kernel, 37.86 % for the polynomial kernel, and 37.14 % for the Gaussian kernel. On the testing data, the error rate of the method is 48.33 % for the linear kernel, 43.33 % for the polynomial kernel, and 40 % for the radial kernel. To graphically represent the method, we can plot the decision boundary on the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, as done in previous cases when we also plotted classification boundaries. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 1.18: Scores of the first two principal components, color-coded according to classification class membership. The decision boundary (line or curves in the plane of the first two principal components) between the classes is marked in black, constructed using the SVM method. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.6.1 Base Coefficients Finally, we will use function representation through B-spline basis. Code # Model building clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the base coefficients on the training data is therefore 4.29 % for the linear kernel, 4.29 % for the polynomial kernel, and 7.14 % for the radial kernel. On the test data, the error rate of the method is 10 % for the linear kernel, 11.67 % for the polynomial kernel, and 15 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 3.3.6.2 Projection onto B-Spline Basis Another way to apply the classical SVM method to functional data is to project the original data onto a \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal{H}\\), which we denote as \\(V_d\\). We assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), which can be expressed as \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Now, we can use the coefficients from the orthogonal projection for classification, applying standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By utilizing this transformation, we have defined a new, so-called adapted kernel, which consists of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector method. Thus, we have the (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This is a dimensionality reduction method that we can call filtering. For the projection itself, we will use the project.basis() function from the fda package in R. The input will be a matrix of the original discrete (non-smoothed) data, the values at which we measure the values in the original data matrix, and the basis object onto which we want to project the data. We will choose to project onto the B-spline basis since using a Fourier basis is not suitable for our non-periodic data. The dimension \\(d\\) is either chosen based on prior expert knowledge or through cross-validation. In our case, we will determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (choosing \\(k \\ll n\\) due to the computational intensity of the method, often \\(k = 5\\) or \\(k = 10\\)). We require B-splines of order 4, and the number of basis functions is determined by the relationship \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). Therefore, we choose a minimum dimension of \\(n_{basis} = 3\\) (for \\(n_{breaks} = 1\\)) and a maximum dimension of \\(n_{basis} = 53\\) (for \\(n_{breaks} = 51\\), corresponding to the number of original discrete data points). However, in R, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and for large values of \\(n_{basis}\\), overfitting occurs, so we choose a maximum of \\(n_{basis} = 43\\). Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # All dimensions we want to try # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components ... matrices for individual kernels -&gt; linear, poly, radial # Empty matrix where we will store the results # Columns will have accuracy values for the respective part of the training set # Rows will have values for the respective dimension value CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Projection of discrete data onto B-spline basis of dimension d Projection &lt;- project.basis(y = XX, # Matrix of discrete data argvals = t, # Vector of arguments basisobj = bbasis) # Basis object # Split into training and testing data for CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Model building clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## Linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies in positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # Compute average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.05578755 ## poly 11 0.07673993 ## radial 10 0.06959707 We can see that the best value of the parameter \\(d\\) is 11 for the linear kernel with the error value calculated using 10-fold CV 0.0558, 11 for the polynomial kernel with the value calculated using 10-fold CV 0.0767, and 10 for the radial kernel with the value 0.0696. For clarity, let’s also plot the progression of validation errors depending on the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation Error&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 3.1: Dependency of validation error on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The optimal values of the dimension \\(V_d\\) for each kernel function are marked by black dots. Now we can train the individual classifiers on all training data and look at their success on the test data. For each kernel function, we choose the dimension of the subspace we project onto based on the results of cross-validation. In the variable Projection, we have stored the matrix of coefficients for the orthogonal projection, i.e., \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Base object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Projection of discrete data onto B-spline basis Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # Splitting into training and test data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Building the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Storing results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is thus 4.29 % for the linear kernel, 3.57 % for the polynomial kernel, and 5 % for the Gaussian kernel. On the test data, the error rate of the method is 10 % for the linear kernel, 8.33 % for the polynomial kernel, and 10 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 3.3.6.3 RKHS + SVM 3.3.6.3.0.1 Gaussian Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Let’s compute the matrix \\(K_S\\) along with its eigenvalues and corresponding eigenvectors. Code # Compute the matrix K gamma &lt;- 0.1 # Fixed value of gamma, optimal value will be determined using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To compute the coefficients in the representation of curves, specifically the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we also need the coefficients from SVM. Unlike the classification problem, we are now dealing with a regression problem, as we are trying to express our observed curves in some basis (chosen using kernel \\(K\\)). Therefore, we will use the Support Vector Regression method, from which we will obtain the coefficients \\(\\alpha_{il}\\). Code # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } Now we can compute the representations of individual curves. First, let’s choose \\(\\hat d\\) as the full dimension, i.e., \\(\\hat d = m ={}\\) 101, and then determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*\\) stored in the columns of the matrix Lambda.RKHS for each curve. We will use these vectors as representations of the given curves and classify the data based on this discretization. Code # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # Loop through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 3.1: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error and \\(\\widehat{Err}_{test}\\) indicates the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.1000 0.3833 SVM poly - RKHS 0.0286 0.3000 SVM rbf - RKHS 0.0786 0.2667 We see that the model classifies the training data very well for all three kernels, while its performance on the test data is not good at all. It is clear that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values of \\(\\gamma\\) and \\(d\\). Code # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Values of hyperparameters to explore dimensions &lt;- 3:40 # Reasonable range for d gamma.cv &lt;- 10^seq(-1, 2, length = 15) # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store results # Columns will contain accuracy values for given hyperparameters # Rows will contain values for given gamma, and layers correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation itself for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies at positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 3.2: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 31 8.4834 0.0655 linear poly 24 8.4834 0.1213 polynomial radial 26 0.7197 0.1218 radial We see that the optimal parameter values are \\(d={}\\) 31 and \\(\\gamma={}\\) 8.4834 for the linear kernel with an error value computed using 10-fold CV 0.0655, \\(d={}\\) 24 and \\(\\gamma={}\\) 8.4834 for the polynomial kernel with an error value computed using 10-fold CV 0.1213, and \\(d={}\\) 26 and \\(\\gamma={}\\) 0.7197 for the radial kernel with an error value of 0.1218. For interest, let’s plot the validation error function as a function of dimension \\(d\\) and hyperparameter \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 3.2: Dependence of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. Since we have already found the optimal values for the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate through the kernels for (kernel_number in 1:3) { # Compute the K matrix gamma &lt;- gamma.opt[kernel_number] # gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 3.3: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0214 0.1833 SVM poly - RKHS - radial 0.0000 0.2000 SVM rbf - RKHS - radial 0.0214 0.1667 The error of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 2.14 % for the linear kernel, 0 % for the polynomial kernel, and 2.14 % for the Gaussian kernel on the training data. On the testing data, the error rates are 18.33 % for the linear kernel, 20 % for the polynomial kernel, and 16.67 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 3.3.7 Results Table Table 2.2: Summary of the results of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3571 0.3833 LDA 0.4143 0.4000 QDA 0.3571 0.4000 LR functional 0.0357 0.0833 SVM linear - discretized 0.0571 0.1667 SVM polynomial - discretized 0.0214 0.1667 SVM RBF - discretized 0.0357 0.1333 SVM linear - PCA 0.4429 0.4833 SVM poly - PCA 0.3786 0.4333 SVM rbf - PCA 0.3714 0.4000 SVM linear - Bbasis 0.0429 0.1000 SVM poly - Bbasis 0.0429 0.1167 SVM rbf - Bbasis 0.0714 0.1500 SVM linear - projection 0.0429 0.1000 SVM poly - projection 0.0357 0.0833 SVM rbf - projection 0.0500 0.1000 SVM linear - RKHS - radial 0.0214 0.1833 SVM poly - RKHS - radial 0.0000 0.2000 SVM rbf - RKHS - radial 0.0214 0.1667 3.4 Simulation Study In the entire previous section, we focused solely on one randomly generated dataset of functions from two classification classes, which we subsequently randomly split into test and training parts. We then evaluated the individual classifiers obtained using the considered methods based on their test and training errors. Since the generated data (and their division into two parts) can vary significantly with each repetition, the errors of the individual classification algorithms can also differ greatly. Therefore, drawing any conclusions about the methods and comparing them based on a single generated dataset can be very misleading. For this reason, we will repeat the entire previous procedure for various generated datasets in this section. We will store the results in a table and finally compute the average characteristics of the models across the repetitions. To ensure our conclusions are sufficiently general, we will choose the number of repetitions as \\(n_{sim} = 25\\). Now, we will repeat the entire previous section n.sim times and store the error values in the object SIMUL_params, while varying the parameter \\(\\sigma_{shift}\\) to observe how the results of the selected classification methods change with this value. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 25 methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39; ) # vektor smerodatnych odchylek definujicich posunuti generovanych krivek shift_vector &lt;- seq(0.1, 5, length = 30) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 4, length(shift_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;), shift = paste0(shift_vector))) for (n_shift in 1:length(shift_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # objekt na ulozeni optimalnich hodnot hyperparametru, ktere se urcuji pomoci CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, shift_vector[n_shift]) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, shift_vector[n_shift]) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 5 # k-fold CV neighbours &lt;- 1:15 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max # k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM ### 7.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.01, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.6, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 500, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 500, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1000, gamma = 0.005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 #length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma # Gauss.kernel &lt;- function(x, y, gamma) { # return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) # } # # Kernel.RKHS &lt;- function(x, gamma) { # K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) # for(i in 1:nrow(K)) { # for(j in 1:ncol(K)) { # K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) # } # } # return(K) # } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(10, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) # CV_RESULTS$SVM_RKHS_radial_gamma[sim] &lt;- gamma.opt # CV_RESULTS$SVM_RKHS_radial_d[sim] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, paste0(n_shift, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) SIMUL_params[, , n_shift] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_parametry_shift_03_rf_cv.RData&#39;) 3.4.1 Graphical Output Let’s examine the dependence of the simulated results on the value of the standard deviation parameter for the normal distribution of measurement errors. First, we’ll plot the data for training errors. Code # For training data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[train]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;bottom&#39;) Figure 3.3: Graph of the dependence of training error for individual classification methods on the value of the parameter \\(\\sigma_{shift}\\), which defines the standard deviation for generating random vertical shifts. Next, we’ll plot the data for test errors. Code # For test data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; ggplot(aes(x = shift, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(sigma[shift]), y = expression(widehat(Err)[test]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;bottom&#39;) Figure 3.4: Graph of the dependence of test error for individual classification methods on the value of the parameter \\(\\sigma_{shift}\\), which defines the standard deviation for generating random vertical shifts. Since the results are subject to random fluctuations and increasing the number of repetitions $ n_{} $ would be computationally intensive, we will now smooth the curves of the average test and training errors. Given that error rates are non-negative, we will perform the smoothing with this in mind. Additionally, to ensure that the smoothed curves closely follow the observed discrete values, we will consider different weights for small and large values of $ $. Code methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- shift_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # Vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # Empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # Smoothing gcv[index] &lt;- mean(BSmooth$gcv) # Average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # Find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # Smooth position XXfd &lt;- BSmooth$fd # Wfdobj fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd evalarg = seq(min(breaks), max(breaks), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), shift = rep(seq(min(breaks), max(breaks), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) To enhance clarity, we will plot the graphs only for a subset of methods. Code methods_names &lt;- c( &#39;$K$ Nearest Neighbors&#39;, &#39;Functional Logistic Regression&#39;, &#39;Random Forest -- Basis Coefficients&#39;, &#39;SVM (Linear) -- Discretization&#39;, &#39;SVM (Linear) -- fPCA&#39;, &#39;SVM (Linear) -- Basis Coefficients&#39;, &#39;SVM (Linear) -- Projection&#39;, &#39;RKHS (Radial SVR) $+$ SVM (Linear)&#39; ) Code # For test data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = shift, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;Shift&#39;, y = &#39;Training Error&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + geom_line(data = df_plot_smooth, aes(x = shift, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) Figure 3.5: Graph of the relationship between test error rates for individual classification methods and the parameter value \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03sigma_curvesErr.tex&quot;, device = tikz, width = 9, height = 4.5) Now we will do the same for the test error rates. Code # methods_subset &lt;- c(&#39;KNN&#39;, &#39;LR_functional&#39;, &#39;SVM linear - diskr&#39;, # &#39;SVM linear - PCA&#39;, # &#39;SVM linear - projection&#39;, &#39;SVM linear - RKHS - radial&#39;) Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- shift_vector rangeval &lt;- range(breaks) norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = -3, to = 3, length.out = 50) # Vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # Empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar) # Smoothing gcv[index] &lt;- mean(BSmooth$gcv) # Average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # Find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar) # Smooth position XXfd &lt;- BSmooth$fd # Wfdobj fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, # eval.posfd evalarg = seq(min(breaks), max(breaks), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(methods, each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), shift = rep(seq(min(breaks), max(breaks), length = 101), length(methods)) ) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) Code # For test data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(shift_vector), names_to = &#39;shift&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), shift = as.numeric(shift)) |&gt; filter(method %in% methods_subset) |&gt; mutate(method = factor(method, levels = methods_subset, labels = methods_subset, ordered = TRUE)) |&gt; ggplot(aes(x = shift, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;$\\\\sigma$&#39;, y = &#39;Test Error Rate&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;, plot.margin = unit(c(0.1, 0.7, 0.3, 0.3), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + geom_line(data = df_plot_smooth, aes(x = shift, y = value, col = method, linetype = method), linewidth = 1.5) + scale_colour_manual(values = rep(c(&#39;deepskyblue2&#39;, &#39;tomato&#39;), c(4, 4)), labels = methods_names) + scale_linetype_manual(values = rep(c(&#39;dotdash&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dotted&#39;), 2), labels = methods_names) + scale_shape_manual(values = rep(c(16, 1, 17, 4, 16, 1, 17, 4)), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.4, alpha = 0.6)), linetype = guide_legend(override.aes = list(linewidth = 0.7))) Figure 2.13: Graph of the relationship between test error rates for individual classification methods and the parameter value \\(\\sigma_{x}\\). Code # ggsave(&quot;figures/kap6_sim_03shift_curvesErr_subset.tex&quot;, device = tikz, width = 8, height = 5) Comments on the graphs can be found in the thesis. It is worth noting that as the value of \\(\\sigma_{shift}\\) increases, the test error rate also rises for some methods. At the same time, for methods that classify data with vertical shifts well, there is no degradation in classification strength as the size of the shift increases. "],["simulace3diskr.html", "Chapter 4 Dependency on Discretization 4.1 Simulation of Functional Data 4.2 Smoothing the Observed Curves 4.3 Classification of Curves 4.4 Simulation Study", " Chapter 4 Dependency on Discretization In this section, we will focus on the dependence of the results from the previous section 1 on the value of \\(p\\), which defines the length of the equidistant sequence of points used for discretizing the observed functional objects. We are interested in how the error rates of individual classification methods change as we refine the interval division, that is, as the dimension of the discretized vectors increases. Our assumption is that the support vector method applied to discretized data should yield lower error rates as \\(p\\) increases, since we better approximate integrals and thus the values of kernel functions for functional analogs. Conversely, we would expect decision trees and random forests to stagnate beyond a certain number of \\(p\\). Since we are discussing the discretization of the interval, this chapter will focus solely on methods that work with discretized data—namely: Decision Tree, Random Forest, SVM Method. In addition to average test error rates, we are also interested in the computational cost of each method as \\(p\\) increases, since minimum error rate is not the only criterion for using a given classification method in practice. Computational complexity and the associated time demands play a relatively important role as well. This chapter consists of two parts. In the first Section 4.3, we will choose a fixed number of \\(p=101\\) discretized values and examine the individual methods, their error rates, and their time demands. Subsequently, in Section 4.4, we will define a sequence of \\(p\\) values and repeat the entire process several times for each value so that we can establish average results for that value. Finally, we will plot the dependence of error rates and computational demands on the value of \\(p\\). 4.1 Simulation of Functional Data First, we will simulate functions that we will later classify. For simplicity, we will consider two classification classes. To simulate, we will: choose appropriate functions, generate points from the chosen interval that contain, for example, Gaussian noise, smooth the resulting discrete points into the form of a functional object using a suitable basis system. This process will yield functional objects along with the value of the categorical variable \\(Y\\), which distinguishes class membership. Code # Load necessary packages library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) set.seed(42) Let’s consider two classification classes, \\(Y \\in \\{0, 1\\}\\), with the same number of generated functions n for each class. First, we will define two functions, each for one class, over the interval \\(I = [0, 6]\\). Now we will create functions using interpolation polynomials. First, we define the points that our curve should pass through and then fit an interpolation polynomial through them, which we will use to generate curves for classification. Code # Defining points for class 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # Defining points for class 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # Plot points dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Classification\\n Class&#39;) Figure 1.1: Points defining the interpolation polynomials. To calculate the interpolation polynomials, we will use the poly.calc() function from the polynom library. We will then define functions poly.0() and poly.1() that will compute the polynomial values at a given point in the interval. To create these, we will use the predict() function, where we input the corresponding polynomial and the point at which we want to evaluate the polynomial. Code # Calculate polynomials polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # Plot polynomials xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Classification\\n Class&#39;) Figure 1.2: Representation of two functions over the interval \\(I = [0, 6]\\), from which we generate observations from classes 0 and 1. Code # Generating functions for Y = 0 and Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Now we will create a function to generate random functions with added noise (or points on a predefined grid) from the chosen generating function. The argument t indicates the vector of values at which we want to evaluate the functions, fun denotes the generating function, n is the number of functions, and sigma is the standard deviation of the normal distribution \\(\\text{N}(\\mu, \\sigma^2)\\), from which we randomly generate Gaussian white noise with \\(\\mu = 0\\). To demonstrate the advantage of using methods that work with functional data, we will also add a random term to each simulated observation, which will serve as a vertical shift for the entire function (parameter sigma_shift). This shift will be generated from a normal distribution with the parameter \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Now we can generate the functions. In each of the two classes, we will consider 100 observations, thus n = 100. Code # Number of generated observations for each class n &lt;- 100 # Vector of time points equidistant on the interval [0, 6] t &lt;- seq(0, 6, length = 51) # For Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # For Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (not yet smoothed) functions in color according to their class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 # Number of curves to plot from each group DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Classification\\n Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.3: The first 10 generated observations from each of the two classification classes. The observed data are not smoothed. 4.2 Smoothing the Observed Curves Now we will convert the observed discrete values (vectors of values) into functional objects, which we will subsequently work with. We will again use a B-spline basis for smoothing. We will take the entire vector t as the knots, and since we will generally consider cubic splines, we will choose (the default choice in R) norder = 4. We will penalize the second derivative of the functions. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # penalize the 2nd derivative We will find a suitable value for the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), i.e., using generalized cross-validation. We will consider the value of \\(\\lambda\\) to be the same for both classification groups because we would not know in advance which value of $to choose if a different choice were made for each class. Code # Combine observations into a single matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -2, to = 1, length.out = 50) # vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # Find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] For better visualization, we will plot the progression of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.4: The progression of \\(GCV(\\lambda)\\) for the chosen vector \\(\\boldsymbol\\lambda\\). The x-axis values are plotted on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is indicated in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically display the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Classification\\n Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.5: The first 10 smoothed curves from each classification class. Let’s visualize all the smoothed observed curves, highlighting them by their classification class, along with the average for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Classification\\n Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + scale_y_continuous(expand = c(0.01, 0.01)) # limits = c(-1, 2) Figure 1.6: Plot of all smoothed observed curves, colored by classification class. The thick line represents the average for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Classification\\n Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 1.7: Plot of all smoothed observed curves, colored by classification class. The thick line represents the average for each class. Close-up view. Here’s the continuation of your text, including the code for classification with decision trees. 4.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training sets library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for LR on scores library(nnet) # for LR on scores library(caret) library(rpart) # decision trees library(rattle) # graphics library(e1071) library(randomForest) # random forest To compare the individual classifiers, we will split the generated observations into two parts in a ratio of 70:30, designated as training and test (validation) sets. The training set will be used for constructing the classifier, while the test set will be used for calculating classification errors and potentially other model characteristics. We can then compare the resulting classifiers based on these computed characteristics regarding their classification success. Code # splitting into test and training sets split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, we will examine the representation of individual groups in the test and training data. 4.3.1 Decision Trees Decision trees are a popular tool for classification; however, similar to some previous methods, they are not directly designed for functional data. Nevertheless, there are ways to convert functional objects into multidimensional ones and then apply decision tree algorithms to them. We can consider the following approaches: An algorithm constructed on basis coefficients, Using scores of principal components, Discretizing the interval and evaluating the function only at some finite grid points. We will, of course, choose the last option. First, we need to define points from the interval \\(I = [0, 6]\\) at which we will evaluate the functions. Next, we will create an object where the rows represent individual (discretized) functions and the columns represent time. Finally, we will append a column \\(Y\\) with information about classification class membership and repeat the same for the test data. Code # sequence of points where we evaluate the functions p &lt;- 101 t.seq &lt;- seq(0, 6, length = p) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can construct a decision tree, using all the times from the vector t.seq as predictors. This classification is not sensitive to multicollinearity, so we do not need to concern ourselves with it. We will choose accuracy as the metric. Code # constructing the model start_time &lt;- Sys.time() clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) end_time &lt;- Sys.time() # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the classifier on the test data is therefore 46.67 %, and on the training data, it is 33.57 %. We will calculate the time taken as follows. Code duration &lt;- end_time - start_time duration |&gt; print() ## Time difference of 0.718204 secs We can graphically visualize the decision tree using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This will be an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.8: Graphical representation of the unpruned decision tree. Nodes belonging to classification class 1 are shown in shades of blue, and those belonging to class 0 are in shades of red. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 2.3: Final pruned decision tree. Finally, let’s add the training and test error rates to the summary table. Code RESULTS &lt;- data.frame(model = &#39;Tree - discr.&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test, Duration = as.numeric(duration)) 4.3.2 Random Forests The classifier constructed using the random forests method consists of building several individual decision trees, which are subsequently combined to create a common classifier (through a collective “vote”). As with decision trees, we have several options regarding what data (finite-dimensional) to use for constructing the model. We will again consider discretization of the interval. In this case, we utilize the evaluation of functions on a given grid of points in the interval \\(I = [0, 6]\\). Code # model construction start_time &lt;- Sys.time() clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = F, nodesize = 1) end_time &lt;- Sys.time() # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is therefore 0 % and on the test data 40 %. We will calculate the time complexity as follows. Code duration &lt;- end_time - start_time duration |&gt; print() ## Time difference of 0.302609 secs Code Res &lt;- data.frame(model = &#39;RForest - discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test, Duration = as.numeric(duration)) RESULTS &lt;- rbind(RESULTS, Res) 4.3.3 Support Vector Machines Now let’s look at classifying our simulated curves using the Support Vector Machines (SVM) method. The advantage of this classification method is its computational efficiency, as it uses only a few (often very few) observations to define the boundary curve between classes. In the case of functional data, we have several options for applying the SVM method. The simplest variant is to use this classification method directly on the discretized function. We will start by applying the support vector method directly to the discretized data (evaluation of the function on a given grid of points in the interval \\(I = [0, 6]\\)), considering all three aforementioned kernel functions. First, however, we will normalize the data. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # split into test and training parts X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # model construction start_time &lt;- Sys.time() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) end_time &lt;- Sys.time() duration.l &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) end_time &lt;- Sys.time() duration.p &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) end_time &lt;- Sys.time() duration.r &lt;- end_time - start_time # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is therefore 5.71 % for the linear kernel, 2.14 % for the polynomial kernel, and 3.57 % for the Gaussian kernel. On the test data, the error rate of the method is 16.67 % for the linear kernel, 16.67 % for the polynomial kernel, and 13.33 % for the radial kernel. We will calculate the time complexity as follows. Code print(&#39;Linear kernel:&#39;, quote = F) ## [1] Linear kernel: Code duration.l |&gt; print() ## Time difference of 0.04756021 secs Code print(&#39;Polynomial kernel:&#39;, quote = F) ## [1] Polynomial kernel: Code duration.p |&gt; print() ## Time difference of 0.03182602 secs Code print(&#39;Radial kernel:&#39;, quote = F) ## [1] Radial kernel: Code duration.r |&gt; print() ## Time difference of 0.06402302 secs Code Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r), Duration = c(as.numeric(duration.l), as.numeric(duration.p), as.numeric(duration.r))) RESULTS &lt;- rbind(RESULTS, Res) 4.3.4 Results Table Table 4.1: Summary of results of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) Duration Tree - discr. 0.3357 0.4667 0.7182 RForest - discr 0.0000 0.4000 0.3026 SVM linear - discr 0.0571 0.1667 0.0476 SVM poly - discr 0.0214 0.1667 0.0318 SVM rbf - discr 0.0357 0.1333 0.0640 4.4 Simulation Study In the entire previous section, we dealt with only one randomly generated dataset from two classification classes, which we then randomly split into test and training parts. We then evaluated the individual classifiers obtained using the considered methods based on test and training error rates. Since the generated data (and their division into two parts) can vary significantly with each repetition, the error rates of the individual classification algorithms will also differ significantly. Therefore, making any conclusions about the methods and comparing them based on a single generated dataset can be very misleading. For this reason, in this section, we will focus on repeating the entire previous procedure for different generated datasets. We will store the results in a table and finally calculate the average characteristics of the models across the individual repetitions. To ensure our conclusions are sufficiently general, we will choose the number of repetitions \\(n_{sim} = 50\\). Now we will repeat the entire previous section \\(n.sim\\) times, and we will store the error rates in the object SIMUL_params. At the same time, we will change the value of the parameter \\(p\\) and observe how the results of the selected classification methods change with this value. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) # pocet simulaci pro kazdou hodnotu simulacniho parametru n.sim &lt;- 50 methods &lt;- c(&#39;RF_discr&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;) # vektor delek p ekvidistantni posloupnosti p_vector &lt;- seq(4, 250, by = 2) # p_vector &lt;- seq(10, 250, by = 10) # vysledny objekt, do nehoz ukladame vysledky simulaci SIMUL_params &lt;- array(data = NA, dim = c(length(methods), 6, length(p_vector)), dimnames = list( method = methods, metric = c(&#39;ERRtrain&#39;, &#39;Errtest&#39;, &#39;SDtrain&#39;, &#39;SDtest&#39;, &#39;Duration&#39;, &#39;SDduration&#39;), sigma = paste0(p_vector))) # CV_res &lt;- data.frame(matrix(NA, ncol = 4, # nrow = length(p_vector), # dimnames = list(paste0(p_vector), # c(&#39;C.l&#39;, &#39;C.p&#39;, &#39;C.r&#39;, &#39;gamma&#39;)))) for (n_p in 1:length(p_vector)) { ## list, do ktereho budeme ukladat hodnoty chybovosti # ve sloupcich budou metody # v radcich budou jednotliva opakovani # list ma dve polozky ... train a test SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), duration = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))#, # CV = as.data.frame(matrix(NA, ncol = 4, # nrow = n.sim, # dimnames = list(1:n.sim, # c(&#39;C.l&#39;, &#39;C.p&#39;, &#39;C.r&#39;, &#39;gamma&#39;)))) ) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 4 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = 2, length.out = 40) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) Rozhodovací stromy # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = p_vector[n_p]) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu # start_time &lt;- Sys.time() # clf.tree &lt;- train(Y ~ ., data = grid.data, # method = &quot;rpart&quot;, # trControl = trainControl(method = &quot;CV&quot;, number = 10), # metric = &quot;Accuracy&quot;) # end_time &lt;- Sys.time() # duration &lt;- end_time - start_time # # # presnost na trenovacich datech # predictions.train &lt;- predict(clf.tree, newdata = grid.data) # presnost.train &lt;- table(Y.train, predictions.train) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # # presnost na trenovacich datech # predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) # presnost.test &lt;- table(Y.test, predictions.test) |&gt; # prop.table() |&gt; diag() |&gt; sum() # # RESULTS &lt;- data.frame(model = &#39;Tree_discr&#39;, # Err.train = 1 - presnost.train, # Err.test = 1 - presnost.test, # Duration = as.numeric(duration)) ## 2) Náhodné lesy # sestrojeni modelu start_time &lt;- Sys.time() clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) end_time &lt;- Sys.time() duration &lt;- end_time - start_time # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test, Duration = as.numeric(duration)) # RESULTS &lt;- rbind(RESULTS, Res) ## 3) SVM # normovani dat norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() start_time &lt;- Sys.time() clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, kernel = &#39;linear&#39;) end_time &lt;- Sys.time() duration.l &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 100, kernel = &#39;polynomial&#39;) end_time &lt;- Sys.time() duration.p &lt;- end_time - start_time start_time &lt;- Sys.time() clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 100000, gamma = 0.0001, kernel = &#39;radial&#39;) end_time &lt;- Sys.time() duration.r &lt;- end_time - start_time # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r), Duration = c(as.numeric(duration.l), as.numeric(duration.p), as.numeric(duration.r))) RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test SIMULACE$duration[sim, ] &lt;- RESULTS$Duration cat(&#39;\\r&#39;, paste0(n_p, &#39;: &#39;, sim)) } # Nyní spočítáme průměrné testovací a trénovací chybovosti pro jednotlivé # klasifikační metody. # dame do vysledne tabulky SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd), Duration = apply(SIMULACE$duration, 2, mean), SD.dur = apply(SIMULACE$duration, 2, sd)) # CV_res[n_p, ] &lt;- apply(SIMULACE$CV, 2, median) SIMUL_params[, , n_p] &lt;- as.matrix(SIMULACE.df) } # ulozime vysledne hodnoty save(SIMUL_params, file = &#39;RData/simulace_params_diskretizace_03_cv.RData&#39;) 4.4.1 Graphical Output Let’s examine the dependence of the simulated results on the value of the parameter \\(p\\). We’ll start with the training error rates. Code methods_names &lt;- c( # &#39;Decision tree&#39;, &#39;Random Forest&#39;, &#39;SVM (linear)&#39;, &#39;SVM (poly)&#39;, &#39;SVM (radial)&#39; ) Code # for training data SIMUL_params[, 1, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = expression(widehat(Err)[train]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;) Figure 4.1: Graph of the dependence of training error rates for individual classification methods on the value of the parameter \\(p\\). Next, we’ll look at the test error rates, which is the graph we are most interested in. Code # for test data SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = expression(widehat(Err)[test]), colour = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;) Figure 1.12: Graph of the dependence of test error rates for individual classification methods on the value of the parameter \\(p\\). Finally, let’s plot the graph for the time complexity of the methods. Code SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Duration&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Duration, colour = method, linetype = method)) + geom_line() + theme_bw() + labs(x = expression(p), y = &quot;Duration [s]&quot;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;) + theme(legend.position = &#39;right&#39;) + scale_y_log10() + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, &#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;)) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;, &#39;dotted&#39;)) Figure 2.7: Graph of the dependence of the time complexity of methods for individual classification methods on the value of the parameter \\(p\\). Since the results are subject to random fluctuations and increasing the number of repetitions n.sim would be computationally intensive, let’s now smooth the curves of the average test error rates with respect to the parameter \\(p\\). Code Dat &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- p_vector rangeval &lt;- range(breaks) norder &lt;- 4 wtvec &lt;- rep(1, 124) # wtvec &lt;- c(seq(50, 10, length = 4), seq(0.1, 0.1, length = 10), rep(1, 110)) bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = 3, to = 5, length.out = 50) # vector of lambda gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for GCV storage for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar, wtvec = wtvec) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar, wtvec = wtvec) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = seq(min(p_vector), max(p_vector), length = 101)) df_plot_smooth &lt;- data.frame( method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), p = rep(seq(min(p_vector), max(p_vector), length = 101), length(methods)) ) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) Code # for test data p1 &lt;- SIMUL_params[, 2, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;p&#39;, y = &#39;Test Error Rate&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;), panel.grid.minor.x = element_blank()) + # scale_y_log10() + geom_line(data = df_plot_smooth, aes(x = p, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, #&#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;), labels = methods_names) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;#, &#39;dotted&#39; ), labels = methods_names) + scale_shape_manual(values = c(16, 1, 16, 1), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) p1 Figure 1.13: Graph of the dependence of test error rates for individual classification methods on the value of the parameter \\(p\\). Code # ggsave(&quot;figures/kap6_sim_03diskr_curvesErr.tex&quot;, device = tikz, width = 4.5, height = 4.5) Let’s add the computational time to the graph. Code Dat &lt;- SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; t() breaks &lt;- p_vector rangeval &lt;- range(breaks) norder &lt;- 4 wtvec &lt;- rep(1, 124) # wtvec &lt;- c(seq(50, 10, length = 4), seq(0.1, 0.1, length = 10), rep(1, 110)) bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(2) lambda.vect &lt;- 10^seq(from = 3, to = 5, length.out = 50) # vector of lambda gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for GCV storage for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(breaks, Dat, curv.Fdpar, wtvec = wtvec) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(breaks, Dat, curv.fdPar, wtvec = wtvec) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = seq(min(p_vector), max(p_vector), length = 101)) df_plot_smooth_2 &lt;- data.frame( method = rep(colnames(fdobjSmootheval), each = dim(fdobjSmootheval)[1]), value = c(fdobjSmootheval), p = rep(seq(min(p_vector), max(p_vector), length = 101), length(methods)) ) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) Code # for test data p2 &lt;- SIMUL_params[, 5, ] |&gt; as.data.frame() |&gt; mutate(method = methods) |&gt; pivot_longer(cols = as.character(p_vector), names_to = &#39;p&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE), p = as.numeric(p)) |&gt; filter(method %in% c(&quot;RF_discr&quot;, &quot;SVM linear - diskr&quot;, &quot;SVM poly - diskr&quot;, &quot;SVM rbf - diskr&quot;)) |&gt; ggplot(aes(x = p, y = Err, colour = method, shape = method)) + geom_point(alpha = 0.7, size = 0.6) + theme_bw() + labs(x = &#39;p&#39;, y = &#39;computational time&#39;, colour = &#39;Classification Method&#39;, linetype = &#39;Classification Method&#39;, shape = &#39;Classification Method&#39;) + theme(legend.position = c(0.7, 0.16),#&#39;right&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;), panel.grid.minor = element_blank(), legend.background = element_rect(fill=&#39;transparent&#39;), legend.key = element_rect(colour = NA, fill = NA)) + scale_y_log10() + geom_line(data = df_plot_smooth_2, aes(x = p, y = value, col = method, linetype = method), linewidth = 0.95) + scale_colour_manual(values = c(&#39;deepskyblue2&#39;, &#39;deepskyblue2&#39;, #&#39;tomato&#39;, &#39;tomato&#39;, &#39;tomato&#39;), labels = methods_names) + scale_linetype_manual(values = c(&#39;solid&#39;, &#39;dashed&#39;, &#39;solid&#39;, &#39;dashed&#39;#, &#39;dotted&#39; ), labels = methods_names) + scale_shape_manual(values = c(16, 1, 16, 1), labels = methods_names) + guides(colour = guide_legend(override.aes = list(size = 1.2, alpha = 0.7)), linetype = guide_legend(override.aes = list(linewidth = 0.8))) p2 Figure 1.14: Graph of the dependence of computational time for individual classification methods on the value of the parameter \\(p\\). Code # ggsave(&quot;figures/kap6_sim_03diskr_curvesTime.tex&quot;, device = tikz, width = 4.5, height = 4.5) Detailed comments on the results of this simulation study can be found in the article. "],["simulace4.html", "Chapter 5 Simulation - Using Derivatives 5.1 Classification Based on the First Derivative 5.2 Classification Based on the Second Derivative", " Chapter 5 Simulation - Using Derivatives In this last section concerning simulated data, we will focus on the same data as in Chapter 1 (and possibly also in Chapters 2 or 3), specifically generating functional data from functions calculated using interpolation polynomials. As we generated data with a random vertical shift with a standard deviation parameter \\(\\sigma_{shift}\\) in Section 1, we could attempt to remove this shift and classify the data after its removal. We observed in Section 3 that the accuracy of especially classical classification methods deteriorates rather dramatically as the value of the standard deviation parameter \\(\\sigma_{shift}\\) increases. In contrast, classification methods that account for the functional nature of the data generally behave quite stably, even as \\(\\sigma_{shift}\\) increases. One way to remove vertical shifts, which we will use in the following section, is to classify data based on the estimate of the first derivative of the generated and smoothed curve, since it is known that \\[ \\frac{\\text d}{\\text d t} \\big( x(t) + c \\big) = \\frac{\\text d}{\\text d t} x(t)= x&#39;(t). \\] 5.1 Classification Based on the First Derivative First, we will simulate functions that we will subsequently want to classify. For simplicity, we will consider two classification classes. To simulate, we will: Choose appropriate functions, Generate points from the chosen interval that contain, for example, Gaussian noise, Smooth the obtained discrete points into a functional object using a suitable basis system. This procedure will yield functional objects along with the value of the categorical variable \\(Y\\), which distinguishes membership in a classification class. Code # Load necessary packages library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) library(polynom) library(tikzDevice) set.seed(42) Let’s consider two classification classes, \\(Y \\in \\{0, 1\\}\\), with the same number of n generated functions for each class. First, we define two functions, each corresponding to one class, on the interval \\(I = [0, 6]\\). Now, we will create the functions using interpolation polynomials. First, we define the points through which our curve will pass and then fit an interpolation polynomial through these points, which we will use to generate the curves for classification. Code # Defining points for class 0 x.0 &lt;- c(0.00, 0.65, 0.94, 1.42, 2.26, 2.84, 3.73, 4.50, 5.43, 6.00) y.0 &lt;- c(0, 0.25, 0.86, 1.49, 1.1, 0.15, -0.11, -0.36, 0.23, 0) # Defining points for class 1 x.1 &lt;- c(0.00, 0.51, 0.91, 1.25, 1.51, 2.14, 2.43, 2.96, 3.70, 4.60, 5.25, 5.67, 6.00) y.1 &lt;- c(0.1, 0.4, 0.71, 1.08, 1.47, 1.39, 0.81, 0.05, -0.1, -0.4, 0.3, 0.37, 0) Code # Plotting the points dat_points &lt;- data.frame(x = c(x.0, x.1), y = c(y.0, y.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(x.0), length(x.1)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + labs(colour = &#39;Class&#39;) Figure 1.1: Points defining both interpolation polynomials. To calculate the interpolation polynomials, we will use the poly.calc() function from the polynom library. We will also define the functions poly.0() and poly.1(), which will compute the values of the polynomials at a given point in the interval. We will use the predict() function for this, where we input the corresponding polynomial and the point at which we want to evaluate the polynomial. Code # Calculation of polynomials polynom.0 &lt;- poly.calc(x.0, y.0) polynom.1 &lt;- poly.calc(x.1, y.1) Code poly.0 &lt;- function(x) return(predict(polynom.0, x)) poly.1 &lt;- function(x) return(predict(polynom.1, x)) Code # Plotting the polynomials xx &lt;- seq(min(x.0), max(x.0), length = 501) yy.0 &lt;- poly.0(xx) yy.1 &lt;- poly.1(xx) dat_poly_plot &lt;- data.frame(x = c(xx, xx), y = c(yy.0, yy.1), Class = rep(c(&#39;Y = 0&#39;, &#39;Y = 1&#39;), c(length(xx), length(xx)))) ggplot(dat_points, aes(x = x, y = y, colour = Class)) + geom_point(size=1.5) + theme_bw() + geom_line(data = dat_poly_plot, aes(x = x, y = y, colour = Class), linewidth = 0.8) + labs(colour = &#39;Class&#39;) Figure 1.2: Illustration of two functions on the interval \\(I = [0, 6]\\), from which we generate observations for classes 0 and 1. Code # Generating functions for Y = 0 and Y = 1 funkce_0 &lt;- poly.0 funkce_1 &lt;- poly.1 Now, we will create a function to generate random functions with added noise (i.e., points on a predetermined grid) from the chosen generating function. The argument t represents the vector of values at which we want to evaluate the functions, fun denotes the generating function, n is the number of functions, and sigma is the standard deviation \\(\\sigma\\) of the normal distribution \\(\\text{N}(\\mu, \\sigma^2)\\) from which we randomly generate Gaussian white noise with \\(\\mu = 0\\). To demonstrate the advantage of using methods that work with functional data, we will also add a random component to each simulated observation that represents a vertical shift of the entire function (the parameter sigma_shift). This shift will be generated from a normal distribution with parameter \\(\\sigma^2 = 4\\). Code generate_values &lt;- function(t, fun, n, sigma, sigma_shift = 0) { # Arguments: # t ... vector of values, where the function will be evaluated # fun ... generating function of t # n ... the number of generated functions / objects # sigma ... standard deviation of normal distribution to add noise to data # sigma_shift ... parameter of normal distribution for generating shift # Value: # X ... matrix of dimension length(t) times n with generated values of one # function in a column X &lt;- matrix(rep(t, times = n), ncol = n, nrow = length(t), byrow = FALSE) noise &lt;- matrix(rnorm(n * length(t), mean = 0, sd = sigma), ncol = n, nrow = length(t), byrow = FALSE) shift &lt;- matrix(rep(rnorm(n, 0, sigma_shift), each = length(t)), ncol = n, nrow = length(t)) return(fun(X) + noise + shift) } Now we can generate functions. In each of the two classes, we will consider 100 observations, thus n = 100. Code # number of generated observations for each class n &lt;- 100 # vector of time points evenly spaced on the interval [0, 6] t &lt;- seq(0, 6, length = 51) # for Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # for Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (not yet smoothed) functions colored by class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 # number of curves we want to plot from each group DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 2.1: The first 10 generated observations from each of the two classification classes. The observed data are not smoothed. 5.1.1 Smoothing Observed Curves Now we will convert the observed discrete values (vectors of values) into functional objects that we will subsequently work with. Again, we will use B-spline basis for smoothing. We take the entire vector t as knots, and since we consider the first derivative, we choose norder = 5. We will penalize the third derivative of the function, as we now require smooth first derivatives as well. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # penalize the 3rd derivative We will find a suitable value of the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), that is, generalized cross-validation. We will consider the value of \\(\\lambda\\) to be the same for both classification groups, as we would not know in advance which value of $to choose for test observations if different values were chosen for each class. Code # combining observations into one matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 1, length.out = 50) # vector of lambdas gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average across all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] For better illustration, we will plot the progression of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.5: The progression of \\(GCV(\\lambda)\\) for the chosen vector \\(\\boldsymbol\\lambda\\). The x-axis values are plotted on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is shown in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically represent the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 1.6: The first 10 smoothed curves from each classification class. Let’s visualize all the smoothed curves along with the mean for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01)) #, limits = c(-1, 2)) Figure 1.7: Plot of all smoothed observed curves, colored by their classification class. The mean for each class is shown with a thick line. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 2.2: Plot of all smoothed observed curves, colored by their classification class. The mean for each class is shown with a thick line. Zoomed view. 5.1.2 Calculation of Derivatives To compute the derivative for the functional object, we will use the deriv.fd() function from the fda package in R. Since we want to classify based on the first derivative, we choose the argument Lfdobj = 1. Code XXder &lt;- deriv.fd(XXfd, 1) Now let’s plot the first few first derivatives for both classification classes. Notice from the figure below that the vertical shift due to differentiation has indeed been successfully removed. However, we have somewhat lost the distinctiveness between the curves because, as implied by the figure, the derivative curves for both classes differ primarily towards the end of the interval, specifically for the argument in the range approximately \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Let’s also illustrate all curves including the average separately for each class. Code abs.labs &lt;- paste(&quot;Classification class:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) # fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = t), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = t)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, #group = interaction(time, group), colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, # y = &quot;$\\\\frac{\\\\text d}{\\\\text d t} x_i(t)$&quot;, y =&quot;$x_i&#39;(t)$&quot;, colour = &#39;Class&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;1&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;tomato&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + # geom_line(data = DFsmooth |&gt; # mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))) |&gt; # filter(group == &#39;0&#39;), # aes(x = t, y = Mean, colour = group), # colour = &#39;deepskyblue2&#39;, linewidth = 0.8, linetype = &#39;solid&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-1.4, 3.5)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Figure 5.1: Plot of all smoothed observed curves, colored differently according to classification class membership. The average for each class is plotted as a solid black line. Code # ggsave(&quot;figures/kap6_sim_04_curves_1der.tex&quot;, device = tikz, width = 8, height = 4) Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1.5, 2)) Figure 5.2: Plot of all smoothed observed curves, colored differently according to classification class membership. The average for each class is plotted as a solid black line. Closer look. 5.1.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training sets library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for logistic regression on scores library(nnet) # for logistic regression on scores library(caret) library(rpart) # decision trees library(rattle) # visualization library(e1071) library(randomForest) # random forest To compare individual classifiers, we will split the generated observations into two parts in a 70:30 ratio for training and testing (validation) sets. The training set will be used to construct the classifier, while the test set will be used to calculate the classification error and potentially other characteristics of our model. The resulting classifiers can then be compared based on these computed characteristics in terms of their classification success. Code # splitting into test and training sets split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, we will examine the representation of individual groups in the test and training portions of the data. Code # absolute representation table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 5.1.3.1 \\(K\\) Nearest Neighbors Let’s start with a non-parametric classification method, specifically the \\(K\\) nearest neighbors method. First, we will create the necessary objects so that we can work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and look at its classification success. The last question remains how to choose the optimal number of neighbors \\(K\\). We could choose this number as the value of \\(K\\) that results in the minimum error rate on the training data. However, this could lead to overfitting the model, so we will use cross-validation. Given the computational complexity and size of the dataset, we will opt for \\(k\\)-fold CV; we will choose a value of \\(k = 10\\). Code # model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # summary of the model # plot(neighb.model$gcv, pch = 16) # plot GCV dependence on the number of neighbors K # neighb.model$max.prob # maximum accuracy (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 12 Let’s proceed with the previous procedure for the training data, which we will split into \\(k\\) parts and repeat this code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # split training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # empty matrix to store the results # columns will contain accuracy values for the corresponding part of the training set # rows will contain values for the given number of neighbors K CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # define the current index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # iterate over each part ... repeat k times for(neighbour in neighbours) { # model for specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predictions on validation set model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # accuracy on validation set accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracy in the position for given K and fold CV.results[neighbour, index] &lt;- accuracy } } # compute average accuracies for individual K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results We can see that the best value for the parameter \\(K\\) is 14, with an error rate calculated using 10-fold CV of 0.2594. For clarity, let’s also plot the validation error rate as a function of the number of neighbors \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 24 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.3: Dependency of validation error rate on the value of \\(K\\), i.e., on the number of neighbors. Now that we have determined the optimal value of the parameter \\(K\\), we can build the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predictions model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # accuracy on test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # error rate # 1 - accuracy Thus, the error rate of the model constructed using the \\(K\\)-nearest neighbors method with the optimal choice of \\(K_{optimal}\\) equal to 14, determined by cross-validation, is 0.3071 on the training data and 0.1833 on the test data. To compare different models, we can use both types of error rates, which we will store in a table for clarity. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 5.1.3.2 Linear Discriminant Analysis As the second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied to functional data, we must first discretize the data, which we will do using Functional Principal Component Analysis (FPCA). We will then perform the classification algorithm on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components together explain at least 90% of the variability in the data. First, let’s perform the functional principal component analysis and determine the number \\(p\\). Code # principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of PCs nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determine p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p PCs data.PCA.train$Y &lt;- factor(Y.train) # class membership In this particular case, we took the number of principal components as \\(p\\) = 3, which together explain 93.96 % of the variability in the data. The first principal component explains 50.6 % and the second 33.44 % of the variability. We can graphically display the scores of the first two principal components, color-coded according to class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Figure 5.4: Scores of the first two principal components for the training data. Points are color-coded according to class membership. To determine the classification accuracy on the test data, we need to calculate the scores for the first 3 principal components for the test data. These scores are determined using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text{ dt}, \\] where \\(\\mu(t)\\) is the mean function and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # compute scores for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of residuals and eigenfunctions (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training portion of the data. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training (31.43 %) and on the test data (23.33 %). To visually represent the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function. Code # add decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # if p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # if p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # if p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.11: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line in the plane of the first two principal components) between the classes constructed using LDA is marked in black. We see that the decision boundary is a line, a linear function in the 2D space, which is indeed what we expected from LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.3 Quadratic Discriminant Analysis Next, we will construct a classifier using Quadratic Discriminant Analysis (QDA). This is an analogous case to LDA, with the difference that we now allow for different covariance matrices for each of the classes from which the corresponding scores are drawn. This relaxed assumption of equal covariance matrices leads to a quadratic boundary between the classes. In R, we perform QDA similarly to how we did LDA in the previous section. We will compute the scores for the training and test functions using the results from the functional Principal Component Analysis (PCA) obtained earlier. Thus, we can proceed directly to constructing the classifier using the qda() function. We will then calculate the accuracy of the classifier on both test and training data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training (35 %) and test data (20 %). To visually represent the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, just like in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 4.1: Scores of the first two principal components, color-coded according to class membership. The decision boundary (parabola in the plane of the first two principal components) between the classes constructed using QDA is marked in black. Notice that the decision boundary between the classification classes is now a parabola. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.4 Logistic Regression We can perform logistic regression in two ways. First, we can use the functional analogue of classical logistic regression, and second, we can apply classical multivariate logistic regression on the scores of the first \\(p\\) principal components. 5.1.3.4.1 Functional Logistic Regression Analogous to the case with finite-dimensional input data, we consider the logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is a linear predictor taking values in the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function (in the case of logistic regression, this is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\)), and \\(\\pi(x)\\) is the conditional probability: \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is a parametric function. Our goal is to estimate this parametric function. For functional logistic regression, we will use the fregre.glm() function from the fda.usc package. First, we will create suitable objects for the classifier construction. Code # create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train$basis To estimate the parametric function \\(\\beta(t)\\), we need to express it in some basis representation, in our case, a B-spline basis. However, we need to determine a suitable number of basis functions. We could determine this based on the error rate on the training data, but this would lead to a preference for selecting a large number of bases, resulting in overfitting. Let us illustrate this with the following case. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\), we will train the model on the training data, determine the error rate on the training data, and also calculate the error rate on the test data. We must remember that we cannot use the same data for estimating the test error rate, as this would underestimate the error rate. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # formula f &lt;- Y ~ x # basis for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) accuracy.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) accuracy.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix pred.baz[as.character(i), ] &lt;- 1 - c(accuracy.train, accuracy.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s visualize the trends of both training and test error rates in a graph based on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error Rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 47 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.13: Dependence of test and training error rates on the number of basis functions for \\(\\beta\\). The red point represents the optimal number \\(n_{optimal}\\) chosen as the minimum test error rate, the black line depicts the test error, and the blue dashed line illustrates the training error rate. We see that as the number of bases for \\(\\beta(t)\\) increases, the training error rate (represented by the blue line) tends to decrease, suggesting that we might choose large values for \\(n_{basis}\\) based solely on it. In contrast, the optimal choice based on the test error rate is \\(n\\) equal to 10, which is significantly smaller than 50. Conversely, as \\(n\\) increases, the test error rate rises, indicating overfitting of the model. For these reasons, we will use 10-fold cross-validation to determine the optimal number of basis functions for \\(\\beta(t)\\). The maximum number of basis functions considered is 35, as we observed that exceeding this value leads to overfitting. Code ### 10-fold cross-validation n.basis.max &lt;- 35 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # divide the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that do not change during the loop # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline basis basis1 &lt;- X.train$basis # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to store results # columns will contain accuracy values for the respective training subset # rows will contain values for the respective number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now that we have everything prepared, we will calculate the error rates for each of the ten subsets of the training set. Subsequently, we will determine the average error and take the argument of the minimum validation error as the optimal \\(n\\). Code for (index in 1:k_cv) { # define the index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on the validation subset newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) accuracy.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- accuracy.valid } } # calculate average accuracies for each n across folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Let’s plot the validation error rates, highlighting the optimal value of \\(n_{optimal}\\), which is 14, with a validation error rate of 0.0684. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = n.basis) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 32 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.15: Dependence of validation error on the value of \\(n_{basis}\\), i.e., on the number of bases. We can now define the final model using functional logistic regression, choosing the B-spline basis for \\(\\beta(t)\\) with 14 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the training error rate (which is 5 %) and the test error rate (which is 11.67 %). For better visualization, we can also plot the estimated probabilities of belonging to the classification class \\(Y = 1\\) on the training data against the values of the linear predictor. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probability Pr(Y = 1|X = x)&#39;, colour = &#39;Class&#39;) Figure 5.5: Dependence of estimated probabilities on the values of the linear predictor. Points are color-coded according to their classification class. For informational purposes, we can also display the progression of the estimated parametric function \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Figure 5.6: Plot of the estimated parametric function \\(\\beta(t), t \\in [0, 6]\\). Finally, we will add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.4.2 Logistic Regression with Principal Component Analysis To construct this classifier, we need to perform functional principal component analysis, determine the appropriate number of components, and calculate the score values for the test data. We have already completed this in the linear discriminant analysis section, so we will use these results in the following section. We can directly construct the logistic regression model using the glm(, family = binomial) function. Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # accuracy on training data predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (31.43 %) and on the test data (23.33 %). For graphical representation of the method, we can plot the decision boundary in the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, just as we did in the LDA and QDA cases. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 5.7: Scores of the first two principal components, color-coded according to classification class. The decision boundary (a line in the plane of the first two principal components) between classes is indicated in black, constructed using logistic regression. Note that the decision boundary between the classification classes is now a line, similar to the case with LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.5 Decision Trees In this section, we will look at a very different approach to constructing a classifier compared to methods such as LDA or logistic regression. Decision trees are a very popular tool for classification; however, like some of the previous methods, they are not directly designed for functional data. There are, however, procedures to convert functional objects into multidimensional ones, allowing us to apply decision tree algorithms. We can consider the following approaches: An algorithm built on basis coefficients, Utilizing principal component scores, Discretizing the interval and evaluating the function only on a finite grid of points. We will first focus on discretizing the interval and then compare the results with the other two approaches to constructing decision trees. 5.1.3.5.1 Interval Discretization First, we need to define points from the interval \\(I = [0, 6]\\), where we will evaluate the functions. Next, we will create an object where the rows represent the individual (discretized) functions and the columns represent time. Finally, we will add a column \\(Y\\) containing information about the classification class and repeat the same for the test data. Code # sequence of points at which we will evaluate the functions t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can construct a decision tree where all times from the vector t.seq will serve as predictors. This classification method is not susceptible to multicollinearity, so we do not need to worry about it. We will choose accuracy as the metric. Code # model construction clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the classifier on the test data is thus 18.33 %, and on the training data 26.43 %. We can visualize the decision tree graphically using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.18: Graphical representation of the unpruned decision tree. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.8: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - discr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.5.2 Principal Component Scores Another option for constructing a decision tree is to use principal component scores. Since we have already calculated the scores for the previous classification methods, we will utilize this knowledge and construct a decision tree based on the scores of the first 3 principal components. Code # model construction clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the test data is thus 23.33 %, and on the training data 30 %. We can visualize the decision tree constructed on the principal component scores using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.20: Graphical representation of the unpruned decision tree constructed on principal component scores. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # final model extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 3.1: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.5.3 Basis Coefficients The final option we will utilize for constructing a decision tree is to use coefficients in the representation of functions in the B-spline basis. First, let’s define the necessary datasets with the coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # test dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now we can construct the classifier. Code # model construction clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the training data is thus 26.43 %, and on the test data 18.33 %. We can visualize the decision tree constructed on the B-spline coefficient representation using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.22: Graphical representation of the unpruned decision tree constructed on basis coefficients. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # final model extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.9: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.6 Random Forests The classifier constructed using the random forests method consists of building several individual decision trees, which are then combined to create a common classifier (via “voting”). As with decision trees, we have several options regarding which data (finite-dimensional) we will use to construct the model. We will again consider the three approaches discussed above. The datasets with the corresponding variables for all three approaches have already been prepared from the previous section, so we can directly construct the models, calculate the characteristics of the classifiers, and add the results to the summary table. 5.1.3.6.1 Interval Discretization In the first case, we utilize the evaluation of functions on a given grid of points over the interval \\(I = [0, 6]\\). Code # model construction clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 0 %, and on the test data 20 %. Code Res &lt;- data.frame(model = &#39;RForest - discretization&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.6.2 Principal Component Scores In this case, we will use the scores of the first $p = $ 3 principal components. Code # model construction clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate on the training data is thus 1.43 %, and on the test data 30 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.6.3 Basis Coefficients Finally, we will use the representation of functions through the B-spline basis. Code # model construction clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 0 %, and on the test data 18.33 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7 Support Vector Machines Now let’s look at classifying our simulated curves using the Support Vector Machines (SVM) method. The advantage of this classification method is its computational efficiency, as it defines the boundary curve between classes using only a few (often very few) observations. In the case of functional data, we have several options for applying the SVM method. The simplest variant is to use this classification method directly on the discretized function (section 5.1.3.7.2). Another option is to utilize the principal component scores to classify curves based on their representation 5.1.3.7.3. A straightforward variant is to use the representation of curves through the B-spline basis and classify curves based on the coefficients of their representation in this basis (section 5.1.3.7.4). A more complex consideration can lead us to several additional options that leverage the functional nature of the data. We can utilize projections of functions onto a subspace generated, for example, by B-spline functions (section 5.1.3.7.5). The final method we will use for classifying functional data involves combining projection onto a certain subspace generated by functions (Reproducing Kernel Hilbert Space, RKHS) and classifying the corresponding representation. This method utilizes not only the classical SVM but also SVM for regression, as discussed in section RKHS + SVM 5.1.3.7.6. 5.1.3.7.1 SVM for Functional Data In the fda.usc library, we will use the function classif.svm() to apply the SVM method directly to functional data. First, we will create suitable objects for constructing the classifier. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = TRUE) # split into test and training sets X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train_norm$basis Finally, we can construct the classifiers on the entire training data with the hyperparameter values (determined previously by CV). We will also determine the errors on the test and training data. Code # Create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # Points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) Code model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = 10) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = 3, coef0 = 1, cost = 10) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = 0.001, cost = 100) ## Warning in Minverse(t(B) %*% B): System is computationally singular (rank 54) ## ## The matrix inverse is computed by svd (effective rank 52) Code # Accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # Accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) The error rate of the SVM method on the training data is thus 15.7143 % for the linear kernel, 12.1429 % for the polynomial kernel, and 15.7143 % for the Gaussian kernel. On the test data, the error rate of the method is 16.6667 % for the linear kernel, 25 % for the polynomial kernel, and 23.3333 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.2 Interval Discretization Let’s continue by applying the Support Vector Machines method directly to the discretized data (evaluation of the function on a grid of points over the interval \\(I = [0, 6]\\)), considering all three aforementioned kernel functions. We will now classify the normalized data using the classic SVM method, selecting parameters as follows. For one generated dataset, we will determine parameters using cross-validation (CV), and these parameters will then be applied to other simulated data. Code # model construction clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is thus 7.86 % for the linear kernel, 12.86 % for the polynomial kernel, and 14.29 % for the Gaussian kernel. On the test data, the error rates are 10 % for the linear kernel, 23.33 % for the polynomial kernel, and 23.33 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discretization&#39;, &#39;SVM poly - discretization&#39;, &#39;SVM rbf - discretization&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.3 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 3 principal components. Code # model construction clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) accuracy.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) accuracy.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) accuracy.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) accuracy.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) accuracy.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) accuracy.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the principal component scores on the training data is therefore 31.43 % for the linear kernel, 31.43 % for the polynomial kernel, and 32.14 % for the Gaussian kernel. On the test data, the error rate is then 23.33 % for the linear kernel, 23.33 % for the polynomial kernel, and 23.33 % for the radial kernel. To graphically illustrate the method, we can mark the decision boundary on the graph of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, just as we did in previous cases when plotting the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;))))) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;, linetype = &#39;Kernel type&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 5.10: Scores of the first two principal components, color-coded according to classification group membership. The decision boundary (line or curves in the plane of the first two principal components) between classes constructed using the SVM method is highlighted in black. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.4 B-spline Coefficients Finally, we will use function representations through the B-spline basis. Code # Building the model clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) accuracy.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) accuracy.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) accuracy.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) accuracy.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) accuracy.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) accuracy.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the B-spline coefficients on the training data is therefore 7.14 % for the linear kernel, 20 % for the polynomial kernel, and 12.86 % for the Gaussian kernel. On the test data, the error rate of the method is 6.67 % for the linear kernel, 21.67 % for the polynomial kernel, and 20 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.5 Projection onto B-spline Basis Another option for using the classical SVM method for functional data is to project the original data onto some \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal{H}\\), denoted as \\(V_d\\). Assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), so we can write: \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Now we can use the coefficients from the orthogonal projection for classification, that is, we apply the standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By using this transformation, we have defined a new so-called adapted kernel, which consists of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector method. Thus, we have (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This is a dimensionality reduction method, which we can call filtering. For the projection itself, we will use the project.basis() function from the fda library in R. Its input will be a matrix of the original discrete (non-smoothed) data, the values at which we measure values in the original data matrix, and the basis object onto which we want to project the data. We will choose projection onto a B-spline basis since the use of a Fourier basis is not suitable for our non-periodic data. We choose the dimension \\(d\\) either from some prior expert knowledge or by using cross-validation. In our case, we will determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (we choose \\(k \\ll n\\) due to the computational intensity of the method, often \\(k = 5\\) or \\(k = 10\\)). We require B-splines of order 4, for which the relationship for the number of basis functions holds: \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). Therefore, the minimum dimension (for \\(n_{breaks} = 1\\)) is chosen as \\(n_{basis} = 3\\), and the maximum (for \\(n_{breaks} = 51\\), corresponding to the number of original discrete data points) is \\(n_{basis} = 53\\). However, in R, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and for large values of \\(n_{basis}\\), we already experience model overfitting; therefore, we choose a maximum \\(n_{basis}\\) of a smaller number, say 43. Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # all dimensions we want to try # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components ... matrices for individual kernels -&gt; linear, poly, radial # An empty matrix where we will insert individual results # Columns will contain accuracy values for each part of the training set # Rows will contain values for each dimension value CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Projection of discrete data onto the B-spline basis of dimension d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matrix of discrete data argvals = t.seq, # vector of arguments basisobj = bbasis) # basis object # Splitting into training and test data within CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Definition of test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Building the models clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) accuracy.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) accuracy.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) accuracy.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.r } } # Compute average accuracies for individual d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 11 0.1890476 ## poly 13 0.1869963 ## radial 7 0.2012821 We see that the best value for the parameter \\(d\\) is 11 for the linear kernel, with an error rate calculated using 10-fold CV of 0.189, 13 for the polynomial kernel with an error rate of 0.187, and 7 for the radial kernel with an error rate of 0.2013. To clarify, let’s plot the validation error rates as a function of the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation error rate&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 1.23: Dependency of validation error rate on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The optimal values of the dimension \\(V_d\\) for each kernel function are marked with black points. Now we can train the individual classifiers on all training data and examine their performance on the test data. For each kernel function, we choose the dimension of the subspace to project onto according to the results of cross-validation. The variable Projection stores the matrix of coefficients from the orthogonal projection, that is, \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Loop through each kernel for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Base object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Project discrete data onto B-spline basis Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # Matrix of discrete data argvals = t.seq, # Vector of arguments basisobj = bbasis) # Basis object # Split into training and testing data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Construct the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is therefore 12.14 % for the linear kernel, 12.86 % for the polynomial kernel, and 13.57 % for the Gaussian kernel. On the test data, the error rates are 20 % for the linear kernel, 25 % for the polynomial kernel, and 28.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.6 RKHS + SVM In this section, we will explore another way to utilize support vector machines (SVM) for classifying functional data. Here, we will again rely on the familiar principle of first expressing functional data as finite-dimensional objects and then applying the traditional SVM method to these objects. However, this time we will use the SVM method for the representation of functional data itself via a certain finite-dimensional object. As the name suggests, this involves a combination of two concepts: the support vector machine method and a space referred to in English literature as Reproducing Kernel Hilbert Space (RKHS). A key concept in this space is the kernel. 5.1.3.7.6.1 Implementation of the Method in R From the last part of Theorem 1.3, we can see how to compute the representations of curves in practice. We will work with discretized data after smoothing the curves. First, let’s define a kernel for the RKHS space. We will use the Gaussian kernel with a parameter \\(\\gamma\\). The value of this hyperparameter significantly affects the behavior and success of the method, so we must pay special attention to its choice (we select it using cross-validation). 5.1.3.7.6.2 Gaussian Kernel Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s compute the matrix \\(K_S\\) along with its eigenvalues and corresponding eigenvectors. Code # Compute the matrix K gamma &lt;- 0.1 # Fixed value for gamma; optimal will be determined using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and vectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To compute the coefficients in the representation of the curves, that is, to calculate the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we also need the coefficients from SVM. Unlike the classification problem, we are now solving a regression problem, as we are trying to express our observed curves in some basis chosen by the kernel \\(K\\). Therefore, we will use the Support Vector Regression method, from which we will obtain the coefficients \\(\\alpha_{il}\\). Code # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } Now we can compute the representations of the individual curves. First, let’s choose \\(\\hat d\\) to be the entire dimension, that is, \\(\\hat d = m ={}\\) 101, and then determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # Determine the vector lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have stored the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) for each curve in the Lambda.RKHS matrix. We will use these vectors as representations of the given curves and classify the data based on this discretization. Code # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Construct the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 5.1: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.4833 SVM poly - RKHS 0.0000 0.4167 SVM rbf - RKHS 0.0214 0.3000 We observe that the model performs very well on the training data for all three kernels, while its success on the testing data is not good at all. It is evident that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values of \\(\\gamma\\) and \\(d\\). Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:40 # Reasonable range of values for d gamma.cv &lt;- 10^seq(-2, 3, length = 15) # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix where we will store individual results # Columns will represent accuracy values for given gamma, and rows will correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies for the respective d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.2: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 34 1000.0000 0.1271 linear poly 37 1000.0000 0.1863 polynomial radial 10 0.2683 0.1785 radial We see that the best parameter value is \\(d={}\\) 34 and \\(\\gamma={}\\) 1000 for the linear kernel with an error value calculated using 10-fold CV of 0.1271, \\(d={}\\) 37 and \\(\\gamma={}\\) 1000 for the polynomial kernel with an error value calculated using 10-fold CV of 0.1863 and \\(d={}\\) 10 and \\(\\gamma={}\\) 0.2683 for the radial kernel with an error value of 0.1785. For curiosity, let’s also plot the validation error function depending on the dimension \\(d\\) and the hyperparameter value \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 5.11: Dependency of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. Since we have already found the optimal values for the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { # Calculate the K matrix gamma &lt;- gamma.opt[kernel_number] # Gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine the alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create empty object # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 1.1: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimated training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.1071 0.2833 SVM poly - RKHS - radial 0.1429 0.2667 SVM rbf - RKHS - radial 0.1357 0.3167 The error rate of the SVM method combined with the projection on the Reproducing Kernel Hilbert Space is thus equal to 10.71 % for the linear kernel, 14.29 % for the polynomial kernel, and 13.57 % for the Gaussian kernel on the training data. On the testing data, the error rate of the method is 28.33 % for the linear kernel, 26.67 % for the polynomial kernel, and 31.67 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.6.3 Polynomial Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Include test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:40 # Reasonable range of d values poly.cv &lt;- 2:5 # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to insert individual results # Columns will hold accuracy values for given parameters # Rows will hold values for given p and layers corresponding to folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies in positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.3: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error and \\(\\widehat{Err}_{test}\\) the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 40 5 0.1940 linear poly 3 5 0.1874 polynomial radial 21 3 0.2068 radial We see that the best values for parameter \\(d={}\\) 40 and \\(p={}\\) 5 are for the linear kernel with an error calculated using 10-fold CV 0.194, \\(d={}\\) 3 and \\(p={}\\) 5 for the polynomial kernel with an error calculated using 10-fold CV 0.1874, and \\(d={}\\) 21 and \\(p={}\\) 3 for the radial kernel with an error 0.2068. Since we have found the optimal values for the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # Iterate over the individual kernels for (kernel_number in 1:3) { # Calculate the matrix K p &lt;- poly.opt[kernel_number] # CV-derived parameter value K &lt;- Kernel.RKHS(t.seq, p = p) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model fitting for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 5.4: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0929 0.3 SVM poly - RKHS - poly 0.1714 0.3 SVM rbf - RKHS - poly 0.1357 0.3 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus on the training data equal to 9.29 % for the linear kernel, 17.14 % for the polynomial kernel, and 13.57 % for the Gaussian kernel. On the test data, the error rate of the method is 30 % for the linear kernel, 30 % for the polynomial kernel, and 30 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.7.6.4 Linear Kernel Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Values of hyperparameters that we will traverse dimensions &lt;- 3:40 # Reasonable range of values for d # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store the individual results # In columns, there will be accuracy values for given d # In rows, there will be values for layers corresponding to folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Traverse dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculation of representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Traverse folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Traverse individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # Store accuracies in positions for given d, gamma, and fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.5: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 9 0.3317 linear poly 35 0.3645 polynomial radial 10 0.3240 radial We see that the optimal parameter value is \\(d={}\\) 9 for the linear kernel with an error rate calculated using 10-fold CV of 0.3317, \\(d={}\\) 35 for the polynomial kernel with an error rate of 0.3645, and \\(d={}\\) 10 for the radial kernel with an error rate of 0.324. Now that we have found the optimal hyperparameter values, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # Iterate over the individual kernels for (kernel_number in 1:3) { # Compute the K matrix K &lt;- Kernel.RKHS(t.seq) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store the results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 5.6: Summary of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.3071 0.3000 SVM rbf - RKHS - linear 0.3071 0.2333 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 30 % for the linear kernel, 30.71 % for the polynomial kernel, and 30.71 % for the Gaussian kernel. For the test data, the error rate is 28.33 % for the linear kernel, 30 % for the polynomial kernel, and 23.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.1.3.8 Results Table Table 5.7: Summary of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.3071 0.1833 LDA 0.3143 0.2333 QDA 0.3500 0.2000 LR functional 0.0500 0.1167 LR score 0.3143 0.2333 Tree - discr. 0.2643 0.1833 Tree - score 0.3000 0.2333 Tree - Bbasis 0.2643 0.1833 RForest - discretization 0.0000 0.2000 RForest - score 0.0143 0.3000 RForest - Bbasis 0.0000 0.1833 SVM linear - func 0.1571 0.1667 SVM poly - func 0.1214 0.2500 SVM rbf - func 0.1571 0.2333 SVM linear - discretization 0.0786 0.1000 SVM poly - discretization 0.1286 0.2333 SVM rbf - discretization 0.1429 0.2333 SVM linear - PCA 0.3143 0.2333 SVM poly - PCA 0.3143 0.2333 SVM rbf - PCA 0.3214 0.2333 SVM linear - Bbasis 0.0714 0.0667 SVM poly - Bbasis 0.2000 0.2167 SVM rbf - Bbasis 0.1286 0.2000 SVM linear - projection 0.1214 0.2000 SVM poly - projection 0.1286 0.2500 SVM rbf - projection 0.1357 0.2833 SVM linear - RKHS - radial 0.1071 0.2833 SVM poly - RKHS - radial 0.1429 0.2667 SVM rbf - RKHS - radial 0.1357 0.3167 SVM linear - RKHS - poly 0.0929 0.3000 SVM poly - RKHS - poly 0.1714 0.3000 SVM rbf - RKHS - poly 0.1357 0.3000 SVM linear - RKHS - linear 0.3000 0.2833 SVM poly - RKHS - linear 0.3071 0.3000 SVM rbf - RKHS - linear 0.3071 0.2333 5.1.4 Simulation Study In the entire previous section, we focused only on one randomly generated set of functions from two classification classes, which we then randomly split into test and training parts. We evaluated the individual classifiers obtained using the considered methods based on test and training error rates. Since the generated data (and their division into two parts) can differ significantly with each repetition, the error rates of the individual classification algorithms may vary greatly as well. Therefore, drawing any conclusions about the methods and comparing them based on a single generated data set can be very misleading. For this reason, in this section, we will repeat the entire previous procedure for different generated data sets. We will store the results in a table and ultimately calculate the average characteristics of the models across individual repetitions. To ensure our conclusions are sufficiently general, we will choose the number of repetitions as \\(n_{sim} = 100\\). Code # Set seed for the pseudorandom number generator set.seed(42) # Number of simulations n.sim &lt;- 100 ## List to store error rates # Columns will represent methods # Rows will represent individual repetitions # The list has two entries: train and test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # Object to store optimal hyperparameter values determined via CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Now we will repeat the entire previous section 100 times, and we will store the error rates in the list SIMULACE. We will also store the optimal hyperparameter values in the data table CV_RESULTS — for the \\(K\\) nearest neighbors method and for SVM, the value of dimension \\(d\\) in the case of projection onto the B-spline basis. Additionally, we will save all hyperparameter values for the SVM + RKHS method. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 5 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(3) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -3, to = 2, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 1) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train$basis ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline baze basis1 &lt;- X.train$basis # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() ### 7.0) SVM for functional data # vytvorime vhodne objekty x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = 10) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = 3, coef0 = 1, cost = 10) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = 0.001, cost = 100) # presnost na trenovacich datech newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.1) Diskretizace intervalu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 25, kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.7, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = 55, gamma = 0.0005, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 0.1, kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.01, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 1, gamma = 0.01, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 50, kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = 0.1, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = 100, gamma = 0.001, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 #length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-1, 2, length = 5) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_cv.RData&#39;) Calculate average training and testing error rates for each classification method. Code SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # Save the final values save(SIMULACE.df, file = &#39;RData/simulace_04_res_cv.RData&#39;) 5.1.4.1 Results Table 1.7: Summary of results for the methods used on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error rate, \\(\\widehat{Err}_{test}\\) indicates the test error rate, \\(\\widehat{SD}_{train}\\) is the estimated standard deviation of training errors, and \\(\\widehat{SD}_{test}\\) is the estimated standard deviation of test errors. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.2196 0.2377 0.0476 0.0685 LDA 0.2204 0.2370 0.0453 0.0674 QDA 0.2130 0.2432 0.0468 0.0647 LR_functional 0.0418 0.1037 0.0259 0.0432 LR_score 0.2213 0.2360 0.0462 0.0664 Tree_discr 0.1754 0.2538 0.0587 0.0740 Tree_score 0.2076 0.2665 0.0590 0.0755 Tree_Bbasis 0.1729 0.2448 0.0464 0.0658 RF_discr 0.0086 0.2257 0.0074 0.0778 RF_score 0.0316 0.2542 0.0182 0.0729 RF_Bbasis 0.0104 0.2232 0.0078 0.0773 SVM linear - func 0.1073 0.1507 0.0488 0.0508 SVM poly - func 0.0671 0.2277 0.0540 0.0704 SVM rbf - func 0.1273 0.1713 0.0534 0.0726 SVM linear - diskr 0.0615 0.1095 0.0275 0.0372 SVM poly - diskr 0.1040 0.2038 0.0547 0.0735 SVM rbf - diskr 0.1278 0.1707 0.0507 0.0680 SVM linear - PCA 0.2224 0.2392 0.0452 0.0643 SVM poly - PCA 0.2296 0.2757 0.0463 0.0729 SVM rbf - PCA 0.2218 0.2448 0.0457 0.0637 SVM linear - Bbasis 0.0514 0.1000 0.0257 0.0420 SVM poly - Bbasis 0.1464 0.2133 0.0486 0.0699 SVM rbf - Bbasis 0.1051 0.1702 0.0494 0.0677 SVM linear - projection 0.1109 0.1448 0.0459 0.0628 SVM poly - projection 0.0884 0.1880 0.0555 0.0706 SVM rbf - projection 0.1169 0.1835 0.0522 0.0701 SVM linear - RKHS - radial 0.1036 0.1695 0.0455 0.0615 SVM poly - RKHS - radial 0.0646 0.1992 0.0421 0.0653 SVM rbf - RKHS - radial 0.0961 0.1868 0.0463 0.0734 SVM linear - RKHS - poly 0.1274 0.2293 0.0602 0.0826 SVM poly - RKHS - poly 0.0759 0.2400 0.0547 0.0831 SVM rbf - RKHS - poly 0.1305 0.2200 0.0499 0.0878 SVM linear - RKHS - linear 0.2758 0.3300 0.0865 0.1030 SVM poly - RKHS - linear 0.2329 0.3328 0.1009 0.0966 SVM rbf - RKHS - linear 0.2602 0.3213 0.0831 0.0995 The table above lists all computed characteristics. Standard deviations are also included to compare the stability or variability of the individual methods. Finally, we can graphically display the calculated values from the simulation for each classification method using box plots, separately for training and testing errors. Code # Set the names of classification methods differently methods_names &lt;- c( &#39;$K$ nearest neighbors&#39;, &#39;Linear Discriminant Analysis&#39;, &#39;Quadratic Discriminant Analysis&#39;, &#39;Functional Logistic Regression&#39;, &#39;Logistic Regression with fPCA&#39;, &#39;Decision Tree -- Discretization&#39;, &#39;Decision Tree -- fPCA&#39;, &#39;Decision Tree -- Basis Coefficients&#39;, &#39;Random Forest -- Discretization&#39;, &#39;Random Forest -- fPCA&#39;, &#39;Random Forest -- Basis Coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- Discretization&#39;, &#39;SVM (poly) -- Discretization&#39;, &#39;SVM (radial) -- Discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- Basis Coefficients&#39;, &#39;SVM (poly) -- Basis Coefficients&#39;, &#39;SVM (radial) -- Basis Coefficients&#39;, &#39;SVM (linear) -- Projection&#39;, &#39;SVM (poly) -- Projection&#39;, &#39;SVM (radial) -- Projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # Alpha for box plots box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # For training data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Figure 5.12: Box plots of training errors for 100 simulations separately for each classification method. The averages are marked with black \\(+\\) symbols. Code # For testing data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 50, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 3, color = &quot;black&quot;, alpha = 0.9) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Figure 5.13: Box plots of testing errors for 100 simulations separately for each classification method. The averages are marked with black \\(+\\) symbols. We would now like to formally test whether some classification methods are better than others based on the previous simulation on this data, or show that we can consider them equally successful. Due to the violation of normality assumptions, we cannot use the classic paired t-test. We will use its non-parametric alternative—the paired Wilcoxon test. However, we must be cautious in our interpretation. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.1473608 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.2346987 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - diskr&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 0.002554202 We are testing at the adjusted significance level \\(\\alpha_{adj} = 0.05 / 3 = 0.0167\\). Table 1.9: Medians of hyperparameter values for selected methods where a hyperparameter was determined using cross-validation. Median Hyperparameter Value KNN_K 12.0 nharm 3.0 LR_func_n_basis 11.0 SVM_d_Linear 11.0 SVM_d_Poly 11.0 SVM_d_Radial 11.0 SVM_RKHS_radial_gamma1 3.2 SVM_RKHS_radial_gamma2 3.2 SVM_RKHS_radial_gamma3 3.2 SVM_RKHS_radial_d1 15.0 SVM_RKHS_radial_d2 15.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 3.0 SVM_RKHS_poly_p3 3.0 SVM_RKHS_poly_d1 25.0 SVM_RKHS_poly_d2 27.5 SVM_RKHS_poly_d3 25.0 SVM_RKHS_linear_d1 15.0 SVM_RKHS_linear_d2 20.0 SVM_RKHS_linear_d3 20.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.14: Histograms of hyperparameter values for KNN, functional logistic regression, and the number of principal components. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.15: Histograms of hyperparameter values for SVM with projection onto B-spline basis. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.26: Histograms of hyperparameter values for RKHS + SVM with radial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 1.27: Histograms of hyperparameter values for RKHS + SVM with polynomial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.16: Histograms of hyperparameter values for RKHS + SVM with linear kernel. 5.2 Classification Based on the Second Derivative In the previous section, we considered the first derivative of the curves. Now, let’s repeat the entire process with the second derivatives. In each of the two classes, we will consider 100 observations, i.e., n = 100. Code # number of generated observations for each class set.seed(42) n &lt;- 100 # equidistant time vector on the interval [0, 6] t &lt;- seq(0, 6, length = 51) # for Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # for Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) We will plot the generated (yet unsmoothed) functions in color according to the class (only the first 10 observations from each class for clarity). Code n_curves_plot &lt;- 10 # number of curves to be plotted from each group DF0 &lt;- cbind(t, X0[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 0 ) DF1 &lt;- cbind(t, X1[, 1:n_curves_plot]) |&gt; as.data.frame() |&gt; reshape(varying = 2:(n_curves_plot + 1), direction = &#39;long&#39;, sep = &#39;&#39;) |&gt; subset(select = -id) |&gt; mutate( time = time - 1, group = 1 ) DF &lt;- rbind(DF0, DF1) |&gt; mutate(group = factor(group)) DF |&gt; ggplot(aes(x = t, y = V, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 5.17: First 10 generated observations from each of the two classification classes. Observed data are not smoothed. 5.2.1 Smoothing of Observed Curves Now we convert the observed discrete values (vectors of values) into functional objects, which we will work with subsequently. We will again use the B-spline basis for smoothing. We take the entire vector t as knots; since we are considering the first derivative, we set norder = 6. We will penalize the fourth derivative of the functions, as we now require smooth second derivatives as well. Code rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalizing the 4th derivative We will find a suitable value for the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), i.e., generalized cross-validation. We will consider the same value of \\(\\lambda\\) for both classification groups, as we would not know in advance which value of \\(\\lambda\\) to choose for test observations if different values were chosen for each class. Code # combining observations into a single matrix XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4, to = -2, length.out = 50) # lambda vector gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for storing GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average over all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # finding the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] For better visualization, we will plot the course of \\(GCV(\\lambda)\\). Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.5) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 2.5) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.18: Course of \\(GCV(\\lambda)\\) for the chosen vector \\(\\boldsymbol\\lambda\\). Values on the \\(x\\) axis are displayed on a logarithmic scale. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is shown in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions and again graphically illustrate the first 10 observed curves from each classification class. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Figure 5.19: First 10 smoothed curves from each classification class. Let’s also illustrate all curves, including the mean, separately for each class. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01))#, limits = c(-1, 2)) Figure 5.20: Plot of all smoothed observed curves, with curves color-coded by classification class. The mean for each class is shown with a thick line. Code DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean, colour = group), linewidth = 1.2, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + #ylim(c(-1, 2)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-1, 2)) Figure 5.21: Plot of all smoothed observed curves, with curves color-coded by classification class. The mean for each class is shown with a thick line. Close-up view. 5.2.2 Derivative Calculation To calculate the derivative for a functional object, we use the deriv.fd() function from the fda package in R. Since we aim to classify based on the second derivative, we set the argument Lfdobj = 2. Code XXder &lt;- deriv.fd(XXfd, 2) Now, let’s plot the first few derivatives for both classification classes. Notice from the figure below that the vertical shift was effectively removed through differentiation. However, this also reduced the distinctiveness between the curves, as the differences in the derivative curves between the two classes primarily occur toward the end of the interval, approximately in the range \\([5, 6]\\). Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DF$Vsmooth &lt;- c(fdobjSmootheval[, c(1 : n_curves_plot, (n + 1) : (n + n_curves_plot))]) DF |&gt; ggplot(aes(x = t, y = Vsmooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.75) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels=c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) Let’s now illustrate all curves, including the mean for each class separately. Code abs.labs &lt;- paste(&quot;Class:&quot;, c(&quot;$Y = 0$&quot;, &quot;$Y = 1$&quot;)) names(abs.labs) &lt;- c(&#39;0&#39;, &#39;1&#39;) tt &lt;- seq(min(t), max(t), length = 91) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = tt) DFsmooth &lt;- data.frame( t = rep(tt, 2 * n), time = rep(rep(1:n, each = length(tt)), 2), Smooth = c(fdobjSmootheval), group = factor(rep(c(0, 1), each = n * length(tt))) ) DFmean &lt;- data.frame( t = rep(tt, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[1:n]), evalarg = tt), eval.fd(fdobj = mean.fd(XXder[(n + 1):(2 * n)]), evalarg = tt)), group = factor(rep(c(0, 1), each = length(tt))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, colour = group)) + geom_line(aes(group = time), linewidth = 0.05, alpha = 0.5) + theme_bw() + labs(x = &quot;$t$&quot;, y = &quot;$x_i&#39;(t)$&quot;, colour = &#39;Class&#39;) + geom_line(data = DFmean |&gt; mutate(group = factor(ifelse(group == &#39;0&#39;, &#39;1&#39;, &#39;0&#39;))), aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 0.8, linetype = &#39;dashed&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean, colour = group), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + facet_wrap(~group, labeller = labeller(group = abs.labs)) + scale_y_continuous(expand = c(0.02, 0.02)) + theme(legend.position = &#39;none&#39;, plot.margin = unit(c(0.1, 0.1, 0.3, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(-20, 20)) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Figure 5.22: Plotting all smoothed observed curves, with color differentiation according to class. The mean for each class is shown by a black line. Code fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) DFsmooth &lt;- data.frame( t = rep(t, 2 * n), time = rep(rep(1:n, each = length(t)), 2), Smooth = c(fdobjSmootheval), Mean = c(rep(apply(fdobjSmootheval[ , 1 : n], 1, mean), n), rep(apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean), n)), group = factor(rep(c(0, 1), each = n * length(t))) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(apply(fdobjSmootheval[ , 1 : n], 1, mean), apply(fdobjSmootheval[ , (n + 1) : (2 * n)], 1, mean)), group = factor(rep(c(0, 1), each = length(t))) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, group = interaction(time, group), colour = group)) + geom_line(linewidth = 0.25, alpha = 0.5) + theme_bw() + labs(x = expression(x[1]), y = expression(x[2]), colour = &#39;Class&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_line(aes(x = t, y = Mean), colour = &#39;grey3&#39;, linewidth = 0.7, linetype = &#39;dashed&#39;) + scale_x_continuous(expand = c(0.01, 0.01)) + scale_y_continuous(expand = c(0.01, 0.01), limits = c(-12, 10)) Figure 1.29: Plotting all smoothed observed curves, with color differentiation according to class. The mean for each class is shown by a black line. Zoomed view. 5.2.3 Classification of Curves First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training sets library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for logistic regression on scores library(nnet) # for logistic regression on scores library(caret) library(rpart) # decision trees library(rattle) # visualization library(e1071) library(randomForest) # random forest To compare individual classifiers, we will split the generated observations into two parts in a 70:30 ratio for training and testing (validation) sets. The training set will be used to construct the classifier, while the test set will be used to calculate the classification error and potentially other characteristics of our model. The resulting classifiers can then be compared based on these computed characteristics in terms of their classification success. Code # splitting into test and training sets split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Next, we will examine the representation of individual groups in the test and training portions of the data. Code # absolute representation table(Y.train) ## Y.train ## 0 1 ## 71 69 Code table(Y.test) ## Y.test ## 0 1 ## 29 31 Code # relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.5071429 0.4928571 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.4833333 0.5166667 5.2.3.1 \\(K\\) Nearest Neighbors Let’s start with a non-parametric classification method, specifically the \\(K\\) nearest neighbors method. First, we will create the necessary objects so that we can work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and look at its classification success. The last question remains how to choose the optimal number of neighbors \\(K\\). We could choose this number as the value of \\(K\\) that results in the minimum error rate on the training data. However, this could lead to overfitting the model, so we will use cross-validation. Given the computational complexity and size of the dataset, we will opt for \\(k\\)-fold CV; we will choose a value of \\(k = 10\\). Code # model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) # summary(neighb.model) # summary of the model # plot(neighb.model$gcv, pch = 16) # plot GCV dependence on the number of neighbors K # neighb.model$max.prob # maximum accuracy (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 12 Let’s proceed with the previous procedure for the training data, which we will split into \\(k\\) parts and repeat this code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # split training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # empty matrix to store the results # columns will contain accuracy values for the corresponding part of the training set # rows will contain values for the given number of neighbors K CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # define the current index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # iterate over each part ... repeat k times for(neighbour in neighbours) { # model for specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predictions on validation set model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # accuracy on validation set accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracy in the position for given K and fold CV.results[neighbour, index] &lt;- accuracy } } # compute average accuracies for individual K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) presnost.opt.cv &lt;- max(CV.results) # CV.results We can see that the best value for the parameter \\(K\\) is 12, with an error rate calculated using 10-fold CV of 0.1617. For clarity, let’s also plot the validation error rate as a function of the number of neighbors \\(K\\). Code CV.results &lt;- data.frame(K = neighbours, CV = CV.results) CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 24 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.23: Dependency of validation error rate on the value of \\(K\\), i.e., on the number of neighbors. Now that we have determined the optimal value of the parameter \\(K\\), we can build the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predictions model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # summary(neighb.model) # accuracy on test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # error rate # 1 - accuracy Thus, the error rate of the model constructed using the \\(K\\)-nearest neighbors method with the optimal choice of \\(K_{optimal}\\) equal to 12, determined by cross-validation, is 0.15 on the training data and 0.15 on the test data. To compare different models, we can use both types of error rates, which we will store in a table for clarity. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 5.2.3.2 Linear Discriminant Analysis As the second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied to functional data, we must first discretize the data, which we will do using Functional Principal Component Analysis (FPCA). We will then perform the classification algorithm on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components together explain at least 90% of the variability in the data. First, let’s perform the functional principal component analysis and determine the number \\(p\\). Code # principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of PCs nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determine p if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p PCs data.PCA.train$Y &lt;- factor(Y.train) # class membership In this particular case, we took the number of principal components as \\(p\\) = 3, which together explain 90.88 % of the variability in the data. The first principal component explains 45.83 % and the second 36.91 % of the variability. We can graphically display the scores of the first two principal components, color-coded according to class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() Figure 5.24: Scores of the first two principal components for the training data. Points are color-coded according to class membership. To determine the classification accuracy on the test data, we need to calculate the scores for the first 3 principal components for the test data. These scores are determined using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text{ dt}, \\] where \\(\\mu(t)\\) is the mean function and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # compute scores for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of residuals and eigenfunctions (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training portion of the data. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training (13.57 %) and on the test data (18.33 %). To visually represent the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function. Code # add decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # if p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # if p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # if p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 5.25: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line in the plane of the first two principal components) between the classes constructed using LDA is marked in black. We see that the decision boundary is a line, a linear function in the 2D space, which is indeed what we expected from LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.3 Quadratic Discriminant Analysis Next, we will construct a classifier using Quadratic Discriminant Analysis (QDA). This is an analogous case to LDA, with the difference that we now allow for different covariance matrices for each of the classes from which the corresponding scores are drawn. This relaxed assumption of equal covariance matrices leads to a quadratic boundary between the classes. In R, we perform QDA similarly to how we did LDA in the previous section. We will compute the scores for the training and test functions using the results from the functional Principal Component Analysis (PCA) obtained earlier. Thus, we can proceed directly to constructing the classifier using the qda() function. We will then calculate the accuracy of the classifier on both test and training data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training (14.29 %) and test data (15 %). To visually represent the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, just like in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 5.26: Scores of the first two principal components, color-coded according to class membership. The decision boundary (parabola in the plane of the first two principal components) between the classes constructed using QDA is marked in black. Notice that the decision boundary between the classification classes is now a parabola. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.4 Logistic Regression We can perform logistic regression in two ways. First, we can use the functional analogue of classical logistic regression, and second, we can apply classical multivariate logistic regression on the scores of the first \\(p\\) principal components. 5.2.3.4.1 Functional Logistic Regression Analogous to the case with finite-dimensional input data, we consider the logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is a linear predictor taking values in the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function (in the case of logistic regression, this is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\)), and \\(\\pi(x)\\) is the conditional probability: \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is a parametric function. Our goal is to estimate this parametric function. For functional logistic regression, we will use the fregre.glm() function from the fda.usc package. First, we will create suitable objects for the classifier construction. Code # create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train$basis nbasis.x &lt;- 50 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) To estimate the parametric function \\(\\beta(t)\\), we need to express it in some basis representation, in our case, a B-spline basis. However, we need to determine a suitable number of basis functions. We could determine this based on the error rate on the training data, but this would lead to a preference for selecting a large number of bases, resulting in overfitting. Let us illustrate this with the following case. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 50\\}\\), we will train the model on the training data, determine the error rate on the training data, and also calculate the error rate on the test data. We must remember that we cannot use the same data for estimating the test error rate, as this would underestimate the error rate. Code n.basis.max &lt;- 50 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # formula f &lt;- Y ~ x # basis for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) accuracy.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) accuracy.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix pred.baz[as.character(i), ] &lt;- 1 - c(accuracy.train, accuracy.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s visualize the trends of both training and test error rates in a graph based on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error Rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 47 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.27: Dependence of test and training error rates on the number of basis functions for \\(\\beta\\). The red point represents the optimal number \\(n_{optimal}\\) chosen as the minimum test error rate, the black line depicts the test error, and the blue dashed line illustrates the training error rate. We see that as the number of bases for \\(\\beta(t)\\) increases, the training error rate (represented by the blue line) tends to decrease, suggesting that we might choose large values for \\(n_{basis}\\) based solely on it. In contrast, the optimal choice based on the test error rate is \\(n\\) equal to 8, which is significantly smaller than 50. Conversely, as \\(n\\) increases, the test error rate rises, indicating overfitting of the model. For these reasons, we will use 10-fold cross-validation to determine the optimal number of basis functions for \\(\\beta(t)\\). The maximum number of basis functions considered is 35, as we observed that exceeding this value leads to overfitting. Code ### 10-fold cross-validation n.basis.max &lt;- 35 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # divide the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that do not change during the loop # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline basis # basis1 &lt;- X.train$basis # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to store results # columns will contain accuracy values for the respective training subset # rows will contain values for the respective number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now that we have everything prepared, we will calculate the error rates for each of the ten subsets of the training set. Subsequently, we will determine the average error and take the argument of the minimum validation error as the optimal \\(n\\). Code for (index in 1:k_cv) { # define the index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on the validation subset newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) accuracy.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- accuracy.valid } } # calculate average accuracies for each n across folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) # CV.results Let’s plot the validation error rates, highlighting the optimal value of \\(n_{optimal}\\), which is 15, with a validation error rate of 0.1026. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation Error Rate&#39;) + scale_x_continuous(breaks = n.basis) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 32 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.28: Dependence of validation error on the value of \\(n_{basis}\\), i.e., on the number of bases. We can now define the final model using functional logistic regression, choosing the B-spline basis for \\(\\beta(t)\\) with 15 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the training error rate (which is 4.29 %) and the test error rate (which is 8.33 %). For better visualization, we can also plot the estimated probabilities of belonging to the classification class \\(Y = 1\\) on the training data against the values of the linear predictor. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probability Pr(Y = 1|X = x)&#39;, colour = &#39;Class&#39;) Figure 5.29: Dependence of estimated probabilities on the values of the linear predictor. Points are color-coded according to their classification class. For informational purposes, we can also display the progression of the estimated parametric function \\(\\beta(t)\\). Code t.seq &lt;- seq(0, 6, length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_line() + theme_bw() + labs(x = &#39;Time&#39;, y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) Figure 5.30: Plot of the estimated parametric function \\(\\beta(t), t \\in [0, 6]\\). Finally, we will add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.4.2 Logistic Regression with Principal Component Analysis To construct this classifier, we need to perform functional principal component analysis, determine the appropriate number of components, and calculate the score values for the test data. We have already completed this in the linear discriminant analysis section, so we will use these results in the following section. We can directly construct the logistic regression model using the glm(, family = binomial) function. Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # accuracy on training data predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (13.57 %) and on the test data (18.33 %). For graphical representation of the method, we can plot the decision boundary in the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, just as we did in the LDA and QDA cases. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Group&#39;) + scale_colour_discrete(labels = c(&#39;Y = 0&#39;, &#39;Y = 1&#39;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 5.31: Scores of the first two principal components, color-coded according to classification class. The decision boundary (a line in the plane of the first two principal components) between classes is indicated in black, constructed using logistic regression. Note that the decision boundary between the classification classes is now a line, similar to the case with LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.5 Decision Trees In this section, we will look at a very different approach to constructing a classifier compared to methods such as LDA or logistic regression. Decision trees are a very popular tool for classification; however, like some of the previous methods, they are not directly designed for functional data. There are, however, procedures to convert functional objects into multidimensional ones, allowing us to apply decision tree algorithms. We can consider the following approaches: An algorithm built on basis coefficients, Utilizing principal component scores, Discretizing the interval and evaluating the function only on a finite grid of points. We will first focus on discretizing the interval and then compare the results with the other two approaches to constructing decision trees. 5.2.3.5.1 Interval Discretization First, we need to define points from the interval \\(I = [0, 6]\\), where we will evaluate the functions. Next, we will create an object where the rows represent the individual (discretized) functions and the columns represent time. Finally, we will add a column \\(Y\\) containing information about the classification class and repeat the same for the test data. Code # sequence of points at which we will evaluate the functions t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can construct a decision tree where all times from the vector t.seq will serve as predictors. This classification method is not susceptible to multicollinearity, so we do not need to worry about it. We will choose accuracy as the metric. Code # model construction clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the classifier on the test data is thus 15 %, and on the training data 8.57 %. We can visualize the decision tree graphically using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.32: Graphical representation of the unpruned decision tree. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.33: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - discr.&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.5.2 Principal Component Scores Another option for constructing a decision tree is to use principal component scores. Since we have already calculated the scores for the previous classification methods, we will utilize this knowledge and construct a decision tree based on the scores of the first 3 principal components. Code # model construction clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the test data is thus 28.33 %, and on the training data 16.43 %. We can visualize the decision tree constructed on the principal component scores using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.34: Graphical representation of the unpruned decision tree constructed on principal component scores. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # final model extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.35: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.5.3 Basis Coefficients The final option we will utilize for constructing a decision tree is to use coefficients in the representation of functions in the B-spline basis. First, let’s define the necessary datasets with the coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # test dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now we can construct the classifier. Code # model construction clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the training data is thus 10 %, and on the test data 11.67 %. We can visualize the decision tree constructed on the B-spline coefficient representation using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color differentiation. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.36: Graphical representation of the unpruned decision tree constructed on basis coefficients. Blue shades represent nodes belonging to classification class 1, and red shades represent class 0. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # final model extra = 104, # display required information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.37: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.6 Random Forests The classifier constructed using the random forests method consists of building several individual decision trees, which are then combined to create a common classifier (via “voting”). As with decision trees, we have several options regarding which data (finite-dimensional) we will use to construct the model. We will again consider the three approaches discussed above. The datasets with the corresponding variables for all three approaches have already been prepared from the previous section, so we can directly construct the models, calculate the characteristics of the classifiers, and add the results to the summary table. 5.2.3.6.1 Interval Discretization In the first case, we utilize the evaluation of functions on a given grid of points over the interval \\(I = [0, 6]\\). Code # model construction clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 0.71 %, and on the test data 13.33 %. Code Res &lt;- data.frame(model = &#39;RForest - discretization&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.6.2 Principal Component Scores In this case, we will use the scores of the first $p = $ 3 principal components. Code # model construction clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate on the training data is thus 2.86 %, and on the test data 20 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.6.3 Basis Coefficients Finally, we will use the representation of functions through the B-spline basis. Code # model construction clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 0.71 %, and on the test data 10 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7 Support Vector Machines Now let’s look at classifying our simulated curves using the Support Vector Machines (SVM) method. The advantage of this classification method is its computational efficiency, as it defines the boundary curve between classes using only a few (often very few) observations. In the case of functional data, we have several options for applying the SVM method. The simplest variant is to use this classification method directly on the discretized function (section 5.2.3.7.2). Another option is to utilize the principal component scores to classify curves based on their representation 5.2.3.7.3. A straightforward variant is to use the representation of curves through the B-spline basis and classify curves based on the coefficients of their representation in this basis (section 5.2.3.7.4). A more complex consideration can lead us to several additional options that leverage the functional nature of the data. We can utilize projections of functions onto a subspace generated, for example, by B-spline functions (section 5.2.3.7.5). The final method we will use for classifying functional data involves combining projection onto a certain subspace generated by functions (Reproducing Kernel Hilbert Space, RKHS) and classifying the corresponding representation. This method utilizes not only the classical SVM but also SVM for regression, as discussed in section RKHS + SVM 5.2.3.7.6. 5.2.3.7.1 SVM for Functional Data In the fda.usc library, we will use the function classif.svm() to apply the SVM method directly to functional data. First, we will create suitable objects for constructing the classifier. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = TRUE) # split into test and training sets X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Code # create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis nbasis.x &lt;- 50 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) Code # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # SVM model model.svm.f &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = 1e2) # accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.train &lt;- mean(factor(Y.train_norm) == predictions.train) # accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.test &lt;- mean(factor(Y.test_norm) == predictions.test) We calculated the training error (which is 4.29 %) and the test error (which is 8.33 %). Now let’s attempt, unlike the procedure in the previous chapters, to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, acknowledging that its optimal value may differ between kernels. For all three kernels, we will explore the values of the hyperparameter \\(C\\) in the range \\([10^{-2}, 10^{5}]\\), while for the polynomial kernel, we will consider the value of the hyperparameter \\(p\\) to be 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use \\(r k_cv\\)-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the range \\([10^{-5}, 10^{-2}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # We split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Which values of gamma do we want to consider gamma.cv &lt;- 10^seq(-5, -2, length = 4) C.cv &lt;- 10^seq(-2, 5, length = 8) p.cv &lt;- 3 coef0 &lt;- 1 # A list with three components... an array for each kernel -&gt; linear, poly, radial # An empty matrix where we will place individual results # The columns will contain the accuracy values for each # The rows will correspond to the values for a given gamma and the layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (cost in C.cv) { # We go through the individual folds for (index_cv in 1:k_cv) { # Definition of the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # Points at which the functions are evaluated tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEAR KERNEL # SVM model clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gam.cv in gamma.cv) { # Model construction clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- accuracy.test.r } } } Now we will average the results of 10-fold CV so that we have one estimate of validation error for one value of the hyperparameter (or one combination of values). At the same time, we will determine the optimal values of the individual hyperparameters. Code # We calculate the average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s take a look at how the optimal values turned out. For linear kernel, we have the optimal value \\(C\\) equal to 0.1, for polynomial kernel \\(C\\) is equal to 1, and for radial kernel, we have two optimal values, for \\(C\\) the optimal value is 10^{4} and for \\(\\gamma\\) it is 10^{-4}. The validation error rates are 0.1046566 for linear, 0.1266346 for polynomial, and 0.1117995 for radial kernel. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # Create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # Points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) Code model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # Accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # Accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) The error rate of the SVM method on the training data is thus 7.8571 % for the linear kernel, 7.1429 % for the polynomial kernel, and 5.7143 % for the Gaussian kernel. On the test data, the error rate of the method is 8.3333 % for the linear kernel, 10 % for the polynomial kernel, and 8.3333 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.2 Interval Discretization Let’s continue by applying the Support Vector Machines method directly to the discretized data (evaluation of the function on a grid of points over the interval \\(I = [0, 6]\\)), considering all three aforementioned kernel functions. Now, let’s attempt to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, and we allow its optimal value to differ among kernels. For all three kernels, we will go through the values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{4}]\\), while for the polynomial kernel we will fix the hyperparameter \\(p\\) at a value of 3, since other integer values do not yield nearly as good results. On the other hand, for the radial kernel, we will use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-5}, 10^{-1}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Values of gamma to consider gamma.cv &lt;- 10^seq(-5, -1, length = 5) C.cv &lt;- 10^seq(-2, 3, length = 6) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components ... arrays for individual kernels -&gt; linear, poly, radial # Empty matrices to store the results # Columns will contain accuracy values for given C # Rows will contain values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (C in C.cv) { # Go through individual folds for (index_cv in 1:k_cv) { # Definition of test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEAR KERNEL # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) accuracy.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) accuracy.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Model construction clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) accuracy.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- accuracy.test.r } } } Now, let’s average the results of the 10-fold CV so that for each value of the hyperparameter (or one combination of values), we have one estimate of the validation error. In this process, we will also determine the optimal values for the individual hyperparameters. Code # Calculate average accuracies for each C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For the linear kernel, the optimal value of \\(C\\) is 0.01, for the polynomial kernel \\(C\\) is 0.1, and for the radial kernel, we have two optimal values: for \\(C\\), the optimal value is 1000, and for \\(\\gamma\\), it is 10^{-4}. The validation errors are 0.1067399 for linear, 0.1373031 for polynomial, and 0.0991804 for radial kernels. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined by 10-fold CV. We will also determine the errors on both test and training datasets. Code # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) accuracy.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) accuracy.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) accuracy.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) accuracy.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) accuracy.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) accuracy.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is 9.2857 % for the linear kernel, 9.2857 % for the polynomial kernel, and 6.4286 % for the Gaussian kernel. On the test data, the error rate is 10 % for the linear kernel, 10 % for the polynomial kernel, and 10 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.3 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 3 principal components. Now, let’s try, unlike the approach in previous chapters, to estimate the classifier hyperparameters from the data using 10-fold cross-validation. Given that each kernel has different hyperparameters in its definition, we will treat each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, although we allow that its optimal value may differ between kernels. For all three kernels, we will test values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{3}]\\). For the polynomial kernel, we fix the hyperparameter \\(p\\) at the value of 3, as for other integer values, the method does not give nearly as good results. In contrast, for the radial kernel, we will use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-5}, 10^{-2}]\\). We set coef0 \\(= 1\\). Code set.seed(42) # gamma values to consider gamma.cv &lt;- 10^seq(-4, -1, length = 4) C.cv &lt;- 10^seq(-3, 3, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components ... array for individual kernels -&gt; linear, poly, radial # empty matrix to store results # columns will have accuracy values for a given # rows will have values for given gamma and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first, go through C values for (C in C.cv) { # iterate over each fold for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEAR KERNEL # build model clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # build model clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # build model clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we average the results of the 10-fold CV to obtain a single estimate of validation error for each hyperparameter value (or combination of values). We also determine the optimal values of each hyperparameter. Code # calculate average accuracies for individual C over folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For the linear kernel, the optimal value of \\(C\\) is 0.01, for the polynomial kernel \\(C\\) is 0.01, and for the radial kernel, there are two optimal values: \\(C\\) is 100 and \\(\\gamma\\) is 0.01. The validation errors are 0.128837 for linear, 0.1227198 for polynomial, and 0.1216941 for the radial kernel. Finally, we can construct the final classifiers on the entire training dataset with hyperparameter values determined using 10-fold CV. We also calculate errors on the test and training datasets. Code # build model clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The training accuracies are 0.8642857 for linear, 0.8714286 for polynomial, and 0.8642857 for radial kernels, and the test accuracies are 0.8166667 for linear, 0.8166667 for polynomial, and 0.8333333 for radial. To visualize the method, we can plot the decision boundary on a graph of the scores for the first two principal components. We compute this boundary on a dense grid of points and display it using the geom_contour() function, just as in previous cases where we also plotted the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;, linetype = &#39;Kernel&#39;) + scale_colour_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 5.38: Scores of the first two principal components, color-coded by classification group. The decision boundary (either a line or curves in the plane of the first two principal components) between classes is displayed in black, created using the SVM method. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.4 B-spline Coefficients Finally, we use a B-spline basis to express the functions. For all three kernels, we examine the values of hyperparameter \\(C\\) in the interval \\([10^{-1}, 10^{3}]\\). For the polynomial kernel, we fix the hyperparameter \\(p\\) at 3, as other integer values do not yield nearly as good results. On the other hand, for the radial kernel, we again use 10-fold CV to select the optimal value of hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-5}, 10^{-1}]\\). We set coef0 \\(= 1\\). Code set.seed(42) # gamma values to consider gamma.cv &lt;- 10^seq(-5, -1, length = 5) C.cv &lt;- 10^seq(-2, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components...array for each kernel -&gt; linear, poly, radial # empty matrix to store individual results # columns hold accuracy values for given # rows represent values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first iterate through values of C for (C in C.cv) { # iterate over individual folds for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEAR KERNEL # model creation clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # model creation clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # model creation clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we average the results from 10-fold CV so that we have a single estimate of validation error for each hyperparameter value (or combination of values). We also determine the optimal values of each hyperparameter. Code # calculate average accuracies for each C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s see how the optimal values turned out. For the linear kernel, the optimal \\(C\\) value is 0.1, for the polynomial kernel, \\(C\\) is 0.1, and for the radial kernel, we have two optimal values: \\(C\\) is 10, and \\(\\gamma\\) is 0.01. The validation error rates are 0.099359 for linear, 0.1208013 for polynomial, and 0.0792399 for radial kernels. Finally, we can construct the final classifiers on the entire training dataset using the hyperparameter values determined by 10-fold CV. We will also calculate the error rates on both the test and training data. Code # Model construction clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the basis coefficients on the training data is 5.71% for the linear kernel, 9.29% for the polynomial kernel, and 1.43% for the Gaussian kernel. On the test data, the error rate is 6.6667% for the linear kernel, 6.6667% for the polynomial kernel, and 8.3333% for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.5 Projection onto B-spline Basis Another option for using the classical SVM method for functional data is to project the original data onto some \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal{H}\\), denoted as \\(V_d\\). Assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), so we can write: \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Now we can use the coefficients from the orthogonal projection for classification, that is, we apply the standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By using this transformation, we have defined a new so-called adapted kernel, which consists of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector method. Thus, we have (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This is a dimensionality reduction method, which we can call filtering. For the projection itself, we will use the project.basis() function from the fda library in R. Its input will be a matrix of the original discrete (non-smoothed) data, the values at which we measure values in the original data matrix, and the basis object onto which we want to project the data. We will choose projection onto a B-spline basis since the use of a Fourier basis is not suitable for our non-periodic data. We choose the dimension \\(d\\) either from some prior expert knowledge or by using cross-validation. In our case, we will determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (we choose \\(k \\ll n\\) due to the computational intensity of the method, often \\(k = 5\\) or \\(k = 10\\)). We require B-splines of order 4, for which the relationship for the number of basis functions holds: \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). Therefore, the minimum dimension (for \\(n_{breaks} = 1\\)) is chosen as \\(n_{basis} = 3\\), and the maximum (for \\(n_{breaks} = 51\\), corresponding to the number of original discrete data points) is \\(n_{basis} = 53\\). However, in R, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and for large values of \\(n_{basis}\\), we already experience model overfitting; therefore, we choose a maximum \\(n_{basis}\\) of a smaller number, say 43. Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max # all dimensions we want to try # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components ... matrices for individual kernels -&gt; linear, poly, radial # An empty matrix where we will insert individual results # Columns will contain accuracy values for each part of the training set # Rows will contain values for each dimension value CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Projection of discrete data onto the B-spline basis of dimension d Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), # matrix of discrete data argvals = t.seq, # vector of arguments basisobj = bbasis) # basis object # Splitting into training and test data within CV XX.train &lt;- t(Projection) # subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Definition of test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Building the models clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) accuracy.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) accuracy.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) accuracy.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- accuracy.test.r } } # Compute average accuracies for individual d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 16 0.10007326 ## poly 11 0.11804945 ## radial 11 0.09126374 We see that the best value for the parameter \\(d\\) is 16 for the linear kernel, with an error rate calculated using 10-fold CV of 0.1001, 11 for the polynomial kernel with an error rate of 0.118, and 11 for the radial kernel with an error rate of 0.0913. To clarify, let’s plot the validation error rates as a function of the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation error rate&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 5.39: Dependency of validation error rate on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The optimal values of the dimension \\(V_d\\) for each kernel function are marked with black points. Now we can train the individual classifiers on all training data and examine their performance on the test data. For each kernel function, we choose the dimension of the subspace to project onto according to the results of cross-validation. The variable Projection stores the matrix of coefficients from the orthogonal projection, that is, \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Loop through each kernel for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Base object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Project discrete data onto B-spline basis Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), # Matrix of discrete data argvals = t.seq, # Vector of arguments basisobj = bbasis) # Basis object # Split into training and testing data XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Construct the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is therefore 5 % for the linear kernel, 2.86 % for the polynomial kernel, and 5 % for the Gaussian kernel. On the test data, the error rates are 8.33 % for the linear kernel, 11.67 % for the polynomial kernel, and 8.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.6 RKHS + SVM In this section, we will explore another way to utilize support vector machines (SVM) for classifying functional data. Here, we will again rely on the familiar principle of first expressing functional data as finite-dimensional objects and then applying the traditional SVM method to these objects. However, this time we will use the SVM method for the representation of functional data itself via a certain finite-dimensional object. As the name suggests, this involves a combination of two concepts: the support vector machine method and a space referred to in English literature as Reproducing Kernel Hilbert Space (RKHS). A key concept in this space is the kernel. 5.2.3.7.6.1 Implementation of the Method in R From the last part of Theorem 1.3, we can see how to compute the representations of curves in practice. We will work with discretized data after smoothing the curves. First, let’s define a kernel for the RKHS space. We will use the Gaussian kernel with a parameter \\(\\gamma\\). The value of this hyperparameter significantly affects the behaviour and success of the method, so we must pay special attention to its choice (we select it using cross-validation). 5.2.3.7.6.2 Gaussian Kernel Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s compute the matrix \\(K_S\\) along with its eigenvalues and corresponding eigenvectors. Code # Compute the matrix K gamma &lt;- 0.1 # Fixed value for gamma; optimal will be determined using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and vectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To compute the coefficients in the representation of the curves, that is, to calculate the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we also need the coefficients from SVM. Unlike the classification problem, we are now solving a regression problem, as we are trying to express our observed curves in some basis chosen by the kernel \\(K\\). Therefore, we will use the Support Vector Regression method, from which we will obtain the coefficients \\(\\alpha_{il}\\). Code # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } Now we can compute the representations of the individual curves. First, let’s choose \\(\\hat d\\) to be the entire dimension, that is, \\(\\hat d = m ={}\\) 101, and then determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # Determine the vector lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have stored the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) for each curve in the Lambda.RKHS matrix. We will use these vectors as representations of the given curves and classify the data based on this discretization. Code # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Construct the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 5.8: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0000 0.3833 SVM poly - RKHS 0.0000 0.2500 SVM rbf - RKHS 0.0143 0.2333 We observe that the model performs very well on the training data for all three kernels, while its success on the testing data is not good at all. It is evident that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values of \\(\\gamma\\) and \\(d\\). Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:40 # Reasonable range of values for d gamma.cv &lt;- 10^seq(-2, 3, length = 15) # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix where we will store individual results # Columns will represent accuracy values for given gamma, and rows will correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies for the respective d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.9: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validace}\\) Model linear 39 1.3895 0.0629 linear poly 40 1.3895 0.0876 polynomial radial 31 1.3895 0.0782 radial We see that the best parameter value is \\(d={}\\) 39 and \\(\\gamma={}\\) 1.3895 for the linear kernel with an error value calculated using 10-fold CV of 0.0629, \\(d={}\\) 40 and \\(\\gamma={}\\) 1.3895 for the polynomial kernel with an error value calculated using 10-fold CV of 0.0876 and \\(d={}\\) 31 and \\(\\gamma={}\\) 1.3895 for the radial kernel with an error value of 0.0782. For curiosity, let’s also plot the validation error function depending on the dimension \\(d\\) and the hyperparameter value \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 5.40: Dependency of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. Since we have already found the optimal values for the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { # Calculate the K matrix gamma &lt;- gamma.opt[kernel_number] # Gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine the alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # Replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Create empty object # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 5.10: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimated training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0.0000 0.1833 SVM poly - RKHS - radial 0.0000 0.1500 SVM rbf - RKHS - radial 0.0214 0.1000 The error rate of the SVM method combined with the projection on the Reproducing Kernel Hilbert Space is thus equal to 0 % for the linear kernel, 0 % for the polynomial kernel, and 2.14 % for the Gaussian kernel on the training data. On the testing data, the error rate of the method is 18.33 % for the linear kernel, 15 % for the polynomial kernel, and 10 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.6.3 Polynomial Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Include test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:40 # Reasonable range of d values poly.cv &lt;- 2:5 # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to insert individual results # Columns will hold accuracy values for given parameters # Rows will hold values for given p and layers corresponding to folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies in positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.11: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error and \\(\\widehat{Err}_{test}\\) the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 20 2 0.0967 linear poly 17 4 0.1119 polynomial radial 22 5 0.1248 radial We see that the best values for parameter \\(d={}\\) 20 and \\(p={}\\) 2 are for the linear kernel with an error calculated using 10-fold CV 0.0967, \\(d={}\\) 17 and \\(p={}\\) 4 for the polynomial kernel with an error calculated using 10-fold CV 0.1119, and \\(d={}\\) 22 and \\(p={}\\) 5 for the radial kernel with an error 0.1248. Since we have found the optimal values for the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # Iterate over the individual kernels for (kernel_number in 1:3) { # Calculate the matrix K p &lt;- poly.opt[kernel_number] # CV-derived parameter value K &lt;- Kernel.RKHS(t.seq, p = p) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model fitting for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 5.12: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0643 0.1167 SVM rbf - RKHS - poly 0.0929 0.1333 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus on the training data equal to 6.43 % for the linear kernel, 6.43 % for the polynomial kernel, and 9.29 % for the Gaussian kernel. On the test data, the error rate of the method is 23.33 % for the linear kernel, 11.67 % for the polynomial kernel, and 13.33 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.7.6.4 Linear Kernel Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Values of hyperparameters that we will traverse dimensions &lt;- 3:40 # Reasonable range of values for d # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store the individual results # In columns, there will be accuracy values for given d # In rows, there will be values for layers corresponding to folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Traverse dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculation of representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Traverse folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Traverse individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # Store accuracies in positions for given d, gamma, and fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 5.13: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 14 0.1509 linear poly 16 0.1642 polynomial radial 36 0.1510 radial We see that the optimal parameter value is \\(d={}\\) 14 for the linear kernel with an error rate calculated using 10-fold CV of 0.1509, \\(d={}\\) 16 for the polynomial kernel with an error rate of 0.1642, and \\(d={}\\) 36 for the radial kernel with an error rate of 0.151. Now that we have found the optimal hyperparameter values, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # Iterate over the individual kernels for (kernel_number in 1:3) { # Compute the K matrix K &lt;- Kernel.RKHS(t.seq) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store the results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 5.14: Summary of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.1214 0.2333 SVM poly - RKHS - linear 0.0571 0.2167 SVM rbf - RKHS - linear 0.0714 0.1500 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 12.14 % for the linear kernel, 5.71 % for the polynomial kernel, and 7.14 % for the Gaussian kernel. For the test data, the error rate is 23.33 % for the linear kernel, 21.67 % for the polynomial kernel, and 15 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 5.2.3.8 Results Table Table 5.15: Summary of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1500 0.1500 LDA 0.1357 0.1833 QDA 0.1429 0.1500 LR functional 0.0429 0.0833 LR score 0.1357 0.1833 Tree - discr. 0.0857 0.1500 Tree - score 0.1643 0.2833 Tree - Bbasis 0.1000 0.1167 RForest - discretization 0.0071 0.1333 RForest - score 0.0286 0.2000 RForest - Bbasis 0.0071 0.1000 SVM linear - func 0.0786 0.0833 SVM poly - func 0.0714 0.1000 SVM rbf - func 0.0571 0.0833 SVM linear - discr 0.0929 0.1000 SVM poly - discr 0.0929 0.1000 SVM rbf - discr 0.0643 0.1000 SVM linear - PCA 0.1357 0.1833 SVM poly - PCA 0.1286 0.1833 SVM rbf - PCA 0.1357 0.1667 SVM linear - Bbasis 0.0571 0.0667 SVM poly - Bbasis 0.0929 0.0667 SVM rbf - Bbasis 0.0143 0.0833 SVM linear - projection 0.0500 0.0833 SVM poly - projection 0.0286 0.1167 SVM rbf - projection 0.0500 0.0833 SVM linear - RKHS - radial 0.0000 0.1833 SVM poly - RKHS - radial 0.0000 0.1500 SVM rbf - RKHS - radial 0.0214 0.1000 SVM linear - RKHS - poly 0.0643 0.2333 SVM poly - RKHS - poly 0.0643 0.1167 SVM rbf - RKHS - poly 0.0929 0.1333 SVM linear - RKHS - linear 0.1214 0.2333 SVM poly - RKHS - linear 0.0571 0.2167 SVM rbf - RKHS - linear 0.0714 0.1500 5.2.4 Simulation Study In the entire previous section, we dealt with only one randomly generated set of functions from two classification classes, which we subsequently divided randomly into test and training parts. Then we evaluated each classifier obtained by the considered methods based on the test and training error rates. Since the generated data (and their division into two parts) may vary significantly with each repetition, the error rates of the individual classification algorithms will also vary considerably. Therefore, drawing any conclusions about the methods and comparing them with each other based on a single generated dataset can be very misleading. For this reason, in this section, we will focus on repeating the entire previous procedure for different generated datasets. We will store the results in a table and, in the end, calculate the average model characteristics across the individual repetitions. To ensure our conclusions are sufficiently general, we will choose the number of repetitions \\(n_{sim} = 100\\). Code # Setting the pseudorandom number generator set.seed(42) # Number of simulations n.sim &lt;- 100 ## List to store error rates # Columns represent methods # Rows represent individual repetitions # The list has two items ... train and test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # Object to store optimal hyperparameter values, determined by CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Now we will repeat the entire previous part 100 times, and we will store the error rate values in the list SIMULACE. In the data table CV_RESULTS, we will store the optimal hyperparameter values—specifically for the \\(K\\)-nearest neighbors method and the SVM dimension \\(d\\) in the case of projection onto a B-spline basis. We will also save all hyperparameter values for the SVM + RKHS method. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # pocet vygenerovanych pozorovani pro kazdou tridu n &lt;- 100 # vektor casu ekvidistantni na intervalu [0, 6] t &lt;- seq(0, 6, length = 51) # pro Y = 0 X0 &lt;- generate_values(t, funkce_0, n, 1, 2) # pro Y = 1 X1 &lt;- generate_values(t, funkce_1, n, 1, 2) rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # spojeni pozorovani do jedne matice XX &lt;- cbind(X0, X1) lambda.vect &lt;- 10^seq(from = -4.5, to = -1.5, length.out = 25) # vektor lambd gcv &lt;- rep(NA, length = length(lambda.vect)) # prazdny vektor pro ulozebi GCV for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # vyhlazeni gcv[index] &lt;- mean(BSmooth$gcv) # prumer pres vsechny pozorovane krivky } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # najdeme hodnotu minima lambda.opt &lt;- lambda.vect[which.min(gcv)] curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd # vypocet derivace XXder &lt;- deriv.fd(XXfd, 2) fdobjSmootheval &lt;- eval.fd(fdobj = XXder, evalarg = t) # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) Y &lt;- rep(c(0, 1), each = n) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- 1:20 #c(1:(2 * ceiling(sqrt(length(y.train))))) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train$basis nbasis.x &lt;- 20 rangeval &lt;- range(tt) norder &lt;- 6 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 15 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] # B-spline baze # basis1 &lt;- X.train$basis basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = 50) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(0, 6, length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM # normovani dat norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm &lt;- XXder XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = T) # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } ### 7.0) SVM for functional data # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-5, -2, length = 5) C.cv &lt;- 10^seq(-2, 5, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (cost in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train_norm$basis nbasis.x &lt;- 20 basis1 &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, nbasis = 50) # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEARNI JADRO # model SVM clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gam.cv in gamma.cv) { # sestrojeni modelu clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # vytvorime vhodne objekty x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # presnost na trenovacich datech newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.1) Diskretizace intervalu # ktere hodnoty chceme uvazovat gamma.cv &lt;- 10^seq(-5, -1, length = 5) C.cv &lt;- 10^seq(-3, 4, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent gamma.cv &lt;- 10^seq(-4, -1, length = 5) C.cv &lt;- 10^seq(-3, 4, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty gamma.cv &lt;- 10^seq(-4, -1, length = 5) C.cv &lt;- 10^seq(-2, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 # length(t) + norder - 2 - 10 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = grid.data |&gt; select(!contains(&#39;Y&#39;)) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) # rozdeleni na trenovaci a testovaci data v ramci CV XX.train &lt;- t(Projection) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = rbind( grid.data |&gt; select(!contains(&#39;Y&#39;)), grid.data.test |&gt; select(!contains(&#39;Y&#39;))) |&gt; as.matrix() |&gt; t(), argvals = t.seq, basisobj = bbasis) XX.train &lt;- t(Projection)[1:sum(split), ] XX.test &lt;- t(Projection)[(sum(split) + 1):length(split), ] data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 3, length = 6) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1, coef0 = 1, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(5, 40, by = 5) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = 0.1) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/simulace_04_2der_cv.RData&#39;) Now we will calculate the average test and training error rates for each classification method. Code # Prepare the final table SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # Save the final values save(SIMULACE.df, file = &#39;RData/simulace_04_res_2der_cv.RData&#39;) 5.2.4.1 Results Table 5.16: Summary results of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) represents the estimated training error, \\(\\widehat{Err}_{test}\\) the test error, \\(\\widehat{SD}_{train}\\) the standard deviation estimate of training errors, and \\(\\widehat{SD}_{test}\\) the standard deviation estimate of test errors. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1722 0.1863 0.0521 0.0745 LDA 0.1544 0.1687 0.0639 0.0839 QDA 0.1487 0.1740 0.0638 0.0842 LR_functional 0.0506 0.0982 0.0304 0.0412 LR_score 0.1532 0.1697 0.0643 0.0825 Tree_discr 0.1040 0.1448 0.0532 0.0782 Tree_score 0.1603 0.2182 0.0497 0.0768 Tree_Bbasis 0.1021 0.1478 0.0396 0.0747 RF_discr 0.0109 0.1360 0.0074 0.0660 RF_score 0.0318 0.1873 0.0203 0.0902 RF_Bbasis 0.0094 0.1323 0.0065 0.0679 SVM linear - func 0.0652 0.1097 0.0282 0.0447 SVM poly - func 0.0546 0.1548 0.0445 0.0679 SVM rbf - func 0.0607 0.1092 0.0297 0.0391 SVM linear - diskr 0.0622 0.1108 0.0289 0.0407 SVM poly - diskr 0.0589 0.1538 0.0453 0.0668 SVM rbf - diskr 0.0584 0.1155 0.0344 0.0437 SVM linear - PCA 0.1522 0.1712 0.0626 0.0819 SVM poly - PCA 0.1398 0.1783 0.0647 0.0830 SVM rbf - PCA 0.1440 0.1740 0.0636 0.0832 SVM linear - Bbasis 0.0518 0.0925 0.0264 0.0397 SVM poly - Bbasis 0.0513 0.1373 0.0388 0.0667 SVM rbf - Bbasis 0.0647 0.1080 0.0367 0.0539 SVM linear - projection 0.0767 0.1107 0.0303 0.0415 SVM poly - projection 0.0529 0.1418 0.0304 0.0586 SVM rbf - projection 0.0754 0.1420 0.0433 0.0596 SVM linear - RKHS - radial 0.0684 0.1180 0.0288 0.0463 SVM poly - RKHS - radial 0.0474 0.1483 0.0328 0.0538 SVM rbf - RKHS - radial 0.0610 0.1420 0.0294 0.0572 SVM linear - RKHS - poly 0.0746 0.1620 0.0389 0.0600 SVM poly - RKHS - poly 0.0457 0.1640 0.0297 0.0599 SVM rbf - RKHS - poly 0.0864 0.1573 0.0336 0.0633 SVM linear - RKHS - linear 0.1181 0.2080 0.0505 0.0714 SVM poly - RKHS - linear 0.0742 0.2150 0.0442 0.0744 SVM rbf - RKHS - linear 0.1126 0.2012 0.0439 0.0696 The table above presents all the calculated metrics, including the standard deviations, to allow comparison of the stability or variability of each method. Finally, we can visually display the calculated values from the simulation for each classification method using box plots, separately for training and testing errors. Code # for training data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Figure 5.41: Box plots of training errors for 100 simulations for each classification method. Black symbols \\(+\\) indicate means. Code # Set different names for classification methods methods_names &lt;- c( &#39;$K$ nearest neighbors&#39;, &#39;Linear discriminant analysis&#39;, &#39;Quadratic discriminant analysis&#39;, &#39;Functional logistic regression&#39;, &#39;Logistic regression with fPCA&#39;, &#39;Decision tree -- discretization&#39;, &#39;Decision tree -- fPCA&#39;, &#39;Decision tree -- basis coefficients&#39;, &#39;Random forest -- discretization&#39;, &#39;Random forest -- fPCA&#39;, &#39;Random forest -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # Alpha for box plots box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # Set different names for classification methods methods_names1 &lt;- c( &#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;functional LR&#39;, &#39;LR with fPCA&#39;, #&#39;Decision Tree -- Discretization&#39;, #&#39;Decision Tree -- fPCA&#39;, #&#39;Decision Tree -- Basis Coefficients&#39;, &#39;RF -- discretization&#39;, &#39;RF -- fPCA&#39;, &#39;RF -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;#, #&#39;RKHS (Linear SVR) $+$ SVM (linear)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (poly)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col1 &lt;- c(&#39;#4dd2ff&#39;, &#39;#00ace6&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#00bfff&#39;, rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 3)) # Alpha for box plots box_alpha1 &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, rep(c(0.9, 0.8, 0.7), 7)) #- 0.3 Code # for testing data sub_methods &lt;- methods[c(1:5, 9:32)] SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; filter(method %in% sub_methods) |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_jitter(position = position_jitter(height = 0, width = 0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 45, hjust = 1), panel.grid.minor = element_blank()) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = 4, size = 2, color = &quot;black&quot;, alpha = 1) + scale_x_discrete(labels = methods_names1) + theme(plot.margin = unit(c(0.1, 0.1, 0.7, 0.5), &quot;cm&quot;)) + scale_fill_manual(values = box_col1) + scale_alpha_manual(values = box_alpha1) + coord_cartesian(ylim = c(0, 0.4)) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray20&#39;, alpha = 0.8) Figure 5.42: Box plots of testing errors for 100 simulations for each classification method. Black symbols \\(+\\) indicate means. Code # ggsave(&quot;figures/results_der2.tex&quot;, device = tikz, width = 7, height = 5) # ggsave(&quot;figures/kap6_sim_04_boxplot_test_2der_subset.tex&quot;, device = tikz, width = 6.5, height = 5) We would now like to formally test whether some classification methods are better than others based on the previous simulation on these data or if we can consider them equally successful. Since the assumption of normality is not met, we cannot use the classical paired t-test. Instead, we will use its non-parametric alternative—the paired Wilcoxon test. However, we must be cautious with interpretation. Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;two.sided&#39;, paired = T)$p.value ## [1] 0.1256738 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - projection&#39;], alternative = &#39;l&#39;, paired = T)$p.value ## [1] 0.003970501 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - projection&#39;], SIMULACE$test[, &#39;SVM linear - Bbasis&#39;], alternative = &#39;g&#39;, paired = T)$p.value ## [1] 1.367196e-05 We test at an adjusted significance level \\(\\alpha_{adj} = 0.05 / 3 = 0.0167\\). Finally, let’s take a look at which hyperparameter values were the most common choices. Table 5.17: Medians of hyperparameter values for selected methods where a hyperparameter was determined using cross-validation. Median Hyperparameter Value KNN_K 9.0 nharm 3.0 LR_func_n_basis 9.5 SVM_d_Linear 12.0 SVM_d_Poly 11.0 SVM_d_Radial 11.0 SVM_RKHS_radial_gamma1 100.0 SVM_RKHS_radial_gamma2 10.0 SVM_RKHS_radial_gamma3 10.0 SVM_RKHS_radial_d1 20.0 SVM_RKHS_radial_d2 25.0 SVM_RKHS_radial_d3 15.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 25.0 SVM_RKHS_poly_d2 27.5 SVM_RKHS_poly_d3 25.0 SVM_RKHS_linear_d1 20.0 SVM_RKHS_linear_d2 25.0 SVM_RKHS_linear_d3 25.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.43: Histograms of hyperparameter values for KNN, functional logistic regression, and also a histogram for the number of principal components. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.44: Histograms of hyperparameter values for the SVM method with projection onto B-spline basis. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.45: Histograms of hyperparameter values for the RKHS + SVM with radial kernel method. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.46: Histograms of hyperparameter values for the RKHS + SVM with polynomial kernel method. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.47: Histograms of hyperparameter values for the RKHS + SVM with linear kernel method. Let’s finally compare the best classifiers from each derivative, namely the linear SVM applied to the coefficients of orthogonal projection onto the B-spline function system for the non-derivative data (error rate of \\(9.55\\,\\%\\)) and applied to the basis coefficients for the first (error rate of \\(9.58\\,\\%\\)) and second (error rate of \\(9.32\\,\\%\\)) derivatives. Since in all three simulation studies we set the pseudorandom number generator to the same value, the generated discrete vectors \\(\\boldsymbol y_i\\) and their distribution into sets \\(\\mathcal T_1\\) and \\(\\mathcal T_2\\) are identical for the given repetition out of the total \\(N=100\\). Therefore, to compare the medians of the test error rates of the most successful methods, we will again use the Wilcoxon paired test. Code # First, we load data from all simulations load(&#39;RData/simulace_03_cv.RData&#39;, verbose = F) data_0der &lt;- SIMULACE load(&#39;RData/simulace_04_cv.RData&#39;, verbose = F) data_1der &lt;- SIMULACE load(&#39;RData/simulace_04_2der_cv.RData&#39;, verbose = F) data_2der &lt;- SIMULACE Let’s check that we have loaded the correct data. Code mean(data_0der$test$`SVM linear - projection`) ## [1] 0.09316667 Code mean(data_1der$test$`SVM linear - Bbasis`) ## [1] 0.1 Code mean(data_2der$test$`SVM linear - Bbasis`) ## [1] 0.0925 Finally, we will conduct formal tests. Code wilcox.test(data_0der$test$`SVM linear - projection`, data_1der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_0der$test$`SVM linear - projection` and data_1der$test$`SVM linear - Bbasis` ## V = 1939, p-value = 0.2058 ## alternative hypothesis: true location shift is not equal to 0 Code wilcox.test(data_0der$test$`SVM linear - projection`, data_2der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_0der$test$`SVM linear - projection` and data_2der$test$`SVM linear - Bbasis` ## V = 1866.5, p-value = 0.988 ## alternative hypothesis: true location shift is not equal to 0 Code wilcox.test(data_1der$test$`SVM linear - Bbasis`, data_2der$test$`SVM linear - Bbasis`, alternative = &#39;t&#39;, paired = T)#$p.value ## ## Wilcoxon signed rank test with continuity correction ## ## data: data_1der$test$`SVM linear - Bbasis` and data_2der$test$`SVM linear - Bbasis` ## V = 2424, p-value = 0.1902 ## alternative hypothesis: true location shift is not equal to 0 We see that all three \\(p\\)-values are significantly above the significance level of 0.05 (and even after adjustment), so we can conclude that the classification power of these methods is comparable. "],["aplikace3.html", "Chapter 6 TECATOR dataset 6.1 Smoothing of Observed Curves 6.2 Computation of Derivatives 6.3 Classification of Original Curves 6.4 Classification Using the Second Derivative 6.5 Simulation Study", " Chapter 6 TECATOR dataset In this chapter, we will focus on the application of previously described methods (for more details, see, for example, section 1) to real data tecator, which are available, for example, in the ddalpha package. A detailed description of the data can be found here. This dataset contains spectrometric curves (absorbance curves measured at 100 wavelengths). For each piece of finely chopped meat, we observe one spectrometric curve, which corresponds to the absorbance measured at 100 wavelengths. The pieces are divided, according to Ferraty and Vieu (2006), into two classes: with low (\\(&lt; 20\\%\\)) and high (\\(\\geq 20\\%\\)) fat content obtained through analytical chemical processing. Our goal will be to classify the spectrometric curves on the interval \\(I = [850 \\text{ nm}, 1050 \\text{ nm}]\\) based on fat content. As we will see from the results in section 6.4, it is advantageous to consider the second derivative of the curves. Let’s start by loading and plotting the data. The data are stored in a somewhat complex way, so to make working with them easier, we will save them in a more practical format. We will also name the respective columns according to whether the fat content is low (small) or high (large). Code # loading data library(fda) library(ggplot2) library(dplyr) library(tidyr) library(ddalpha) data &lt;- ddalpha::dataf.tecator() data.gr &lt;- data$dataf[[1]]$vals for(i in 2:length(data$labels)) { data.gr &lt;- rbind(data.gr, data$dataf[[i]]$vals) } data.gr &lt;- cbind(data.frame(wave = data$dataf[[1]]$args), t(data.gr)) # vector of classes labels &lt;- data$labels |&gt; unlist() # renaming according to class colnames(data.gr) &lt;- c(&#39;wavelength&#39;, paste0(labels, 1:length(data$labels))) Let’s plot the spectrometric curves by group. Code abs.labs &lt;- c(&quot;Fat content &lt; 20 %&quot;, &quot;Fat content &gt; 20 %&quot;) names(abs.labs) &lt;- c(&#39;small&#39;, &#39;large&#39;) pivot_longer(data.gr, cols = large1:large215, names_to = &#39;sample&#39;, values_to = &#39;absorbance&#39;, cols_vary = &#39;slowest&#39;) |&gt; mutate(sample = as.factor(sample), Abs = factor(rep(labels, each = length(data.gr$wavelength)), levels = c(&#39;small&#39;, &#39;large&#39;))) |&gt; ggplot(aes(x = wavelength, y = absorbance, colour = Abs, group = sample)) + geom_line(linewidth = 0.5) + theme_bw() + facet_wrap(~Abs, labeller = labeller(Abs = abs.labs)) + labs(x = &quot;Wavelength [in nm]&quot;, y = &quot;Absorbance&quot;, colour = &quot;Fat content&quot;) + theme(legend.position = &#39;none&#39;) + scale_color_discrete(labels = abs.labs) Figure 6.1: Absorbance curves by group. 6.1 Smoothing of Observed Curves We will now transform the observed discrete values (value vectors) into functional objects, with which we will subsequently work. Since these are non-periodic curves on the interval \\(I = [850, 1050]\\), we will use a B-spline basis for smoothing. We take the entire wavelength vector as knots, and we would normally consider cubic splines, but since we want to work with the second derivative, we choose norder = 6. For the same reason, we will penalize the fourth derivative of the functions. Code t &lt;- data.gr$wavelength rangeval &lt;- range(t) breaks &lt;- t norder &lt;- 6 bbasis &lt;- create.bspline.basis(rangeval = rangeval, norder = norder, breaks = breaks) curv.Lfd &lt;- int2Lfd(4) # penalize 4th derivative We will find a suitable value of the smoothing parameter \\(\\lambda &gt; 0\\) using \\(GCV(\\lambda)\\), i.e., generalized cross-validation. The value of \\(\\lambda\\) will be considered the same for both classes, as for test observations we would not know in advance which value of \\(\\lambda\\) to choose if different values were selected for each class. Code # combine observations into one matrix XX &lt;- data.gr[, -1] |&gt; as.matrix() lambda.vect &lt;- 10^seq(from = -1, to = 0.5, length.out = 50) # lambda vector gcv &lt;- rep(NA, length = length(lambda.vect)) # empty vector for GCV storage for(index in 1:length(lambda.vect)) { curv.Fdpar &lt;- fdPar(bbasis, curv.Lfd, lambda.vect[index]) BSmooth &lt;- smooth.basis(t, XX, curv.Fdpar) # smoothing gcv[index] &lt;- mean(BSmooth$gcv) # average across all observed curves } GCV &lt;- data.frame( lambda = round(log10(lambda.vect), 3), GCV = gcv ) # find the minimum value lambda.opt &lt;- lambda.vect[which.min(gcv)] To illustrate better, we will plot the \\(GCV(\\lambda)\\) progression. Code GCV |&gt; ggplot(aes(x = lambda, y = GCV)) + geom_line(linetype = &#39;solid&#39;, linewidth = 0.6) + geom_point(size = 1.7) + theme_bw() + labs(x = bquote(paste(log[10](lambda), &#39; ; &#39;, lambda[optimal] == .(round(lambda.opt, 4)))), y = expression(GCV(lambda))) + geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &#39;red&#39;, size = 3) ## Warning in geom_point(aes(x = log10(lambda.opt), y = min(gcv)), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 50 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 6.2: Progression of \\(GCV(\\lambda)\\) for the chosen \\(\\boldsymbol\\lambda\\) vector. On the \\(x\\)-axis, values are shown on a logarithmic scale with base 10. The optimal value of the smoothing parameter \\(\\lambda_{optimal}\\) is marked in red. With this optimal choice of the smoothing parameter \\(\\lambda\\), we will now smooth all functions. Code curv.fdPar &lt;- fdPar(bbasis, curv.Lfd, lambda.opt) BSmooth &lt;- smooth.basis(t, XX, curv.fdPar) XXfd &lt;- BSmooth$fd fdobjSmootheval &lt;- eval.fd(fdobj = XXfd, evalarg = t) Finally, we will display all curves including the average separately for each class. Code library(tikzDevice) n &lt;- dim(XX)[2] DFsmooth &lt;- data.frame( t = rep(t, n), time = factor(rep(1:n, each = length(t))), Smooth = c(fdobjSmootheval), Fat = factor(rep(labels, each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(t, 2), Mean = c(eval.fd(fdobj = mean.fd(XXfd[labels == &#39;small&#39;]), evalarg = t), eval.fd(fdobj = mean.fd(XXfd[labels == &#39;large&#39;]), evalarg = t)), # c(apply(fdobjSmootheval[ , labels == &#39;small&#39;], 1, mean), # apply(fdobjSmootheval[ , labels == &#39;large&#39;], 1, mean)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(t)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat, labeller = labeller(Fat = abs.labs) ) + labs(x = &quot;Wavelength&quot;, y = &quot;Absorbance&quot;, colour = &quot;Fat content&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Figure 6.3: Plot of all smoothed observed curves, with colors distinguishing curves by class. The average for each class is shown in black. Code # ggsave(&quot;figures/kap7_tecator_curves_mean.tex&quot;, device = tikz, width = 9, height = 4.5) We observe that the curves for both groups (by fat content) are relatively similar, with the average indicated in black. The curves differ mainly in the middle of the interval, where fattier samples show one additional local extremum, while less fatty samples appear simpler with only one extremum. 6.2 Computation of Derivatives As mentioned above, it will be advantageous to classify the curves based on the second derivative. To compute the derivative for a functional object, we will use the deriv.fd() function from the fda package in R. Since we want to classify based on the second derivative, we set the argument Lfdobj = 2. The use of this data will be shown in Section 6.4. Code XXder &lt;- deriv.fd(XXfd, 2) ttt &lt;- seq(min(t), max(t), length = 501) fdobjSmootheval_der2 &lt;- eval.fd(fdobj = XXder, evalarg = ttt) Let’s display all curves, including the average, separately for each class. Code DFsmooth &lt;- data.frame( t = rep(ttt, n), time = factor(rep(1:n, each = length(ttt))), Smooth = c(fdobjSmootheval_der2), Fat = factor(rep(labels, each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFmean &lt;- data.frame( t = rep(ttt, 2), Mean = c(eval.fd(fdobj = mean.fd(XXder[labels == &#39;small&#39;]), evalarg = ttt), eval.fd(fdobj = mean.fd(XXder[labels == &#39;large&#39;]), evalarg = ttt)), Fat = factor(rep(c(&#39;small&#39;, &#39;large&#39;), each = length(ttt)), levels = c(&#39;small&#39;, &#39;large&#39;)) ) DFsmooth |&gt; ggplot(aes(x = t, y = Smooth, color = Fat)) + geom_line(linewidth = 0.05, aes(group = time), alpha = 0.5) + theme_bw() + facet_wrap(~Fat) + labs(x = &quot;Wavelength&quot;, y = &quot;Absorbance&quot;, colour = &quot;Fat content&quot;) + theme(legend.position = &#39;none&#39;) + geom_line(data = DFmean, aes(x = t, y = Mean), colour = &#39;grey2&#39;, linewidth = 1.25, linetype = &#39;solid&#39;) + scale_colour_manual(values = c(&#39;tomato&#39;, &#39;deepskyblue2&#39;)) Figure 6.4: Plot of all smoothed observed curves, with colors distinguishing curves by classification class. The average for each class is shown in black. Code # ggsave(&quot;figures/kap7_tecator_curves_derivatives.tex&quot;, device = tikz, width = 9, height = 4.5) From the figure above, we see that the average curves between the two groups of samples now differ much more significantly than in the case of the original non-derivative curves. 6.3 Classification of Original Curves In the first part of this chapter, we will focus on the classification of the original non-derivative curves. Classification based on the second derivative of the original curves will be shown later in Section 6.4. First, we will load the necessary libraries for classification. Code library(caTools) # for splitting into test and training library(caret) # for k-fold CV library(fda.usc) # for KNN, fLR library(MASS) # for LDA library(fdapace) library(pracma) library(refund) # for LR on scores library(nnet) # for LR on scores library(caret) library(rpart) # trees library(rattle) # graphics library(e1071) library(randomForest) # random forest set.seed(42) We will split the data in a 30:70 ratio into test and training sets to evaluate the classification accuracy of individual methods. The training set will be used to construct the classifier, while the test set will be used to calculate the classification error and potentially other characteristics of our model. The resulting classifiers can then be compared with each other in terms of their classification accuracy based on these calculated characteristics. Code # splitting into test and training set set.seed(42) split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # creating a 0 and 1 vector, 0 for &lt; 20 and 1 for &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) Let’s check the distribution of individual groups in the test and training parts of the data. Code # absolute distribution table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relative distribution table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 6.3.1 \\(K\\) Nearest Neighbors We start with a non-parametric classification method, specifically the \\(K\\) nearest neighbors method. First, we create the necessary objects to work with the classif.knn() function from the fda.usc package. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now, we can define the model and examine its classification accuracy. The remaining question is how to choose the optimal number of neighbors \\(K\\). We could choose this \\(K\\) value as the one that minimizes the error on the training data. However, this might lead to overfitting, so we will use cross-validation. Given the computational complexity and size of the dataset, we choose \\(k\\)-fold CV, with \\(k = 10\\) for example. Code # model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) 1 - neighb.model$max.prob # minimum error ## [1] 0.1466667 Code (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 1 Let’s apply this process to the training data, which we will split into \\(k\\) parts and repeat this code section \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # split training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # empty matrix to store individual results # columns will contain accuracy values for each training subset part # rows will contain values for each K neighbors CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # define the index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # repeat for each part ... k times for(neighbour in neighbours) { # model for a specific K choice neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # prediction on validation part model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # accuracy on validation part accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracy at position for given K and fold CV.results[neighbour, index] &lt;- accuracy } } # calculate average accuracy for each K across folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) optimal.cv.accuracy &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) We see that the optimal value of the parameter \\(K\\) is 1, with an error rate calculated using 10-fold CV of 0.1478. For clarity, let’s plot the course of validation error with respect to the number of neighbors \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - optimal.cv.accuracy), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation Error&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - optimal.cv.accuracy), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 26 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.7: Dependence of validation error on the value of \\(K\\), i.e., on the number of neighbors. Now that we know the optimal value of parameter \\(K\\), we can build the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # prediction model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # accuracy on test data accuracy &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we see that the error rate of the model built using the \\(K\\) nearest neighbors method with the optimal choice of \\(K_{optimal}\\) equal to 1, determined by cross-validation, is 0.1467 on the training data and 0.1692 on the test data. To compare different models, we can use both types of error rates, and for clarity, we will store them in a table. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - accuracy) 6.3.2 Linear Discriminant Analysis As a second method for constructing a classifier, we will consider linear discriminant analysis (LDA). Since this method cannot be applied to functional data directly, we first need to discretize the data, which we will do using functional principal component analysis. The classification algorithm will then be performed on the scores of the first \\(p\\) principal components. The number of components \\(p\\) is chosen so that the first \\(p\\) principal components together explain at least 90% of the variability in the data. Let’s start by performing functional principal component analysis and determining the number \\(p\\). Code # principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of PCs nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determining p if(nharm == 1) nharm &lt;- 2 # to allow plotting graphs, # we need at least 2 PCs data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of first p PCs data.PCA.train$Y &lt;- factor(Y.train) # class membership In this specific case, we selected \\(p=\\) 2 as the number of principal components, which together explain 99.57 \\(\\%\\) of the variability in the data. The first principal component explains 98.47 % and the second 1.09 \\(\\%\\) of the variability. Graphically, we can display the scores of the first two principal components, with points colored according to class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() Figure 6.5: Scores of the first two principal components for the training data, with points colored by class membership. To determine the classification accuracy on the test data, we need to calculate the scores for the first 2 principal components for the test data. These scores are calculated using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t) \\, \\text{dt}, \\] where \\(\\mu(t)\\) is the mean (average function) and \\(\\rho_j(t)\\) are the eigenfunctions (functional principal components). Code # score calculation for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of residual and eigenfunctions rho (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training data. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (32 %) and on the test data (29.23 %). For a graphical representation of the method, we can add the decision boundary to the plot of the scores of the first two principal components. This boundary is calculated on a dense grid of points and displayed using the geom_contour() function. Code # add discriminant boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # if p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # if p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # if p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.8: Scores of the first two principal components, colored by classification class. The black line represents the decision boundary (a line in the plane of the first two principal components) between classes constructed using LDA. We observe that the decision boundary is a line, a linear function in the 2D space, as expected with LDA. Finally, we add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.3 Quadratic Discriminant Analysis Next, let’s construct a classifier using Quadratic Discriminant Analysis (QDA). This approach is analogous to LDA, with the difference that we now allow for each class to have a distinct covariance matrix for the normal distribution from which the respective scores originate. This relaxed assumption of equal covariance matrices results in a quadratic boundary between classes. In R, QDA is performed similarly to LDA as shown in the previous section; that is, we would calculate scores for both training and test functions using functional principal component analysis, construct a classifier on the scores of the first \\(p\\) principal components, and use it to predict the class membership of test curves in \\(Y^* \\in \\{0, 1\\}\\). We don’t need to perform functional PCA again, as we can use the results from the LDA section. We can now proceed directly to constructing the classifier using the qda() function. We then calculate the classifier’s accuracy on the training and test data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error rate of the classifier on the training data (32 %) and on the test data (30.77 %). For a graphical representation of the method, we can add the decision boundary to the plot of the scores of the first two principal components. This boundary is calculated on a dense grid of points and displayed using the geom_contour() function, as we did for LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 5.3: Scores of the first two principal components, colored by classification class. The black line represents the decision boundary (a parabola in the plane of the first two principal components) between classes constructed using QDA. Notice that the decision boundary between the classification classes is now a parabola. Finally, we add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.4 Logistic Regression Logistic regression can be performed in two ways: either by using the functional equivalent of traditional logistic regression or by applying classical multivariate logistic regression on the scores of the first \\(p\\) principal components. 6.3.4.1 Functional Logistic Regression Analogously to the finite-dimensional case, we consider a logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is the linear predictor with values in the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function (in the case of logistic regression, it is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\)), and \\(\\pi(x)\\) is the conditional probability \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant, and \\(\\beta(t) \\in L^2[a, b]\\) is the parameter function. Our objective is to estimate this parameter function. For functional logistic regression, we use the fregre.glm() function from the fda.usc package. First, we create appropriate objects for constructing the classifier. Code # Create appropriate objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # Points where the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # Choose a basis for the functional observations, typically the same # basis used for smoothing curves. However, this choice leads to numerical error, # so we choose a basis with fewer functions. # After trying several options, 7 functions appear to be sufficient. nbasis.x &lt;- 100 # B-spline basis basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = nbasis.x) To estimate the parameter function \\(\\beta(t)\\), we need to express it in a basis representation—in our case, a B-spline basis. However, we need to determine the appropriate number of basis functions. This could be decided based on the training error, but the training data will favor a larger number of bases, leading to model overfitting. To illustrate this, let’s train a model on the training data for each basis count \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\), calculate the training error, and also compute the test error. Note that to determine the optimal number of basis functions, we cannot use the same data as those for estimating the test error, as this would underestimate the error. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # Basis for beta basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # Relationship f &lt;- Y ~ x # Basis for x and beta basis.x &lt;- list(&quot;x&quot; = basis1) # Smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # Model input data ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # Binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # Accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert into matrix pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s plot the dependence of both types of errors on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 27 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.4: Dependence of test and training error on the number of basis functions for \\(\\beta\\). The red dot represents the optimal number \\(n_{optimal}\\) chosen as the minimum test error, with the test error shown by the black line and training error by the blue dashed line. We can see that with an increasing number of basis functions for \\(\\beta(t)\\), the training error (blue line) tends to decrease, suggesting we would choose large values of \\(n_{basis}\\). However, the optimal choice based on the test error is \\(n\\) equal to 6, which is significantly smaller than 30. As \\(n\\) increases, the test error rises, indicating model overfitting. To determine the optimal number of basis functions for \\(\\beta(t)\\), we will use 10-fold cross-validation. The maximum number of considered basis functions is set to 25, as we observed earlier that values above this lead to overfitting. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # divide training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that remain unchanged during the cycle # points where functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline basis # basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # relationship f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to store individual results # columns contain accuracy values for each part of the training set # rows contain values for each basis count CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now we are ready to compute the error for each of the ten subsets of the training set. Next, we determine the average and take the argument of the minimum validation error as the optimal \\(n\\). Code for (index in 1:k_cv) { # define the index subset fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # basis for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on the validation set newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # calculate average accuracy for each n across folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Let’s plot the course of validation error with the optimal value \\(n_{optimal}\\) marked, which is 19 with a validation error of 0.0604. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation error&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 22 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 1.11: Dependence of validation error on the value of \\(n_{basis}\\), i.e., the number of bases. We can now define the final model using functional logistic regression, choosing the B-spline basis for \\(\\beta(t)\\) with 19 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # basis for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # accuracy on the training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on the test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the training error (which is 0 %) and the test error (which is 9.23 %). To gain a better understanding, we can plot the estimated probabilities of belonging to the classification class \\(Y = 1\\) on the training data as a function of the linear predictor values. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;small&quot;, &quot;large&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probabilities Pr(Y = 1|X = x)&#39;, colour = &#39;Fat Content&#39;) Figure 6.6: Dependence of estimated probabilities on the values of the linear predictor. Points are colored based on their classification class. Additionally, we can display the course of the estimated parametric function \\(\\beta(t)\\) for reference. Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 4.1: Course of the estimated parametric function \\(\\beta(t), t \\in [850, 1050]\\). Code t.seq &lt;- seq(min(tt), max(tt), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) # data.frame(t = t.seq, beta = beta.seq) |&gt; # ggplot(aes(t, beta)) + # geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, # linewidth = 0.5, colour = &#39;grey&#39;) + # geom_line(colour = &#39;deepskyblue2&#39;, linewidth = 0.8) + # theme_bw() + # labs(x = expression(t), # y = expression(widehat(beta)(t))) + # theme(panel.grid.major = element_blank(), # panel.grid.minor = element_blank()) # # ggsave(&quot;figures/betahat_presentation.tex&quot;, width = 4, height = 2.5, # device = tikz) Finally, we will add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.4.2 Logistic Regression with Principal Component Analysis To construct this classifier, we need to perform functional principal component analysis, determine the appropriate number of components, and calculate the score values for the test data. We have already done this in the linear discriminant analysis section, so we will utilize these results in the following part. We can now directly build the logistic regression model using the glm(, family = binomial) function. Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # accuracy on training data predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we have calculated the error rate of the classifier on the training data (30.67 %) and on the test data (29.23 %). For graphical representation of the method, we can mark the decision boundary in the score plot of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, as we did in the case of LDA and QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_colour_discrete(labels = c(&quot;small&quot;, &quot;large&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.14: Scores of the first two principal components, colored according to class membership. The decision boundary (a line in the plane of the first two principal components) between classes, constructed using logistic regression, is marked in black. Notice that the decision boundary between the classification classes is now a line, just like in the case of LDA. Finally, we will add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5 Decision Trees In this section, we will explore a very different approach to constructing a classifier compared to methods like LDA or logistic regression. Decision trees are a popular tool for classification, but, similar to some previous methods, they are not directly designed for functional data. However, there are ways to transform functional objects into multidimensional data and then apply decision tree algorithms to them. We can consider the following approaches: An algorithm constructed on basis coefficients, Utilizing principal component scores, Discretizing the interval and evaluating the function only at a finite grid of points. We will first focus on interval discretization and then compare the results with the other two approaches to constructing a decision tree. 6.3.5.1 Interval Discretization First, we need to define the points from the interval \\(I = [850, 1050]\\), where we will evaluate the functions. Then we will create an object where the rows represent individual (discretized) functions and the columns represent time points. Finally, we will append a column \\(Y\\) containing information about class membership and repeat the same process for the test data. Code # sequence of points where we will evaluate the functions t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose to have functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can build a decision tree where all times from the t.seq vector will serve as predictors. This classification is not susceptible to multicollinearity, so we do not need to worry about it. We will choose accuracy as the metric. Code # building the model clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the classifier on the test data is thus 35.38 %, and on the training data, it is 29.33 %. We can visualize the decision tree using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color distinctions. This is an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.6: Graphical representation of the unpruned decision tree. Nodes corresponding to class 1 are depicted in shades of blue, and those for class 0 in shades of red. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # display desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 6.7: Final pruned decision tree. Finally, we will again add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - discretization&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.2 Principal Component Scores Another option for constructing a decision tree is to use principal component scores. Since we have already calculated scores for previous classification methods, we will leverage this knowledge to build a decision tree based on the scores of the first 2 principal components. Code # building the model clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the test data is thus 38.46 %, and on the training data, it is 32.67 %. We can visualize the decision tree constructed from the principal component scores using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color distinctions. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.7: Graphical representation of the unpruned decision tree constructed from principal component scores. Nodes corresponding to class 1 are depicted in shades of blue, and those for class 0 in shades of red. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # final model extra = 104, # display desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 6.8: Final pruned decision tree. Finally, we will add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.5.3 Basis Coefficients The last option we will use to construct a decision tree is to utilize coefficients in the representation of functions in B-spline basis. First, let’s define the necessary data files containing the coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testing dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now we can build the classifier. Code # building the model clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the decision tree on the training data is thus 22.67 %, and on the test data, it is 24.62 %. We can visualize the decision tree constructed from the B-spline coefficients using the fancyRpartPlot() function. We will set the colors of the nodes to reflect the previous color distinctions. This is an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.8: Graphical representation of the unpruned decision tree constructed from basis coefficients. Nodes corresponding to class 1 are depicted in shades of blue, and those for class 0 in shades of red. We can also plot the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # final model extra = 104, # display desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 6.9: Final pruned decision tree. Finally, we will add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6 Random Forests A classifier constructed using the random forests method involves building several individual decision trees, which are then combined to create a common classifier through “voting.” As with decision trees, we have several options regarding which data (finite-dimensional) we will use to construct the model. We will again consider the three approaches discussed earlier. The data files with the relevant variables for all three approaches have already been prepared in the previous section, so we can directly construct the models, calculate the characteristics of the classifier, and add the results to the summary table. 6.3.6.1 Interval Discretization In the first case, we utilize the evaluation of functions at a grid of points in the interval \\(I = [850, 1050]\\). Code # building the model clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 2 %, and on the test data, it is 12.31 %. Code Res &lt;- data.frame(model = &#39;RForest - discretization&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.2 Principal Component Scores In this case, we will use the scores of the first $p = $ 2 principal components. Code # building the model clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 4.67 %, and on the test data, it is 30.77 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.6.3 Basis Coefficients Finally, we will use the representation of functions through the B-spline basis. Code # building the model clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 1.33 %, and on the test data, it is 12.31 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.3.7 Support Vector Machines Now let’s take a look at curve classification using the Support Vector Machines (SVM) method. The advantage of this classification method is its computational efficiency, as it uses only a few (often very few) observations to define the decision boundary between classes. The main advantage of SVM is the use of the so-called kernel trick, which allows us to replace the ordinary scalar product with another scalar product of transformed data, without having to explicitly define this transformation. This results in a generally nonlinear separating boundary between classification classes. A kernel (kernel function) \\(K\\) is a function that satisfies \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] where \\(\\phi\\) is some (unknown) transformation (feature map), \\(\\mathcal H\\) is a Hilbert space, and \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) is some scalar product in this Hilbert space. In practice, three types of kernel functions are most commonly chosen: Linear kernel – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), Polynomial kernel – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), Radial (Gaussian) kernel – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). For all the above-mentioned kernels, we must choose a constant \\(C &gt; 0\\), which indicates the penalty for exceeding the decision boundary between classes (inverse regularization parameter). As the value of \\(C\\) increases, the method penalizes misclassified data more and the shape of the boundary less. Conversely, for small values of \\(C\\), the method does not give much importance to misclassified data but focuses more on penalizing the shape of the boundary. This constant \\(C\\) is typically set to 1 by default, but we can also determine it directly, for example, using cross-validation. By utilizing cross-validation, we can also determine the optimal values of other hyperparameters, which now depend on our choice of kernel function. In the case of a linear kernel, no other parameters are chosen besides the constant \\(C\\). For polynomial and radial kernels, we must determine the hyperparameters \\(\\alpha_0, \\gamma \\text{ and } d\\), whose default values in R are \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})} \\text{ and } d^{default} = 3\\). We typically choose \\(\\alpha_0^{default} = 1\\), as this value yields significantly better results. When dealing with functional data, we have several options for applying the SVM method. The simplest variant is to use this classification method directly on the discretized function (section 6.3.7.2). Another option is to use the scores of the principal components and classify curves based on their representation (section 6.3.7.3). A straightforward variant is to use the representation of curves through the B-spline basis and classify curves based on the coefficients of their representation in this basis (section 6.3.7.4). A more complex consideration leads us to several additional options that utilize the functional nature of the data. One option is to classify not the original curve but its derivative (or second, third derivatives, etc.). Another option is to utilize projections of functions onto a subspace generated, for example, by B-spline functions (section 6.3.7.5). The last method we will use for classifying functional data combines projection onto a certain subspace generated by functions (Reproducing Kernel Hilbert Space, RKHS) and classification of the corresponding representation. This method utilizes not only the classical SVM but also SVM for regression, which we elaborate on in section RKHS + SVM (section 6.3.7.6). 6.3.7.1 SVM for Functional Data In the fda.usc library, we will use the function classif.svm() to apply the SVM method directly to functional data. First, we will create suitable objects for constructing the classifier. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXfd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # splitting into test and training parts X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) Code # create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = nbasis.x) Code # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # SVM model model.svm.f &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, degree = 3, coef0 = 1, cost = 1e4) # accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.train &lt;- mean(factor(Y.train_norm) == predictions.train) # accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.test &lt;- mean(factor(Y.test_norm) == predictions.test) We calculated the training error (which is 0 %) and the test error (which is 9.23 %). Now let’s attempt, unlike the procedure in the previous chapters, to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, acknowledging that its optimal value may differ between kernels. For all three kernels, we will explore the values of the hyperparameter \\(C\\) in the range \\([10^{-2}, 10^{5}]\\), while for the polynomial kernel, we will consider the value of the hyperparameter \\(p\\) to be 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use \\(r k_cv\\)-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the range \\([10^{-5}, 10^{-2}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # We split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Which values of gamma do we want to consider gamma.cv &lt;- 10^seq(-5, -2, length = 4) C.cv &lt;- 10^seq(-2, 5, length = 8) p.cv &lt;- 3 coef0 &lt;- 1 # A list with three components... an array for each kernel -&gt; linear, poly, radial # An empty matrix where we will place individual results # The columns will contain the accuracy values for each # The rows will correspond to the values for a given gamma and the layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (cost in C.cv) { # We go through the individual folds for (index_cv in 1:k_cv) { # Definition of the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # Points at which the functions are evaluated tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEAR KERNEL # SVM model clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gam.cv in gamma.cv) { # Model construction clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- accuracy.test.r } } } Now we will average the results of 10-fold CV so that we have one estimate of validation error for one value of the hyperparameter (or one combination of values). At the same time, we will determine the optimal values of the individual hyperparameters. Code # We calculate the average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s take a look at how the optimal values turned out. For linear kernel, we have the optimal value \\(C\\) equal to 1, for polynomial kernel \\(C\\) is equal to 1, and for radial kernel, we have two optimal values, for \\(C\\) the optimal value is 10^{4} and for \\(\\gamma\\) it is 0. The validation error rates are 0.0066667 for linear, 0.02625 for polynomial, and 0.0066667 for radial kernel. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # Create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # Points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) Code model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # Accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # Accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) The error rate of the SVM method on the training data is thus 0.6667 % for the linear kernel, 1.3333 % for the polynomial kernel, and 1.3333 % for the Gaussian kernel. On the test data, the error rate of the method is 9.2308 % for the linear kernel, 6.1538 % for the polynomial kernel, and 4.6154 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.2 Discretization of the Interval Let’s start by applying the Support Vector Machines method directly to the discretized data (evaluating the function on a given grid of points over the interval \\(I = [850, 1050]\\)), considering all three aforementioned kernel functions. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXfd$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(BSmooth$fd[i]))) } XXfd_norm &lt;- XXfd XXfd_norm$coefs &lt;- XXfd_norm$coefs * matrix(norms, ncol = dim(XXfd$coefs)[2], nrow = dim(XXfd$coefs)[1], byrow = T) # split into test and training sets X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() Now let’s estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, and we allow for its optimal value to differ between kernels. For all three kernels, we will explore values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{3}]\\), fixing the hyperparameter \\(p\\) for the polynomial kernel at the value 3, since other integer values do not yield nearly as good results. For the radial kernel, we will again use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-4}, 10^{0}]\\). We will set coef0 to \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # values of gamma to consider gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-3, 3, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components ... array for individual kernels -&gt; linear, poly, radial # empty matrix to store results # columns will contain accuracy values for given C # rows will contain values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first loop over values of C for (C in C.cv) { # loop through the individual folds for (index_cv in 1:k_cv) { # define the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEAR KERNEL # model construction clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) accuracy.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for the respective C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # model construction clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) accuracy.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for the respective C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # model construction clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) accuracy.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # store accuracies for the respective C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- accuracy.test.r } } } Now we average the results of the 10-fold cross-validation so that we have a single estimate of validation error for each value of the hyperparameter (or each combination of values). We will also determine the optimal values of the individual hyperparameters. Code # Calculate average accuracies for each C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at the optimal values. For the linear kernel, we have an optimal value of \\(C\\) equal to 0.1; for the polynomial kernel, \\(C\\) is equal to 1; and for the radial kernel, we have two optimal values: the optimal value for \\(C\\) is 1000 and for \\(\\gamma\\) it is 10^{-4}. The validation errors are 0.0066667 for linear, 0.0195833 for polynomial, and 0.0066667 for radial kernels. Finally, we can construct the final classifiers on the entire training dataset using the hyperparameter values determined by the 10-fold cross-validation. We will also determine the errors on the test and training data. Code # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is therefore 1.3333 % for the linear kernel, 1.3333 % for the polynomial kernel, and 0.6667 % for the radial kernel. On the test data, the error rate of the method is 4.6154 % for the linear kernel, 4.6154 % for the polynomial kernel, and 9.2308 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - class&#39;, &#39;SVM poly - class&#39;, &#39;SVM rbf - class&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.3 Principal Component Scores In this case, we will use the scores of the first $p = $ 2 principal components. Now, let’s attempt to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation, differing from the approach in previous chapters. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) is present in all kernel functions, allowing for the possibility that its optimal value may differ across kernels. For all three kernels, we will examine values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{3}]\\), while for the polynomial kernel we will fix the hyperparameter \\(p\\) at the value 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use 10-fold cross-validation to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-3}, 10^{2}]\\). We will set coef0 to \\(= 1\\). Code set.seed(42) # Define the gamma values to consider gamma.cv &lt;- 10^seq(-3, 2, length = 6) C.cv &lt;- 10^seq(-3, 3, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store results # Columns will represent accuracies for given C values # Rows will represent values for given gamma, with layers corresponding to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, iterate over the values of C for (C in C.cv) { # Iterate over the individual folds for (index_cv in 1:k_cv) { # Define the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEAR KERNEL # Model construction clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies at positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies at positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Model construction clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies at positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we will average the results of the 10-fold CV so that for each hyperparameter value (or combination of values), we have one estimate of the validation error. We will also determine the optimal values for the individual hyperparameters. Code # Compute average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For the linear kernel, we have the optimal value of \\(C\\) equal to 0.1, for the polynomial kernel \\(C\\) is equal to 0.1, and for the radial kernel, we have two optimal values: for \\(C\\), the optimal value is 100 and for \\(\\gamma\\), it is 1. The validation errors are 0.3417857 for linear, 0.3292857 for polynomial, and 0.3222024 for radial kernel. Finally, we can construct the final classifiers on the entire training data using the hyperparameter values determined through 10-fold CV. We will also determine the errors on the test and training data. Code # Model construction clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the principal component scores on the training data is 32 % for the linear kernel, 33.33 % for the polynomial kernel, and 17.33 % for the Gaussian kernel. On the test data, the error rates are 30.7692 % for the linear kernel, 27.6923 % for the polynomial kernel, and 40 % for the radial kernel. For graphical representation of the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, as done in previous cases when we plotted the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat content&#39;, linetype = &#39;Kernel&#39;) + scale_colour_discrete(labels = c(&quot;small&quot;, &quot;large&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 3.4: Scores of the first two principal components, color-coded according to class membership. The decision boundary (line or curves in the plane of the first two principal components) between classes is shown in black, constructed using the SVM method. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.4 Basis Coefficients Finally, we use a B-spline basis to express the functions. For all three kernels, we examine the values of hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{3}]\\). For the polynomial kernel, we fix the hyperparameter \\(p\\) at 3, as other integer values do not yield nearly as good results. On the other hand, for the radial kernel, we again use 10-fold CV to select the optimal value of hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-4}, 10^{0}]\\). We set coef0 \\(= 1\\). Code set.seed(42) # gamma values to consider gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-3, 3, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # list with three components...array for each kernel -&gt; linear, poly, radial # empty matrix to store individual results # columns hold accuracy values for given # rows represent values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # first iterate through values of C for (C in C.cv) { # iterate over individual folds for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEAR KERNEL # model creation clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # model creation clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # model creation clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert accuracies in positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we will average the results from 10-fold cross-validation so that we have a single estimate of validation error for each hyperparameter value (or combination of values). We will also determine the optimal values for each of the hyperparameters. Code # let&#39;s calculate the average accuracies for individual C over folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s look at how the optimal values turned out. For linear kernel, the optimal value of \\(C\\) is 10, for polynomial kernel, \\(C\\) is 100, and for radial kernel, we have two optimal values; for \\(C\\), the optimal value is 100 and for \\(\\gamma\\), it is 0.01. The validation errors are 0.0195833 for linear, 0.0325 for polynomial, and 0.0392262 for radial kernel. Finally, we can build the final classifiers on the entire training data with hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # building the models clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the base coefficients on the training data is 0.67 % for the linear kernel, 0.67 % for the polynomial kernel, and 1.33 % for the Gaussian kernel. On the test data, the error rate is 6.1538 % for the linear kernel, 9.2308 % for the polynomial kernel, and 6.1538 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.5 Projection onto B-spline Basis Another way to apply the classic SVM method to functional data is to project the original data onto some \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal H\\), denoted as \\(V_d\\). Let’s assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), which we can express as \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Now we can use the coefficients from the orthogonal projection for classification, applying standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By using this transformation, we have defined a new, so-called adapted kernel, which consists of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector machine. Thus, we have the (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This is a dimensionality reduction method that we can call filtering. For the actual projection, we will use the project.basis() function from the fda library in R. Its input will be a matrix of the original discrete (unsmoothed) data, the values in which we measure values in the original data matrix, and the basis object onto which we want to project the data. We choose to project onto a B-spline basis because the use of a Fourier basis is not suitable for our non-periodic data. The dimension \\(d\\) is chosen either based on some prior expert knowledge or through cross-validation. In our case, we will determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (with \\(k \\ll n\\) due to the computational intensity of the method; commonly \\(k = 5\\) or \\(k = 10\\) is chosen). We require B-splines of order 4, and the relationship for the number of basis functions is given by \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). However, in R, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and for large values of \\(n_{basis}\\), we risk overfitting the model. Thus, we choose a maximum of \\(n_{basis}\\) to be a smaller number, say 20. Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # All dimensions we want to try # Split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components ... matrices for each kernel -&gt; linear, poly, radial # Empty matrix to store results # Columns will be accuracy values for a given part of the training set # Rows will correspond to values for a given dimension CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Projecting discrete data onto B-spline basis of dimension d Projection &lt;- project.basis(y = XX, # Matrix of discrete data argvals = t, # Vector of arguments basisobj = bbasis) # Basis object # Splitting into training and testing data for CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Model construction clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## Linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## Radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracies in positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # Compute average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 We see that the optimal value of the parameter \\(d\\) is 9 for the linear kernel with an error rate calculated using 10-fold CV of 0.0196, 7 for the polynomial kernel with an error rate calculated using 10-fold CV of 0.033, and 6 for the radial kernel with an error rate of 0.1412. For clarity, let’s plot the validation error rates as a function of the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation Error&#39;, colour = &#39;Kernel&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 1.23: Dependence of validation error on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The optimal values of the dimension \\(V_d\\) for each kernel function are marked with black dots. Now we can train individual classifiers on all training data and examine their performance on the test data. For each kernel function, we choose the dimension of the subspace onto which we project based on the results of cross-validation. In the variable Projection, we have stored the matrix of coefficients for the orthogonal projection, which is given by \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Loop through the individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Create the base object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Project discrete data onto the B-spline basis Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # Split into training and testing data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is therefore 2 % for the linear kernel, 2.67 % for the polynomial kernel, and 9.33 % for the Gaussian kernel. The error rate of the method on the test data is then 6.15 % for the linear kernel, 6.15 % for the polynomial kernel, and 10.77 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.6 RKHS + SVM In this section, we will explore another way to utilize the support vector machine (SVM) method for the classification of functional data. Again, we will follow the familiar principle of expressing functional data as finite-dimensional objects, to which we then apply the traditional SVM method. However, this time we will use SVM for the representation of functional data itself through a certain finite-dimensional object. As the name suggests, it combines two concepts: the support vector machine and a space known in English literature as the Reproducing Kernel Hilbert Space (RKHS). The key concept for this space is the kernel. 6.3.7.6.1 Implementation of the method in R From the last part of Theorem 1.3, we derive how to compute the representations of curves in practice. We will work with discretized data after smoothing the curves. First, let’s define the kernel for the RKHS space. We will use the Gaussian kernel with the parameter \\(\\gamma\\). The value of this hyperparameter significantly affects the behavior and success of the method, so we must pay special attention to its choice (we will select it using cross-validation). After trying several options, good hyperparameter choices appear to be \\(\\varepsilon = 0.01\\) and \\(C = 1\\). Due to computational demands, we will not estimate these hyperparameters using CV. Code eps &lt;- 0.01 C &lt;- 1 6.3.7.6.1.1 Gaussian Kernel Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s compute the matrix \\(K_S\\) and its eigenvalues and corresponding eigenvectors. Code # Compute the matrix K gamma &lt;- 0.1 # fixed value for gamma; we will determine the optimal value using CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To compute the coefficients in the representation of curves, that is, the calculation of vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat d l}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we also need the coefficients from SVM. Unlike the classification problem, we are now dealing with a regression problem since we are trying to express our observed curves in some (chosen by us via the kernel \\(K\\)) basis. Thus, we will use the Support Vector Regression method, from which we will subsequently obtain the coefficients \\(\\alpha_{il}\\). Code # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } We can now compute the representations of the individual curves. First, let’s set \\(\\hat d\\) to the full dimension, that is, \\(\\hat d = m ={}\\) 101, and then we will determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # Determine the vector lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have stored in the matrix Lambda.RKHS the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) for each curve in the columns. We will use these vectors as a representation of the given curves and classify the data according to this discretization. Code # Split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on testing data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 6.1: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error, and \\(\\widehat{Err}_{test}\\) denotes the testing error estimate. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0.0133 0.1231 SVM poly - RKHS 0.0267 0.0308 SVM rbf - RKHS 0.0400 0.0308 We see that the model classifies the training data very well for all three kernels, while its success on the testing data is not good for the linear kernel. Therefore, we will use cross-validation to determine the optimal values for \\(\\gamma\\) and \\(d\\). Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:30 # reasonable range of values for d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # List with three components... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store individual results # Columns will contain accuracy values for given # Rows will contain values for given gamma, and layers correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation itself for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate over folds for (index_cv in 1:k_cv) { # Define testing and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Build the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - accuracy.test } # Insert accuracies into positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # We calculate the average accuracies for each method across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.2: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 11 1.0000 0 linear poly 27 1.0000 0 polynomial radial 30 3.7276 0 radial We see that the optimal parameter values are \\(d={}\\) 11 and \\(\\gamma={}\\) 1 for the linear kernel with an error value calculated using 10-fold CV 0, \\(d={}\\) 27 and \\(\\gamma={}\\) 1 for the polynomial kernel with an error value calculated using 10-fold CV 0, and \\(d={}\\) 30 and \\(\\gamma={}\\) 3.7276 for the radial kernel with an error value of 0. For interest, let’s plot the validation error function in relation to the dimension \\(d\\) and the hyperparameter \\(\\gamma\\) values. Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 6.10: Dependence of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. In the above graphs, we observe how the validation error varied depending on the hyperparameter values \\(d\\) and \\(\\gamma\\). Notably, in all three graphs for the individual kernels, significant horizontal structures are apparent. From this, we can draw significant theoretical and practical insights— the considered classification method (projection onto RKHS using SVM + SVM classification) is robust to the choice of hyperparameter \\(d\\) (i.e., a small change in this parameter value does not lead to a significant deterioration in validation error), while we must be very cautious with the choice of hyperparameter \\(\\gamma\\) (even a small change in its value can lead to a large change in validation error). This behavior is most evident with the Gaussian kernel. Since we have already found the optimal values for the hyperparameters, we can construct the final models and assess their classification success on the test data. Code # We remove the last column containing the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # We also add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data table for storing results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate through the individual kernels for (kernel_number in 1:3) { # Calculate the K matrix gamma &lt;- gamma.opt[kernel_number] # gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Model building clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 6.3: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0308 SVM rbf - RKHS - radial 0 0.0154 The error rate of the SVM method combined with the projection onto the Reproducing Kernel Hilbert Space is therefore 0 % on the training data for the linear kernel, 0 % for the polynomial kernel, and 0 % for the Gaussian kernel. On the test data, the error rate of the method is 3.08 % for the linear kernel, 3.08 % for the polynomial kernel, and 1.54 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.6.1.2 Polynomial Kernel Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate through dimensions &lt;- 2:30 # reasonable range for d poly.cv &lt;- 2:5 # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store the results # Columns will be accuracy values for the given parameters # Rows will correspond to values for p and layers will correspond to folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation itself for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate through folds for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Model construction clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies for the given d, gamma, and fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.4: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes testing error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 20 5 0.0474 linear poly 10 3 0.0461 polynomial radial 7 5 0.0403 radial We see that the optimal value for the parameter \\(d={}\\) 20 and \\(p={}\\) 5 is achieved for the linear kernel, with an error rate calculated using 10-fold CV of 0.0474. For the polynomial kernel, \\(d={}\\) 10 and \\(p={}\\) 3 yield an error rate of 0.0461, and for the radial kernel, \\(d={}\\) 7 and \\(p={}\\) 5 result in an error rate of 0.0403. Since we have already identified the optimal values for the hyperparameters, we can now construct the final models and determine their classification performance on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # Iterate over each kernel for (kernel_number in 1:3) { # Calculate matrix K p &lt;- poly.opt[kernel_number] # value of p determined by CV K &lt;- Kernel.RKHS(t.seq, p = p) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model fitting for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # Assigning alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replacing zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine lambda vectors Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Model fitting clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 6.5: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is therefore 3.33 % on the training data for the linear kernel, 2.67 % for the polynomial kernel, and 3.33 % for the Gaussian kernel. On the test data, the error rates are 6.15 % for the linear kernel, 10.77 % for the polynomial kernel, and 10.77 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.7.6.1.3 Linear Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to explore dimensions &lt;- 2:40 # reasonable range of values for d # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix to store results # Columns will have accuracy values for given d # Rows will have values for folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model fitting for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate over folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate over individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Model fitting clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies for given d, gamma, and fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # Calculate average accuracies for individual d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.6: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error rate and \\(\\widehat{Err}_{test}\\) the test error rate. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 15 0.0667 linear poly 16 0.0454 polynomial radial 25 0.0526 radial We see that the best value of the parameter \\(d={}\\) 15 for the linear kernel with an error rate calculated using 10-fold CV 0.0667, \\(d={}\\) 16 for the polynomial kernel with an error rate calculated using 10-fold CV 0.0454 and \\(d={}\\) 25 for the radial kernel with an error rate 0.0526. Since we have already found the optimal values of the hyperparameters, we can construct the final models and determine their classification success on the test data. Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # Loop through individual kernels for (kernel_number in 1:3) { # Calculate the matrix K K &lt;- Kernel.RKHS(t.seq) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine coefficients alpha from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine vector lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 6.7: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimated training error rate and \\(\\widehat{Err}_{test}\\) the test error rate. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 The error rate of the SVM method combined with projection onto Reproducing Kernel Hilbert Space is thus on the training data equal to 4 % for the linear kernel, 1.33 % for the polynomial kernel, and 2 % for the Gaussian kernel. On the test data, the error rate is then 9.23 % for the linear kernel, 3.08 % for the polynomial kernel, and 3.08 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.3.8 Results Table From the table below, we can see that the individual classification methods show significant differences in classification success. In particular, traditional methods like KNN, LDA, or QDA perform quite poorly. We can notice that all methods based on functional principal component analysis do not achieve comparable results to some other methods. In contrast, the RKHS method, along with SVM, stands out for its good classification ability. It is worth noting that classical SVM with a linear kernel also performs quite well. Generally, a linear kernel is a good choice (as we have already seen earlier), as it approximates a certain integral well on the considered interval \\(I\\) for a sufficiently dense network of points. Table 6.8: Summary of results of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of the training error rate and \\(\\widehat{Err}_{test}\\) the test error rate. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.1467 0.1692 LDA 0.3200 0.2923 QDA 0.3200 0.3077 LR functional 0.0000 0.0923 LR score 0.3067 0.2923 Tree - discretization 0.2933 0.3538 Tree - score 0.3267 0.3846 Tree - Bbasis 0.2267 0.2462 RForest - discretization 0.0200 0.1231 RForest - score 0.0467 0.3077 RForest - Bbasis 0.0133 0.1231 SVM linear - func 0.0067 0.0923 SVM poly - func 0.0133 0.0615 SVM rbf - func 0.0133 0.0462 SVM linear - class 0.0133 0.0462 SVM poly - class 0.0133 0.0462 SVM rbf - class 0.0067 0.0923 SVM linear - PCA 0.3200 0.3077 SVM poly - PCA 0.3333 0.2769 SVM rbf - PCA 0.1733 0.4000 SVM linear - Bbasis 0.0067 0.0615 SVM poly - Bbasis 0.0067 0.0923 SVM rbf - Bbasis 0.0133 0.0615 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0308 SVM rbf - RKHS - radial 0.0000 0.0154 SVM linear - RKHS - poly 0.0333 0.0615 SVM poly - RKHS - poly 0.0267 0.1077 SVM rbf - RKHS - poly 0.0333 0.1077 SVM linear - RKHS - linear 0.0400 0.0923 SVM poly - RKHS - linear 0.0133 0.0308 SVM rbf - RKHS - linear 0.0200 0.0308 6.4 Classification Using the Second Derivative As we have mentioned earlier, it is appropriate to consider the second derivative for classification with this data. We have already calculated it above, so we can now proceed directly to constructing the models. We will perform a similar analysis as in the previous situation, then (since we randomly split the data into test and training parts) we will conduct a simulation study that will allow us to compare the individual classification methods better and with much greater power. Code # splitting into test and training parts set.seed(42) split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # creating a vector of 0s and 1s, 0 for &lt; 20 and 1 for &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) We will also look at the representation of the individual groups in the test and training parts of the data. Code # absolute representation table(Y.train) ## Y.train ## 0 1 ## 91 59 Code table(Y.test) ## Y.test ## 0 1 ## 47 18 Code # relative representation table(Y.train) / sum(table(Y.train)) ## Y.train ## 0 1 ## 0.6066667 0.3933333 Code table(Y.test) / sum(table(Y.test)) ## Y.test ## 0 1 ## 0.7230769 0.2769231 6.4.1 \\(K\\) Nearest Neighbors Let’s start with a nonparametric classification method, namely the \\(K\\) nearest neighbors method. First, we will create the necessary objects so that we can further work with them using the classif.knn() function from the fda.usc library. Code x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) Now we can define the model and look at its classification success. The last question remains, how to choose the optimal number of neighbors \\(K\\). We could select this number as the \\(K\\) at which the minimum error rate occurs on the training data. However, this could lead to model overfitting, so we will use cross-validation. Given the computational complexity and the size of the dataset, we will choose \\(k\\)-fold CV; we will select, for example, \\(k = 10\\). Code # model for all training data for K = 1, 2, ..., sqrt(n_train) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = c(1:round(sqrt(length(y.train))))) neighb.model$max.prob # maximum accuracy ## [1] 0.9866667 Code (K.opt &lt;- neighb.model$h.opt) # optimal value of K ## [1] 3 Let’s repeat the previous procedure for the training data, which we will split into \\(k\\) parts and thus repeat this part of the code \\(k\\) times. Code k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:(2 * ceiling(sqrt(length(y.train))))) # number of neighbors # we split the training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # empty matrix to insert individual results # the columns will hold the accuracy values for the given part of the training set # the rows will hold the values for the given value of K neighbors CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # defining the specific index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # we go through each part ... repeating this k times for(neighbour in neighbours) { # model for a specific choice of K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # prediction on the validation part model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # accuracy on the validation part accuracy &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # we insert the accuracy in the position for the given K and fold CV.results[neighbour, index] &lt;- accuracy } } # calculate average accuracies for each K over folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) accuracy.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) We see that the best value for the parameter \\(K\\) is 3, with an error rate calculated using 10-fold CV of 0.0103. For clarity, let’s also plot the validation error rate as a function of the number of neighbors \\(K\\). Code CV.results |&gt; ggplot(aes(x = K, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(K, &#39; ; &#39;, K[optimal] == .(K.opt))), y = &#39;Validation Error&#39;) + scale_x_continuous(breaks = neighbours) ## Warning in geom_point(aes(x = K.opt, y = 1 - accuracy.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 26 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.12: Dependency of validation error on the value of \\(K\\), i.e., the number of neighbors. Now that we know the optimal value of the parameter \\(K\\), we can construct the final model. Code neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predictions model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) # accuracy on test data presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, we see that the error of the model built using the \\(K\\) nearest neighbors method with the optimal choice of \\(K_{optimal}\\) equal to 3, which we determined through cross-validation, is 0.0133 on the training data and 0.0769 on the test data. To compare individual models, we can use both types of errors; for clarity, we will store them in a table. Code RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) 6.4.2 Linear Discriminant Analysis As a second method for constructing a classifier, we will consider Linear Discriminant Analysis (LDA). Since this method cannot be applied to functional data, we must first discretize it using functional principal component analysis. We will then perform the classification algorithm on the scores of the first \\(p\\) principal components. We will choose the number of components \\(p\\) such that the first \\(p\\) principal components together explain at least 90% of the variability in the data. Let’s first perform functional principal component analysis and determine the number \\(p\\). Code # Principal component analysis data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximum number of PCs nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # determining p if(nharm == 1) nharm &lt;- 2 # to plot graphs, we need at least 2 PCs data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # scores of the first p PCs data.PCA.train$Y &lt;- factor(Y.train) # membership to classes In this specific case, we took the number of principal components to be \\(p=\\) 2, which together explain 93.12 \\(\\%\\) of the variability in the data. The first principal component explains 77.7 % and the second 15.42 \\(\\%\\) of the variability. We can visually display the scores of the first two principal components, color-coded according to classification class membership. Code data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() Figure 6.11: Scores of the first two principal components for training data. Points are color-coded according to classification class membership. To determine the classification accuracy on the test data, we need to compute the scores for the first 2 principal components for the test data. These scores are calculated using the formula: \\[ \\xi_{i, j} = \\int \\left( X_i(t) - \\mu(t)\\right) \\cdot \\rho_j(t)\\text dt, \\] where \\(\\mu(t)\\) is the mean function and \\(\\rho_j(t)\\) is the eigenfunction (functional principal component). Code # Calculation of scores for test functions scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # empty matrix for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-th observation - mean function scores[k, ] = inprod(xfd, data.PCA$harmonics) # scalar product of the residual and eigenfunctions rho (functional principal components) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) Now we can construct the classifier on the training part of the data. Code # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the error of the classifier on the training data (4 %) as well as on the test data (9.23 %). To visually represent the method, we can mark the decision boundary in the plot of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function. Code # Add the decision boundary np &lt;- 1001 # number of grid points # x-axis ... 1st PC nd.x &lt;- seq(from = min(data.PCA.train$V1), to = max(data.PCA.train$V1), length.out = np) # y-axis ... 2nd PC nd.y &lt;- seq(from = min(data.PCA.train$V2), to = max(data.PCA.train$V2), length.out = np) # case for 2 PCs ... p = 2 nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y) # case for p = 3 if(dim(data.PCA.train)[2] == 4) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1])} # case for p = 4 if(dim(data.PCA.train)[2] == 5) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1])} # case for p = 5 if(dim(data.PCA.train)[2] == 6) { nd &lt;- expand.grid(V1 = nd.x, V2 = nd.y, V3 = data.PCA.train$V3[1], V4 = data.PCA.train$V4[1], V5 = data.PCA.train$V5[1])} # Add Y = 0, 1 nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 6.12: Scores of the first two principal components, color-coded according to classification class. The decision boundary (a line in the plane of the first two principal components) between classes constructed using LDA is marked in black. We see that the decision boundary is a line, a linear function in the 2D space, which is what we expected from LDA. Finally, we will add the errors to the summary table. Code Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.3 Quadratic Discriminant Analysis Next, let’s build a classifier using Quadratic Discriminant Analysis (QDA). This is analogous to LDA, with the difference that we now allow for different covariance matrices for each class of the normal distribution from which the corresponding scores originate. This dropped assumption of equal covariance matrices leads to a quadratic boundary between the classes. In R, QDA is performed similarly to LDA in the previous section. We would again calculate scores for the training and test functions using functional principal component analysis and construct a classifier based on the scores of the first \\(p\\) principal components to predict the membership of the test curves to class \\(Y^* \\in \\{0, 1\\}\\). However, we do not need to perform functional PCA, as we can use the results from the LDA section. Thus, we can directly proceed to constructing the classifier using the qda() function. Subsequently, we will calculate the accuracy of the classifier on the test and training data. Code # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # accuracy on training data predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() We have thus calculated the error of the classifier on the training data (0.67 %) as well as on the test data (1.54 %). To visually represent the method, we can mark the decision boundary in the plot of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, just as we did in the case of LDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.QDA, newdata = nd)$class)) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (explained variability&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;) + scale_color_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.26: Scores of the first two principal components, color-coded according to class. The decision boundary (a parabola in the plane of the first two principal components) between classes constructed using QDA is marked in black. Note that the decision boundary between the classification classes is now a parabola, which only (at least visually) differs very little from a line. Finally, we will add the errors to the summary table. Code Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.4 Logistic Regression Logistic regression can be performed in two ways. First, we can use the functional analogue of classical logistic regression, and second, we can perform classical multivariate logistic regression on the scores of the first \\(p\\) principal components. 6.4.4.1 Functional Logistic Regression Similarly to the finite-dimensional case of input data, we consider a logistic model in the form: \\[ g\\left(\\mathbb E [Y|X = x]\\right) = \\eta (x) = g(\\pi(x)) = \\alpha + \\int \\beta(t)\\cdot x(t) \\text d t, \\] where \\(\\eta(x)\\) is the linear predictor taking values from the interval \\((-\\infty, \\infty)\\), \\(g(\\cdot)\\) is the link function (in the case of logistic regression, this is the logit function \\(g: (0,1) \\rightarrow \\mathbb R,\\ g(p) = \\ln\\frac{p}{1-p}\\)), and \\(\\pi(x)\\) is the conditional probability \\[ \\pi(x) = \\text{Pr}(Y = 1 | X = x) = g^{-1}(\\eta(x)) = \\frac{\\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}{1 + \\text e^{\\alpha + \\int \\beta(t)\\cdot x(t) \\text d t}}, \\] where \\(\\alpha\\) is a constant and \\(\\beta(t) \\in L^2[a, b]\\) is a parameter function. Our goal is to estimate this parameter function. For functional logistic regression, we will use the fregre.glm() function from the fda.usc package. First, we will create suitable objects for constructing the classifier. Code # create suitable objects x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # points where the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 100 # B-spline basis basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = nbasis.x) To estimate the parameter function \\(\\beta(t)\\), we need to express it in some basis representation, in this case, the B-spline basis. However, we need to find a suitable number of basis functions. We could determine this based on the error on the training data, but this data would tend to favor selecting a large number of bases, leading to overfitting the model. Let’s illustrate this with the following case. For each number of bases \\(n_{basis} \\in \\{4, 5, \\dots, 30\\}\\), we will train a model on the training data, determine the error rate on it, and also calculate the error rate on the test data. Remember that we cannot use the same data for selecting the appropriate number of bases when estimating the test error, as this would underestimate the error. Code n.basis.max &lt;- 30 n.basis &lt;- 4:n.basis.max pred.baz &lt;- matrix(NA, nrow = length(n.basis), ncol = 2, dimnames = list(n.basis, c(&#39;Err.train&#39;, &#39;Err.test&#39;))) for (i in n.basis) { # bases for betas basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = i) # relationship f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) # smoothed data basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into matrix pred.baz[as.character(i), ] &lt;- 1 - c(presnost.train, presnost.test) } pred.baz &lt;- as.data.frame(pred.baz) pred.baz$n.basis &lt;- n.basis Let’s visualize the progression of both types of errors in a graph depending on the number of basis functions. Code n.basis.beta.opt &lt;- pred.baz$n.basis[which.min(pred.baz$Err.test)] pred.baz |&gt; ggplot(aes(x = n.basis, y = Err.test)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + geom_line(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, linetype = &#39;dashed&#39;, linewidth = 0.5) + geom_point(size = 1.5) + geom_point(aes(x = n.basis, y = Err.train), colour = &#39;deepskyblue3&#39;, size = 1.5) + geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.beta.opt))), y = &#39;Error Rate&#39;) ## Warning: Use of `pred.baz$Err.test` is discouraged. ## ℹ Use `Err.test` instead. ## Warning in geom_point(aes(x = n.basis.beta.opt, y = min(pred.baz$Err.test)), : All aesthetics have length 1, but the data has 27 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.16: Dependence of test and training error rates on the number of basis functions for \\(\\beta\\). The red dot represents the optimal number \\(n_{optimal}\\) chosen as the minimum of the test error rate, the black line represents the test error, and the blue dashed line represents the training error rate. We see that as the number of bases for \\(\\beta(t)\\) increases, the training error rate (blue line) tends to decrease, suggesting that we would choose large values for \\(n_{basis}\\) based on it. Conversely, the optimal choice based on the test error rate is \\(n\\) equal to 9, which is significantly smaller than 30. On the other hand, as \\(n\\) increases, the test error rate rises, indicating model overfitting. For these reasons, we will use 10-fold cross-validation to determine the optimal number of basis functions for \\(\\beta(t)\\). We take a maximum of 25 considered basis functions since, as we saw above, beyond this value, model overfitting occurs. Code ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # divide training data into k parts folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) ## elements that do not change during the loop # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # B-spline bases basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) # relationship f &lt;- Y ~ x # bases for x basis.x &lt;- list(&quot;x&quot; = basis1) # empty matrix to insert individual results # columns will hold accuracy values for each part of the training set # rows will hold values for a given number of bases CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) Now we have everything prepared to calculate the error rate on each of the ten subsets of the training set. We will then determine the average and take the optimal \\(n\\) as the argument of the minimum validation error rate. Code for (index in 1:k_cv) { # define the given index set fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # bases for betas basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # accuracy on the validation part newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # insert into the matrix CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # calculate average accuracy for each n across folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] presnost.opt.cv &lt;- max(CV.results) Let’s plot the course of validation error with the highlighted optimal value of \\(n_{optimal}\\) equal to 16 and validation error equal to 0.0483. Code CV.results &lt;- data.frame(n.basis = n.basis, CV = CV.results) CV.results |&gt; ggplot(aes(x = n.basis, y = 1 - CV)) + geom_line(linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) + geom_point(size = 1.5) + geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &#39;red&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(n[basis], &#39; ; &#39;, n[optimal] == .(n.basis.opt))), y = &#39;Validation Error&#39;) + scale_x_continuous(breaks = n.basis) + theme(panel.grid.minor = element_blank()) ## Warning in geom_point(aes(x = n.basis.opt, y = 1 - presnost.opt.cv), colour = &quot;red&quot;, : All aesthetics have length 1, but the data has 22 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing ## a single row. Figure 5.17: Dependence of validation error on the value of \\(n_{basis}\\), that is, the number of bases. Now we can define the final model using functional logistic regression, choosing a B-spline basis for \\(\\beta(t)\\) with 16 bases. Code # optimal model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # bases for x and betas basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # input data for the model dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomial model ... logistic regression model model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b, maxit = 1000, epsilon = 1e-2) # accuracy on the training data predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on the test data newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() We have calculated the training error (equal to 0 %) and the test error (equal to 7.69 %). For better understanding, we can also plot the values of the estimated probabilities of belonging to the classification class \\(Y = 1\\) on the training data in relation to the values of the linear predictor. Code data.frame( linear.predictor = model.glm$linear.predictors, response = model.glm$fitted.values, Y = factor(y.train) ) |&gt; ggplot(aes(x = linear.predictor, y = response, colour = Y)) + geom_point(size = 1.5) + scale_color_discrete(labels = c(&quot;small&quot;, &quot;large&quot;)) + geom_abline(aes(slope = 0, intercept = 0.5), linetype = &#39;dashed&#39;) + theme_bw() + labs(x = &#39;Linear Predictor&#39;, y = &#39;Estimated Probabilities Pr(Y = 1|X = x)&#39;, colour = &#39;Fat Content&#39;) Figure 6.13: Dependence of estimated probabilities on the values of the linear predictor. Points are colored according to their belonging to the classification class. We can also display the course of the estimated parametric function \\(\\beta(t)\\) for reference. Code t.seq &lt;- seq(min(t), max(t), length = 1001) beta.seq &lt;- eval.fd(evalarg = t.seq, fdobj = model.glm$beta.l$x) data.frame(t = t.seq, beta = beta.seq) |&gt; ggplot(aes(t, beta)) + geom_abline(aes(slope = 0, intercept = 0), linetype = &#39;dashed&#39;, linewidth = 0.5, colour = &#39;grey&#39;) + geom_line() + theme_bw() + labs(x = expression(x[1]), y = expression(widehat(beta)(t))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Figure 5.18: Course of the estimate of the parametric function \\(\\beta(t), t \\in [850, 1050]\\). We see that the values of the function \\(\\hat\\beta(t)\\) stay around zero for times \\(t\\) from the middle and beginning of the interval \\([850, 1050]\\), while for later times, the values are higher. This implies the differences between functions from the classification classes at the beginning and end of the interval, while in the middle of the interval, the functions are very similar. We will again add the results to the summary table. Code Res &lt;- data.frame(model = &#39;LR functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.4.2 Logistic Regression with Principal Component Analysis To construct this classifier, we need to perform functional principal component analysis, determine the appropriate number of components, and compute score values for the test data. We have already done this in the linear discriminant analysis section, so we will use these results in the following part. We can now construct the logistic regression model using the glm(, family = binomial) function. Code # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Code # accuracy on training data predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) accuracy.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) accuracy.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() We have thus calculated the error rate of the classifier on the training (0.67 %) and test data (4.62 %). For a graphical representation of the method, we can mark the decision boundary in the plot of the scores of the first two principal components. We will calculate this boundary on a dense grid of points and display it using the geom_contour() function, just as we did for LDA and QDA. Code nd &lt;- nd |&gt; mutate(prd = as.numeric(predict(clf.LR, newdata = nd, type = &#39;response&#39;))) nd$prd &lt;- ifelse(nd$prd &gt; 0.5, 1, 0) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st principal component (explained variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd principal component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat content&#39;) + scale_colour_discrete(labels = c(&quot;low&quot;, &quot;high&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd), colour = &#39;black&#39;) Figure 1.28: Scores of the first two principal components, color-coded according to class membership. The decision boundary (a line in the plane of the first two principal components) between the classes is marked in black and constructed using logistic regression. Note that the decision boundary between the classification classes is now a line, as in the case of LDA. Finally, let’s add the error rates to the summary table. Code Res &lt;- data.frame(model = &#39;LR score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.5 Decision Trees In this section, we will look at a very different approach to constructing a classifier than methods like LDA or logistic regression. Decision trees are a very popular tool for classification, but as with some previous methods, they are not directly designed for functional data. However, there are ways to convert functional objects into multidimensional ones and then apply the decision tree algorithm to them. We can consider the following approaches: An algorithm constructed on the basis of coefficients, Using principal component scores, Applying discretization of the interval and evaluating the function only on a finite grid of points. We will first focus on interval discretization and then compare the results with the remaining two approaches to constructing a decision tree. 6.4.5.1 Interval Discretization First, we need to define the points from the interval \\(I = [850, 1050]\\) at which we will evaluate the functions. Next, we will create an object in which the rows represent individual (discretized) functions and the columns represent time points. Finally, we will append the column \\(Y\\) with information about class membership, and we will repeat the same for the test data. Code # sequence of points at which we will evaluate the functions t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpose for functions in rows grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() Now we can construct a decision tree where all time points from the vector t.seq will serve as predictors. This classification method is not susceptible to multicollinearity, so we do not need to worry about it. We will choose accuracy as the metric. Code # constructing the model clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree, newdata = grid.data) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, the error rate of the classifier on the test data is 1.54 % and on the training data is 0.67 %. Graphically, we can visualize the decision tree using the fancyRpartPlot() function. We will set the node colors to reflect the previous color coding. This will be an unpruned tree. Code colnames(grid.data) &lt;- c(paste0(&#39;time:&#39;, t.seq), &#39;Y&#39;) fancyRpartPlot(rpart(Y ~ ., data = grid.data, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.30: Graphical representation of the unpruned decision tree. Nodes belonging to class 1 are colored in shades of blue, and nodes belonging to class 0 are colored in shades of red. We can also visualize the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree$finalModel, # final model ... pruned tree extra = 104, # display of desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 6.14: Final pruned decision tree. Finally, let’s add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - discretization&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.5.2 Principal Component Scores Another option for constructing a decision tree is to use principal component scores. Since we have already computed the scores for previous classification methods, we will utilize this knowledge and build a decision tree based on the scores of the first 2 principal components. Code # constructing the model clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, the error rate of the decision tree on the test data is 6.15 % and on the training data is 0.67 %. We can visualize the decision tree constructed on the principal component scores using the fancyRpartPlot() function. We will set the node colors to reflect the previous color coding. This will be an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.PCA.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 1.32: Graphical representation of the unpruned decision tree constructed on principal component scores. Nodes belonging to class 1 are colored in shades of blue, and nodes belonging to class 0 are colored in shades of red. We can also visualize the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.PCA$finalModel, # final model extra = 104, # display of desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 5.23: Final pruned decision tree. Finally, let’s add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.5.3 B-spline Coefficients The last option we will use for constructing the decision tree is to utilize the coefficients expressed in the B-spline basis. First, let’s define the necessary datasets with the coefficients. Code # training dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testing dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) Now we can build the classifier. Code # constructing the model clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # accuracy on training data predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Thus, the error rate of the decision tree on the training data is 0.67 % and on the test data is 1.54 %. We can visualize the decision tree constructed on the B-spline coefficients using the fancyRpartPlot() function. We will set the node colors to reflect the previous color coding. This will be an unpruned tree. Code fancyRpartPlot(rpart(Y ~ ., data = data.Bbasis.train, method = &quot;class&quot;), sub = &#39;&#39;, palettes = c(&#39;Reds&#39;, &#39;Blues&#39;)) Figure 5.24: Graphical representation of the unpruned decision tree constructed on B-spline coefficients. Nodes belonging to class 1 are colored in shades of blue, and nodes belonging to class 0 are colored in shades of red. We can also visualize the final pruned decision tree. Code rpart.plot::rpart.plot(clf.tree.Bbasis$finalModel, # final model extra = 104, # display of desired information box.palette = c(&#39;Reds&#39;, &#39;Blues&#39;), branch.lty = 3, # dotted branch lines shadow.col = 0, # shadows under the node boxes nn = FALSE, under = FALSE, digits = 2) Figure 6.15: Final pruned decision tree. Finally, let’s add the training and test error rates to the summary table. Code Res &lt;- data.frame(model = &#39;Tree - Bbasis&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.6 Random Forests A classifier built using the random forest method involves constructing several individual decision trees, which are then combined to create a single classifier through “voting.” Just as with decision trees, we have several options regarding which data (finite-dimensional) to use for constructing the model. We will again consider the three approaches discussed earlier. The data files with the corresponding variables for all three approaches have already been prepared from the previous section, so we can proceed directly to build the models, calculate the characteristics of each classifier, and add the results to the summary table. 6.4.6.1 Discretization of the Interval In the first case, we utilize the evaluation of functions on a specified grid of points in the interval \\(I = [850, 1050]\\). Code # constructing the model clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF, newdata = grid.data) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 0 % and on the test data is 4.62 %. Code Res &lt;- data.frame(model = &#39;RForest - diskr&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.6.2 Principal Component Scores In this case, we will use the scores of the first \\(p =\\) 2 principal components. Code # constructing the model clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the random forest on the training data is thus 0.67 % and on the test data is 4.62 %. Code Res &lt;- data.frame(model = &#39;RForest - score&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.6.3 B-spline Coefficients Finally, we will use the representation of functions through the B-spline basis. Code # constructing the model clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # number of trees importance = TRUE, nodesize = 5) # accuracy on training data predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of this classifier on the training data is 0 % and on the test data is 3.08 %. Code Res &lt;- data.frame(model = &#39;RForest - Bbasis&#39;, Err.train = 1 - accuracy.train, Err.test = 1 - accuracy.test) RESULTS &lt;- rbind(RESULTS, Res) 6.4.7 Support Vector Machines Now let’s look at the classification of our curves using Support Vector Machines (SVM). The advantage of this classification method is its computational efficiency, as it uses only a few (often very few) observations to define the boundary curve between classes. The main advantage of SVM is the use of the so-called kernel trick, which allows us to replace the standard scalar product with a different scalar product of transformed data without having to define this transformation explicitly. This results in a generally nonlinear decision boundary between the classification classes. A kernel (kernel function) \\(K\\) is a function that satisfies \\[ K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal H}, \\] where \\(\\phi\\) is some (unknown) transformation (feature map), \\(\\mathcal H\\) is a Hilbert space, and \\(\\langle \\cdot, \\cdot \\rangle_{\\mathcal H}\\) is a scalar product on this Hilbert space. In practice, three types of kernel functions are most commonly used: Linear kernel – \\(K(x_i, x_j) = \\langle x_i, x_j \\rangle\\), Polynomial kernel – \\(K(x_i, x_j) = \\big(\\alpha_0 + \\gamma \\langle x_i, x_j \\rangle \\big)^d\\), Radial (Gaussian) kernel – \\(\\displaystyle{K(x_i, x_j) = \\text e^{-\\gamma \\|x_i - x_j \\|^2}}\\). For all the aforementioned kernels, we need to choose a constant \\(C &gt; 0\\), which indicates the degree of penalty for exceeding the decision boundary between classes (inverse regularization parameter). As the value of \\(C\\) increases, the method will penalize misclassified data more and shape the boundary less; conversely, for small values of \\(C\\), the method pays less attention to misclassified data and focuses more on penalizing the shape of the boundary. This constant \\(C\\) is typically set to 1 by default, but we can also determine it directly, for example, using cross-validation. By using cross-validation, we can also determine the optimal values of other hyperparameters that now depend on our choice of kernel function. In the case of a linear kernel, no other parameters are chosen apart from the constant \\(C\\). For polynomial and radial kernels, we need to specify the values of the hyperparameters \\(\\alpha_0, \\gamma, \\text{ and } d\\), whose default values in R are \\(\\alpha_0^{default} = 0, \\gamma^{default} = \\frac{1}{dim(\\texttt{data})}, \\text{ and } d^{default} = 3\\). In the case of functional data, we have several options for using the SVM method. The simplest variant is to apply this classification method directly to the discretized function (see section 6.4.7.2). Another option is to utilize the scores of the principal components to classify the curves based on their representation (see section 6.4.7.3). Another straightforward variant is to use the representation of the curves through the B-spline basis and classify the curves based on the coefficients of their representation in this basis (see section 6.4.7.4). With more complex considerations, we can arrive at several additional options that utilize the functional nature of the data. Firstly, instead of classifying the original curve, we can use its derivative (or even its second, third derivative, etc.). Secondly, we can use projections of functions onto a subspace generated by, for example, B-spline functions (see section 6.4.7.5). The last method we will use for classifying functional data involves combining projection onto a specific subspace generated by functions (Reproducing Kernel Hilbert Space, RKHS) and classifying the corresponding representation. This method utilizes not only classical SVM but also SVM for regression, which is further elaborated in section RKHS + SVM 6.4.7.6. Code # set norm equal to one norms &lt;- c() for (i in 1:dim(XXder$coefs)[2]) { norms &lt;- c(norms, as.numeric(1 / norm.fd(XXder[i]))) } XXfd_norm_der &lt;- XXder XXfd_norm_der$coefs &lt;- XXfd_norm_der$coefs * matrix(norms, ncol = dim(XXder$coefs)[2], nrow = dim(XXder$coefs)[1], byrow = TRUE) # split into test and training set X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() 6.4.7.1 SVM for Functional Data In the fda.usc library, we will use the function classif.svm() to apply the SVM method directly to functional data. First, we will create suitable objects for constructing the classifier. Code # create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; nbasis.x &lt;- 100 # B-spline basis basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = nbasis.x) Code # formula f &lt;- Y ~ x # basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # SVM model model.svm.f &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, degree = 5, coef0 = 1, cost = 1e4) # accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.train &lt;- mean(factor(Y.train_norm) == predictions.train) # accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test &lt;- predict(model.svm.f, newdat, type = &#39;class&#39;) presnost.test &lt;- mean(factor(Y.test_norm) == predictions.test) We calculated the training error (which is 0 %) and the test error (which is 6.15 %). Now let’s attempt, unlike the procedure in the previous chapters, to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will approach each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, acknowledging that its optimal value may differ between kernels. For all three kernels, we will explore the values of the hyperparameter \\(C\\) in the range \\([10^{-2}, 10^{4}]\\), while for the polynomial kernel, we will consider the value of the hyperparameter \\(p\\) to be 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use \\(r k_cv\\)-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the range \\([10^{-5}, 10^{0}]\\). We will set coef0 to 1. Code set.seed(42) k_cv &lt;- 10 # k-fold CV # We split the training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Which values of gamma do we want to consider gamma.cv &lt;- 10^seq(-5, 0, length = 6) C.cv &lt;- 10^seq(-2, 4, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # A list with three components... an array for each kernel -&gt; linear, poly, radial # An empty matrix where we will place individual results # The columns will contain the accuracy values for each # The rows will correspond to the values for a given gamma and the layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we go through the values of C for (cost in C.cv) { # We go through the individual folds for (index_cv in 1:k_cv) { # Definition of the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # Points at which the functions are evaluated tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEAR KERNEL # SVM model clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- accuracy.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Model construction clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- accuracy.test.p } ## RADIAL KERNEL for (gam.cv in gamma.cv) { # Model construction clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # Accuracy on the test data newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(y.test.cv == predictions.test) # We insert the accuracies into positions for the given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- accuracy.test.r } } } Now we will average the results of 10-fold CV so that we have one estimate of validation error for one value of the hyperparameter (or one combination of values). At the same time, we will determine the optimal values of the individual hyperparameters. Code # We calculate the average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] accuracy.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s take a look at how the optimal values turned out. For linear kernel, we have the optimal value \\(C\\) equal to 0.01, for polynomial kernel \\(C\\) is equal to 0.01, and for radial kernel, we have two optimal values, for \\(C\\) the optimal value is 1000 and for \\(\\gamma\\) it is 0.01. The validation error rates are 0 for linear, 0.0066667 for polynomial, and 0 for radial kernel. Finally, we can construct the final classifiers on the entire training data with the hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # Create suitable objects x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # Points at which the functions are evaluated tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline basis # basis1 &lt;- X.train_norm$basis # Formula f &lt;- Y ~ x # Basis for x basis.x &lt;- list(&quot;x&quot; = basis1) # Input data for the model ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) Code model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # Accuracy on training data newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # Accuracy on test data newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) accuracy.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) accuracy.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) accuracy.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) The error rate of the SVM method on the training data is thus 0 % for the linear kernel, 0 % for the polynomial kernel, and 0 % for the Gaussian kernel. On the test data, the error rate of the method is 3.0769 % for the linear kernel, 0 % for the polynomial kernel, and 4.6154 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(accuracy.train.l, accuracy.train.p, accuracy.train.r), Err.test = 1 - c(accuracy.test.l, accuracy.test.p, accuracy.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.2 Discretization of the Interval Let’s first apply the Support Vector Machine method directly to discretized data (evaluating the function on a given grid of points over the interval \\(I = [850, 1050]\\)), considering all three aforementioned kernel functions. Now, in contrast to the approach in previous chapters, let’s estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation. Since each kernel has different hyperparameters in its definition, we will treat each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, and we acknowledge that its optimal value may differ between kernels. For all three kernels, we will explore values of the hyperparameter \\(C\\) in the interval \\([10^{-3}, 10^{3}]\\), while for the polynomial kernel, we will fix the hyperparameter \\(p\\) at a value of 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use 10-fold CV to determine the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-3}, 10^{2}]\\). We will set coef0 to \\(= 1\\). Code set.seed(42) k_cv &lt;- 10 # k-fold CV # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Values of gamma to consider gamma.cv &lt;- 10^seq(-4, -1, length = 4) C.cv &lt;- 10^seq(-4, 2, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components... array for each kernel -&gt; linear, poly, radial # Empty matrix to store individual results # Columns will contain accuracy values for the given # Rows will contain values for the given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we will go through the values of C for (C in C.cv) { # Go through individual folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEAR KERNEL # Construct the model clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy at the positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Construct the model clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy at the positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Construct the model clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy at the positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we will average the results of 10-fold CV so that we have one estimate of validation error for each hyperparameter value (or combination of values). We will also determine the optimal values for the individual hyperparameters. Code # Calculate average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s see how the optimal values turned out. For the linear kernel, we have the optimal value of \\(C\\) equal to 0.001, for the polynomial kernel, \\(C\\) is equal to 0.1, and for the radial kernel, we have two optimal values: the optimal value for \\(C\\) is 10 and for \\(\\gamma\\), it is 0.01. The validation errors are 0.0066667 for linear, 0.0066667 for polynomial, and 0.0066667 for radial kernels. Finally, we can build the final classifiers on the entire training data with hyperparameter values determined using 10-fold CV. We will also determine the errors on the test and training data. Code # Building the models clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method on the training data is 0.6667 % for the linear kernel, 0.6667 % for the polynomial kernel, and 0 % for the radial kernel. The error rate on the test data is then 1.5385 % for the linear kernel, 6.1538 % for the polynomial kernel, and 4.6154 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.3 Principal Component Scores In this case, we will use the scores of the first \\(p ={}\\) 2 principal components. Now let’s try to estimate the hyperparameters of the classifiers from the data using 10-fold cross-validation, unlike the approach in previous chapters. Since each kernel has different hyperparameters in its definition, we will treat each kernel function separately. However, the hyperparameter \\(C\\) appears in all kernel functions, with the assumption that its optimal value may differ between kernels. For all three kernels, we will iterate over the hyperparameter \\(C\\) values in the interval \\([10^{-2}, 10^{5}]\\), while for the polynomial kernel, we will fix the hyperparameter \\(p\\) at the value of 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use 10-fold CV to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the interval \\([10^{-2}, 10^{2}]\\). We will set coef0 to \\(= 1\\). Code set.seed(42) # Which gamma values do we want to consider gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-2, 5, length = 8) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix where we will insert individual results # Columns will contain accuracy values for given C # Rows will contain values for given gamma, and layers correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, we will iterate over values of C for (C in C.cv) { # Iterate over individual folds for (index_cv in 1:k_cv) { # Definition of the test and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEAR KERNEL # Building the model clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Building the model clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Building the model clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now we average the results of the 10-fold CV so that we have one estimate of validation error for each hyperparameter value (or combination of values). We will also determine the optimal values for each hyperparameter. Code # Calculate average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s take a look at the optimal values. For the linear kernel, we have the optimal value of \\(C\\) equal to 10, for the polynomial kernel \\(C\\) is equal to 1, and for the radial kernel, we have two optimal values: for \\(C\\), the optimal value is 100 and for \\(\\gamma\\), it is 1. The validation errors are respectively 0.0066667 for linear, 0.0066667 for polynomial, and 0.0066667 for radial kernels. Finally, we can build the final classifiers on the entire training dataset with the hyperparameter values determined by the 10-fold CV. We will also compute the errors on the test and training datasets. Code # Build models clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error rate of the SVM method applied to the principal component scores on the training data is thus 0.67 % for the linear kernel, 0.67 % for the polynomial kernel, and 0.67 % for the Gaussian kernel. On the test data, the error rate of the method is 3.0769 % for the linear kernel, 3.0769 % for the polynomial kernel, and 1.5385 % for the radial kernel. For a graphical representation of the method, we can indicate the decision boundary in the plot of the scores of the first two principal components. We will compute this boundary on a dense grid of points and display it using the geom_contour() function, as we did in previous cases where we also plotted the classification boundary. Code nd &lt;- rbind(nd, nd, nd) |&gt; mutate( prd = c(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.p.PCA, newdata = nd, type = &#39;response&#39;)), as.numeric(predict(clf.SVM.r.PCA, newdata = nd, type = &#39;response&#39;))), kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(as.numeric(predict(clf.SVM.l.PCA, newdata = nd, type = &#39;response&#39;)))) |&gt; as.factor()) data.PCA.train |&gt; ggplot(aes(x = V1, y = V2, colour = Y)) + geom_point(size = 1.5) + labs(x = paste(&#39;1st Principal Component (Explained Variance&#39;, round(100 * data.PCA$varprop[1], 2), &#39;%)&#39;), y = paste(&#39;2nd Principal Component (&#39;, round(100 * data.PCA$varprop[2], 2), &#39;%)&#39;), colour = &#39;Fat Content&#39;, linetype = &#39;Kernel&#39;) + scale_colour_discrete(labels = c(&quot;small&quot;, &quot;large&quot;)) + theme_bw() + geom_contour(data = nd, aes(x = V1, y = V2, z = prd, linetype = kernel), colour = &#39;black&#39;) Figure 5.33: Scores of the first two principal components, colored according to class membership. The decision boundary (line or curves in the plane of the first two principal components) between classes constructed using the SVM method is indicated in black. Code Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.4 Basis Coefficients Finally, we will use the representation of functions with a B-spline basis. For all three kernels, we will iterate over values of the hyperparameter \\(C\\) in the range \\([10^{-4}, 10^{2}]\\), while for the polynomial kernel, we will fix the hyperparameter \\(p\\) at the value of 3, as other integer values do not yield nearly as good results. Conversely, for the radial kernel, we will again use \\(k_{cv}\\)-fold cross-validation to choose the optimal value of the hyperparameter \\(\\gamma\\), considering values in the range \\([10^{-4}, 10^{0}]\\). We will set coef0 = 1. Code set.seed(42) # Values of gamma to consider gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-4, 2, length = 7) p.cv &lt;- 3 coef0 &lt;- 1 # List with three components ... arrays for each kernel -&gt; linear, poly, radial # Empty matrix to store results # Columns will contain accuracy values for given parameters # Rows will correspond to gamma values and layers will correspond to folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # First, iterate over values of C for (C in C.cv) { # Iterate over each fold for (index_cv in 1:k_cv) { # Define test and training sets for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEAR KERNEL # Build the model clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # Accuracy on validation data predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for the given C and fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIAL KERNEL for (p in p.cv) { # Build the model clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # Accuracy on validation data predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for the given C, p, and fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIAL KERNEL for (gamma in gamma.cv) { # Build the model clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # Accuracy on validation data predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store accuracy for the given C, gamma, and fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } Now, let’s average the results from \\(k_{cv}\\)-fold cross-validation so that we have one estimate of the validation error for each hyperparameter value (or combination of values). At the same time, we will determine the optimal values for each hyperparameter. Code # Calculate average accuracies for individual C across folds ## Linear kernel CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomial kernel CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radial kernel CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) Let’s see how the optimal values turned out. For the linear kernel, the optimal value of \\(C\\) is 0.01, for the polynomial kernel \\(C\\) is 0.1, and for the radial kernel we have two optimal values: the optimal value for \\(C\\) is 100 and for \\(\\gamma\\) it is 0.01. The validation errors are 0.0066667 for linear, 0.0133333 for polynomial, and 0.0066667 for radial kernels. Finally, we can build the final classifiers on the entire training data using the hyperparameter values determined by \\(k_{cv}\\)-fold cross-validation. We will also determine the errors on the test and training datasets. Code # Build the models clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # Accuracy on training data predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() The error of the SVM method applied to the basis coefficients on the training data is 0.67 % for the linear kernel, 0.67 % for the polynomial kernel, and 0 % for the radial kernel. On the test data, the error of the method is 7.6923 % for the linear kernel, 7.6923 % for the polynomial kernel, and 6.1538 % for the radial kernel. Code Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.5 Projection onto B-spline Basis Another option for applying the classical SVM method to functional data is to project the original data onto a \\(d\\)-dimensional subspace of our Hilbert space \\(\\mathcal H\\), denoted as \\(V_d\\). Assume that this subspace \\(V_d\\) has an orthonormal basis \\(\\{\\Psi_j\\}_{j = 1, \\dots, d}\\). We define the transformation \\(P_{V_d}\\) as the orthogonal projection onto the subspace \\(V_d\\), which can be expressed as \\[ P_{V_d} (x) = \\sum_{j = 1}^d \\langle x, \\Psi_j \\rangle \\Psi_j. \\] Now, we can use the coefficients from the orthogonal projection for classification, applying standard SVM to the vectors \\(\\left( \\langle x, \\Psi_1 \\rangle, \\dots, \\langle x, \\Psi_d \\rangle\\right)^\\top\\). By utilizing this transformation, we have defined a new, so-called adapted kernel, which consists of the orthogonal projection \\(P_{V_d}\\) and the kernel function of the standard support vector method. Thus, we have (adapted) kernel \\(Q(x_i, x_j) = K(P_{V_d}(x_i), P_{V_d}(x_j))\\). This is a method of dimension reduction, which we can refer to as filtering. For the projection itself, we will use the project.basis() function from the fda library in R. Its input will be a matrix of original discrete (unsmoothed) data, the values in which we measure the values in the original data matrix, and the basis object onto which we want to project the data. We will choose projection onto a B-spline basis because the use of Fourier basis is not suitable for our non-periodic data. The dimension \\(d\\) is chosen either from some prior expert knowledge or using cross-validation. In our case, we will determine the optimal dimension of the subspace \\(V_d\\) using \\(k\\)-fold cross-validation (choosing \\(k \\ll n\\) due to the computational intensity of the method; often, \\(k = 5\\) or \\(k = 10\\) is chosen). We require B-splines of order 4; for the number of basis functions, the relationship is given by \\[ n_{basis} = n_{breaks} + n_{order} - 2, \\] where \\(n_{breaks}\\) is the number of knots and \\(n_{order} = 4\\). In R, however, the value of \\(n_{basis}\\) must be at least \\(n_{order} = 4\\), and for large values of \\(n_{basis}\\), overfitting of the model occurs, so we will choose a maximum of \\(n_{basis}\\) to be a smaller number, say 20. Code k_cv &lt;- 10 # k-fold CV # Values for B-spline basis rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max # all dimensions we want to try # Divide training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # List with three components... matrices for individual kernels -&gt; linear, poly, radial # Empty matrix to insert individual results # Columns will contain accuracy values for each part of the training set # Rows will contain values for each dimension value CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) # Project discrete data onto B-spline basis of dimension d Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # Split into training and testing data within CV XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { # Define testing and training parts for CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # Model construction clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # Accuracy on validation data ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # Insert accuracies into positions for given d and fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # Calculate average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) data.frame(d_opt = d.opt, ERR = 1 - presnost.opt.cv, row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) ## d_opt ERR ## linear 9 0.01958333 ## poly 7 0.03297619 ## radial 6 0.14125000 We see that the optimal value of the parameter \\(d\\) is 9 for the linear kernel, with an error rate calculated using 10-fold CV of 0.0196, 7 for the polynomial kernel with an error rate of 0.033, and 6 for the radial kernel with an error rate of 0.1412. To clarify, let’s plot the validation error rates as a function of the dimension \\(d\\). Code CV.results &lt;- data.frame(d = dimensions |&gt; rep(3), CV = c(CV.results$SVM.l, CV.results$SVM.p, CV.results$SVM.r), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions)) |&gt; factor()) CV.results |&gt; ggplot(aes(x = d, y = 1 - CV, colour = Kernel)) + geom_line(linetype = &#39;dashed&#39;) + geom_point(size = 1.5) + geom_point(data = data.frame(d.opt, presnost.opt.cv), aes(x = d.opt, y = 1 - presnost.opt.cv), colour = &#39;black&#39;, size = 2) + theme_bw() + labs(x = bquote(paste(d)), y = &#39;Validation error rate&#39;, colour = &#39;Kernel&#39;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = dimensions) Figure 6.16: Dependency of validation error on the dimension of the subspace \\(V_d\\), separately for all three considered kernels in the SVM method. The black points indicate the optimal dimension values \\(V_d\\) for each kernel function. Now we can train individual classifiers on all training data and examine their performance on the test data. For each kernel function, we choose the dimension of the subspace to project onto based on the results of cross-validation. In the variable Projection, we have stored the matrix of orthogonal projection coefficients, given by \\[ \\texttt{Projection} = \\begin{pmatrix} \\langle x_1, \\Psi_1 \\rangle &amp; \\langle x_2, \\Psi_1 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_1 \\rangle\\\\ \\langle x_1, \\Psi_2 \\rangle &amp; \\langle x_2, \\Psi_2 \\rangle &amp; \\cdots &amp; \\langle x_n, \\Psi_2 \\rangle\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\langle x_1, \\Psi_d \\rangle &amp; \\langle x_2, \\Psi_d \\rangle &amp; \\dots &amp; \\langle x_n, \\Psi_d \\rangle \\end{pmatrix}_{d \\times n}. \\] Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) # Iterate through the kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] # Basis object bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) # Project discrete data onto B-spline basis Projection &lt;- project.basis(y = XX, # matrix of discrete data argvals = t, # vector of arguments basisobj = bbasis) # basis object # Split into training and test data XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # Build the model clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store the results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } The error rate of the SVM method applied to the basis coefficients on the training data is 2 % for the linear kernel, 2.67 % for the polynomial kernel, and 9.33 % for the Gaussian kernel. The error rate on the test data is 6.15 % for the linear kernel, 6.15 % for the polynomial kernel, and 10.77 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.6 RKHS + SVM In this section, we will explore another way to use the support vector machine (SVM) method for classifying functional data. In this case, we will once again apply a known principle where we first express functional data as finite-dimensional objects, and then apply the classical SVM method to these objects. From the last part of Theorem 1.3, we see how to compute curve representations in practice. We will work with discretized data after smoothing the curves. First, we define the kernel for the RKHS space. We will use the Gaussian kernel with parameter \\(\\gamma\\). The value of this hyperparameter significantly influences the behavior and success of the method, so we must pay special attention to its choice (we choose it using cross-validation). Code # hyperparameter values same as in the previous section eps &lt;- 0.01 C &lt;- 1 6.4.7.6.0.1 Gaussian Kernel Code # remove the last column containing values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # add test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # kernel and kernel matrix ... Gaussian with parameter gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } Now let’s compute the matrix \\(K_S\\) and its eigenvalues and corresponding eigenvectors. Code # compute matrix K gamma &lt;- 0.1 # fixed value of gamma, optimal value determined via CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors To calculate the coefficients in the representation of the curves, i.e., the vectors \\(\\hat{\\boldsymbol \\lambda}_l^* = \\left( \\hat\\lambda_{1l}^*, \\dots, \\hat\\lambda_{\\hat dl}^*\\right)^\\top, l = 1, 2, \\dots, n\\), we need the coefficients from the SVM. Unlike the classification problem, we now solve a regression problem, as we aim to express our observed curves in a basis chosen by the kernel \\(K\\). Therefore, we use the Support Vector Regression method, from which we subsequently obtain the coefficients \\(\\alpha_{il}\\). Code # determine the alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } Now we can compute the representations of the individual curves. First, we choose \\(\\hat d\\) to be the entire dimension, i.e., \\(\\hat d = m ={}\\) 101, and then we determine the optimal \\(\\hat d\\) using cross-validation. Code # d d.RKHS &lt;- dim(alpha.RKHS)[1] # determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } Now we have the matrix Lambda.RKHS storing the vectors \\(\\hat{\\boldsymbol \\lambda}_l^*, l = 1, 2, \\dots, n\\) in its columns for each curve. These vectors will be used as representations of the curves, and we will classify the data based on this discretization. Code # split into training and testing data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] # prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.train = NA, Err.test = NA) # iterate over different kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # construct the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, cost = C, coef0 = coef0, scale = TRUE, kernel = kernel_type) # accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } Table 6.9: Summary results of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) indicates the testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS 0 0 SVM poly - RKHS 0 0 SVM rbf - RKHS 0 0 We see that the model classifies the training data very well for all three kernels, while its performance on the test data is quite poor. It is evident that overfitting has occurred; therefore, we will use cross-validation to determine the optimal values for \\(\\gamma\\) and \\(d\\). Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Hyperparameter values to iterate over dimensions &lt;- 3:30 # reasonable range of values for d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # List with three components ... array for each kernel -&gt; linear, poly, radial # Empty matrix to store results # Columns will contain accuracy values for given parameters # Rows will contain values for given gamma, and layers correspond to folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Cross-validation itself for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Iterate over dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Calculate representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Iterate over folds for (index_cv in 1:k_cv) { # Define test and training portions for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Iterate over kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Constructing the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies in the positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Calculate the average accuracy for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.10: Summary results of cross-validation for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad\\gamma\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 12 1.0000 0 linear poly 18 1.0000 0 polynomial radial 17 1.9307 0 radial We see that the best parameter values are \\(d={}\\) 12 and \\(\\gamma={}\\) 1 for the linear kernel, with an error rate calculated using 10-fold CV of 0. The optimal values for \\(d={}\\) 18 and \\(\\gamma={}\\) 1 correspond to the polynomial kernel, with an error rate of 0, and for \\(d={}\\) 17 and \\(\\gamma={}\\) 1.9307 for the radial kernel, yielding an error rate of 0. For interest, let’s plot the validation error as a function of dimension \\(d\\) and hyperparameter \\(\\gamma\\). Code CV.results.plot &lt;- data.frame(d = rep(dimensions |&gt; rep(3), each = length(gamma.cv)), gamma = rep(gamma.cv, length(dimensions)) |&gt; rep(3), CV = c(c(CV.results$SVM.l), c(CV.results$SVM.p), c(CV.results$SVM.r)), Kernel = rep(c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;), each = length(dimensions) * length(gamma.cv)) |&gt; factor()) CV.results.plot |&gt; ggplot(aes(x = d, y = gamma, z = CV)) + geom_contour_filled() + scale_y_continuous(trans=&#39;log10&#39;) + facet_wrap(~Kernel) + theme_bw() + labs(x = expression(d), y = expression(gamma)) + scale_fill_brewer(palette = &quot;Spectral&quot;) + geom_point(data = df.RKHS.res, aes(x = d, y = gamma), size = 5, pch = &#39;+&#39;) Figure 6.17: Dependency of validation error on the choice of hyperparameters \\(d\\) and \\(\\gamma\\), separately for all three considered kernels in the SVM method. Since we have already found the optimal hyperparameter values, we can construct the final models and assess their classification performance on the test data. Code # Remove the last column, which contains the values of Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # Iterate through the individual kernels for (kernel_number in 1:3) { # Calculate the K matrix gamma &lt;- gamma.opt[kernel_number] # gamma value from CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create empty object # Calculate the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Build models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 6.11: Summary of SVM method results combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) denotes testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - radial 0 0.0308 SVM poly - RKHS - radial 0 0.0769 SVM rbf - RKHS - radial 0 0.0308 The accuracy of the SVM method combined with projection onto Reproducing Kernel Hilbert Space is thus equal to 0 % for the linear kernel, 0 % for the polynomial kernel, and 0 % for the Gaussian kernel on the training data. On the test data, the accuracy of the method is 3.08 % for the linear kernel, 7.69 % for the polynomial kernel, and 3.08 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.6.0.2 Polynomial Kernel Code # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data as well data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # Kernel and kernel matrix ... polynomial with parameter p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } Code # Split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # Remove the last column containing Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Values of hyperparameters to be searched dimensions &lt;- 2:30 # reasonable range of values d poly.cv &lt;- 2:5 # List with three components ... array for individual kernels -&gt; linear, poly, radial # Empty matrix where we will store individual results # In columns will be the accuracy values for the given # In rows will be values for given p and layers corresponding to folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) Code # Actual CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, coef0 = 1, cost = C, epsilon = eps, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # Loop through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # Compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Loop through folds for (index_cv in 1:k_cv) { # Define test and training parts for CV fold &lt;- folds[[index_cv]] # Split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # Prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # Loop through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # Construct the model clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, gamma = 1, kernel = kernel_type) # Accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # Store accuracies at positions for given d, gamma, and fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } } Code # Compute the average accuracies for each d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.12: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\quad\\quad\\quad\\quad\\quad p\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 30 3 0.0077 linear poly 17 2 0.0077 polynomial radial 3 4 0.0254 radial We see that the optimal parameter values are \\(d={}\\) 30 and \\(p={}\\) 3 for the linear kernel with an error value computed using 10-fold CV 0.0077, \\(d={}\\) 17 and \\(p={}\\) 2 for the polynomial kernel with an error value computed using 10-fold CV 0.0077, and \\(d={}\\) 3 and \\(p={}\\) 4 for the radial kernel with an error value 0.0254. Since we have found the optimal hyperparameter values, we can construct the final models and determine their classification success on the test data. Code # Remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # Add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # Prepare a data frame to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # Iterate through individual kernels for (kernel_number in 1:3) { # Compute the K matrix p &lt;- poly.opt[kernel_number] # gamma value using CV K &lt;- Kernel.RKHS(t.seq, p = p) # Determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # Determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # Model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, cost = C, gamma = 1, degree = p) # Determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # Determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # Compute the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # Split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # Construct models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, cost = C, gamma = 1, kernel = kernel_type) # Accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # Accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # Save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 6.13: Summary of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - poly 0.00 0.0308 SVM poly - RKHS - poly 0.00 0.0154 SVM rbf - RKHS - poly 0.02 0.0308 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 0 % for the linear kernel, 0 % for the polynomial kernel, and 2 % for the Gaussian kernel. On the test data, the error rate of the method is 3.08 % for the linear kernel, 1.54 % for the polynomial kernel, and 3.08 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4.7.6.0.3 Linear Kernel Code # remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # also add test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # kernel and kernel matrix ... polynomial with parameter p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } Code # split training data into k parts folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hyperparameter values that we will iterate over dimensions &lt;- 2:40 # reasonable range of values for d # list with three components ... array for individual kernels -&gt; linear, poly, radial # empty matrix into which we will insert the individual results # in columns will be accuracy values for given d # in rows will be values for layers corresponding to folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) Code # the actual CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # iterate through dimensions for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # compute representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # iterate through folds for (index_cv in 1:k_cv) { # define test and training parts for CV fold &lt;- folds[[index_cv]] # split into training and validation data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # prepare a data table to store results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # iterate through individual kernels for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # model construction clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # accuracy on validation data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # store results Res[kernel_number, 2] &lt;- 1 - presnost.test } # insert accuracies into positions for given d, gamma, and fold CV.results$SVM.l[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[1, 2] CV.results$SVM.p[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[2, 2] CV.results$SVM.r[d.RKHS - min(dimensions) + 1, index_cv] &lt;- Res[3, 2] } } Code # calculate the average accuracies for individual d across folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) Table 6.14: Summary of cross-validation results for the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. \\(\\quad\\quad\\quad\\quad\\quad d\\) \\(\\widehat{Err}_{cross\\_validation}\\) Model linear 31 0.0267 linear poly 21 0.0267 polynomial radial 6 0.0592 radial We see that the optimal value of the parameter \\(d={}\\) 31 for the linear kernel yields a cross-validation error of 0.0267, \\(d={}\\) 21 for the polynomial kernel with a cross-validation error of 0.0267, and \\(d={}\\) 6 for the radial kernel with a cross-validation error of 0.0592. Since we have already found the optimal values of the hyperparameters, we can construct the final models and determine their classification performance on the test data. Code # remove the last column, which contains the Y values data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # add the test data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) Code # prepare a data table to store the results Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # iterate over individual kernels for (kernel_number in 1:3) { # calculate the K matrix K &lt;- Kernel.RKHS(t.seq) # determine eigenvalues and eigenvectors Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # determine alpha coefficients from SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # empty object # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # determine alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # replace zeros with coefficients } # d d.RKHS &lt;- d.opt[kernel_number] # determine the lambda vector Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # create an empty object # calculate the representation for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # split into training and test data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # build the models clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, kernel = kernel_type, epsilon = eps, coef0 = 1, gamma = 1, cost = C) # accuracy on training data predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) accuracy.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # accuracy on test data predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) accuracy.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # save results Res[kernel_number, c(2, 3)] &lt;- 1 - c(accuracy.train, accuracy.test) } Table 6.15: Summary of the SVM method combined with RKHS on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error and \\(\\widehat{Err}_{test}\\) the test error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) SVM linear - RKHS - linear 0.00 0.0462 SVM poly - RKHS - linear 0.00 0.0308 SVM rbf - RKHS - linear 0.02 0.1231 The error rate of the SVM method combined with projection onto the Reproducing Kernel Hilbert Space is thus 0 % on the training data for the linear kernel, 0 % for the polynomial kernel, and 2 % for the Gaussian kernel. On the test data, the error rate is 4.62 % for the linear kernel, 3.08 % for the polynomial kernel, and 12.31 % for the radial kernel. Code RESULTS &lt;- rbind(RESULTS, Res) 6.4.8 Results Table From the table below, let us note two significant points. The first is that the methods classify the data significantly better than in the case of the original non-derivative data. In some methods, the improvement is even by tens of percent. The second significant point is that there is not such a pronounced difference between the results of the individual methods now. Table 6.16: Summary of the results of the methods used on the simulated data. \\(\\widehat{Err}_{train}\\) indicates the estimate of training error and \\(\\widehat{Err}_{test}\\) of testing error. Model \\(\\widehat{Err}_{train}\\quad\\quad\\quad\\quad\\quad\\) \\(\\widehat{Err}_{test}\\quad\\quad\\quad\\quad\\quad\\) KNN 0.0133 0.0769 LDA 0.0400 0.0923 QDA 0.0067 0.0154 LR functional 0.0000 0.0769 LR score 0.0067 0.0462 Tree - discretization 0.0067 0.0154 Tree - score 0.0067 0.0615 Tree - Bbasis 0.0067 0.0154 RForest - diskr 0.0000 0.0462 RForest - score 0.0067 0.0462 RForest - Bbasis 0.0000 0.0308 SVM linear - func 0.0000 0.0308 SVM poly - func 0.0000 0.0000 SVM rbf - func 0.0000 0.0462 SVM linear - discr 0.0067 0.0154 SVM poly - discr 0.0067 0.0615 SVM rbf - discr 0.0000 0.0462 SVM linear - PCA 0.0067 0.0308 SVM poly - PCA 0.0067 0.0308 SVM rbf - PCA 0.0067 0.0154 SVM linear - Bbasis 0.0067 0.0769 SVM poly - Bbasis 0.0067 0.0769 SVM rbf - Bbasis 0.0000 0.0615 SVM linear - projection 0.0200 0.0615 SVM poly - projection 0.0267 0.0615 SVM rbf - projection 0.0933 0.1077 SVM linear - RKHS - radial 0.0000 0.0308 SVM poly - RKHS - radial 0.0000 0.0769 SVM rbf - RKHS - radial 0.0000 0.0308 SVM linear - RKHS - poly 0.0000 0.0308 SVM poly - RKHS - poly 0.0000 0.0154 SVM rbf - RKHS - poly 0.0200 0.0308 SVM linear - RKHS - linear 0.0000 0.0462 SVM poly - RKHS - linear 0.0000 0.0308 SVM rbf - RKHS - linear 0.0200 0.1231 6.5 Simulation Study In the entire previous section, we dealt with a set of functions from two classification classes, which we then randomly split into testing and training parts. We then evaluated the individual classifiers obtained using the considered methods based on testing and training error rates. Since the division of data into two parts can vary significantly with each repetition, the error rates of the individual classification algorithms will also differ significantly. Therefore, making any conclusions about the methods and comparing them to each other based on a single generated training dataset can be very misleading. For this reason, this section will focus on repeating the entire previous procedure for different splits. We will store the results in a table and finally calculate the average characteristics of the models across the individual repetitions. To ensure our conclusions are sufficiently general, we will choose the number of repetitions \\(n_{sim} = 100\\). Code # Set different names for classification methods methods_names &lt;- c( &#39;$K$ nearest neighbors&#39;, &#39;Linear discriminant analysis&#39;, &#39;Quadratic discriminant analysis&#39;, &#39;Functional logistic regression&#39;, &#39;Logistic regression with fPCA&#39;, &#39;Decision tree -- discretization&#39;, &#39;Decision tree -- fPCA&#39;, &#39;Decision tree -- basis coefficients&#39;, &#39;Random forest -- discretization&#39;, &#39;Random forest -- fPCA&#39;, &#39;Random forest -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # box_col &lt;- c(&#39;#CA0A0A&#39;, &#39;#fa2600&#39;, &#39;#fa2600&#39;, &#39;#D15804&#39;, # &#39;#D15804&#39;, rep(&#39;#D3006D&#39;, 3), rep(&#39;#BE090F&#39;, 3), c(&quot;#12DEE8&quot;, &quot;#4ECBF3&quot;, &quot;#127DE8&quot;, &quot;#4C3CD3&quot;, &quot;#4E65F3&quot;, &quot;#4E9EF3&quot;, &quot;#081D58&quot;) |&gt; rep(each = 3)) # Alpha for box plots box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code # Set different names for classification methods methods_names1 &lt;- c( &#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;functional LR&#39;, &#39;LR with fPCA&#39;, #&#39;Decision Tree -- Discretization&#39;, #&#39;Decision Tree -- fPCA&#39;, #&#39;Decision Tree -- Basis Coefficients&#39;, &#39;RF -- discretization&#39;, &#39;RF -- fPCA&#39;, &#39;RF -- basis coefficients&#39;, &#39;SVM (linear) -- functional&#39;, &#39;SVM (poly) -- functional&#39;, &#39;SVM (radial) -- functional&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;#, #&#39;RKHS (Linear SVR) $+$ SVM (linear)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (poly)&#39;, #&#39;RKHS (Linear SVR) $+$ SVM (radial)&#39; ) # Colors for box plots box_col1 &lt;- c(&#39;#4dd2ff&#39;, &#39;#00ace6&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#00bfff&#39;, rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 3)) # alpha for box plots box_alpha1 &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, rep(c(0.9, 0.8, 0.7), 7)) #- 0.3 6.5.1 Simulation for original Data First, let us look at the simulation of the original, that is, non-derivative data. Code # setting the pseudorandom number generator set.seed(42) # number of simulations n.sim &lt;- 100 ## list to store error values # methods will be in the columns # individual repetitions will be in the rows # the list has two items ... train and test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;, &#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # object to store optimal values of hyperparameters determined using CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Now we will repeat the entire previous part 100 times and store the error values in the list SIMULACE. We will also save the optimal hyperparameter values in the data table CV_RESULTS—for the \\(K\\) nearest neighbors method and for SVM the dimension \\(d\\) in the case of projection onto the B-spline basis. We will also store all hyperparameter values for the SVM + RKHS method. Code # nastaveni generatoru pseudonahodnych cisel set.seed(42) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXfd$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXfd, split == TRUE) X.test &lt;- subset(XXfd, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm, split == TRUE) X.test_norm &lt;- subset(XXfd_norm, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } ### 7.0) SVM for functional data # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-5, 2, length = 5) C.cv &lt;- 10^seq(-2, 5, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (cost in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = nbasis.x) # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEARNI JADRO # model SVM clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gam.cv in gamma.cv) { # sestrojeni modelu clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # vytvorime vhodne objekty x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # presnost na trenovacich datech newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.1) Diskretizace intervalu # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-3, 3, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - discr&#39;, &#39;SVM poly - discr&#39;, &#39;SVM rbf - discr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-3, 2, length = 5) C.cv &lt;- 10^seq(-3, 3, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-3, 3, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = 1, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, coef0 = 1, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = 1, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03neder.RData&#39;) Now we will compute the average test and training error rates for individual classification methods. Code # Add to the final table SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # Save the final values save(SIMULACE.df, file = &#39;RData/aplikace_03neder_res.RData&#39;) 6.5.1.1 Results Table 6.17: Summary of the results of the methods applied to simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error, \\(\\widehat{Err}_{test}\\) denotes testing error, \\(\\widehat{SD}_{train}\\) denotes the estimate of the standard deviation of training errors, and \\(\\widehat{SD}_{test}\\) is the estimate of the standard deviation of testing errors. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.1715 0.1768 0.0271 0.0511 LDA 0.3001 0.3152 0.0222 0.0447 QDA 0.3019 0.3237 0.0229 0.0459 LR_functional 0.0017 0.0486 0.0076 0.0401 LR_score 0.2922 0.3118 0.0223 0.0480 Tree_discr 0.1943 0.2862 0.0495 0.0577 Tree_score 0.2502 0.3448 0.0523 0.0566 Tree_Bbasis 0.1901 0.2843 0.0433 0.0581 RF_discr 0.0131 0.1985 0.0089 0.0479 RF_score 0.0365 0.3128 0.0119 0.0535 RF_Bbasis 0.0127 0.1991 0.0080 0.0491 SVM linear - func 0.0077 0.0269 0.0077 0.0221 SVM poly - func 0.0112 0.0483 0.0130 0.0264 SVM rbf - func 0.0052 0.0315 0.0060 0.0244 SVM linear - discr 0.0041 0.0238 0.0051 0.0196 SVM poly - discr 0.0130 0.0406 0.0111 0.0257 SVM rbf - discr 0.0051 0.0355 0.0080 0.0228 SVM linear - PCA 0.2980 0.3278 0.0232 0.0521 SVM poly - PCA 0.2790 0.3462 0.0391 0.0506 SVM rbf - PCA 0.1665 0.3363 0.1006 0.0470 SVM linear - Bbasis 0.0078 0.0246 0.0081 0.0203 SVM poly - Bbasis 0.0105 0.0434 0.0091 0.0248 SVM rbf - Bbasis 0.0221 0.0486 0.0153 0.0295 SVM linear - projection 0.0321 0.0389 0.0100 0.0252 SVM poly - projection 0.0353 0.0508 0.0130 0.0343 SVM rbf - projection 0.1385 0.1868 0.0293 0.0558 SVM linear - RKHS - radial 0.0009 0.0215 0.0025 0.0167 SVM poly - RKHS - radial 0.0013 0.0151 0.0026 0.0187 SVM rbf - RKHS - radial 0.0027 0.0208 0.0038 0.0158 SVM linear - RKHS - poly 0.0562 0.0805 0.0129 0.0316 SVM poly - RKHS - poly 0.0292 0.0898 0.0138 0.0299 SVM rbf - RKHS - poly 0.0297 0.0674 0.0097 0.0334 SVM linear - RKHS - linear 0.0467 0.0802 0.0143 0.0326 SVM poly - RKHS - linear 0.0403 0.0737 0.0116 0.0291 SVM rbf - RKHS - linear 0.0775 0.1118 0.0188 0.0408 The table above includes all the computed characteristics. It also includes standard deviations to allow comparison of the consistency or variability of the individual methods. We can also formally test whether some methods are more successful than others. Given the violation of the normality assumption, we cannot use the classical paired t-test. We will utilize its non-parametric alternative - the Wilcoxon test. Code wilcox.test(SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - discr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.0007795578 Code wilcox.test(SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - discr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.1468789 Code wilcox.test(SIMULACE$test[, &#39;SVM linear - RKHS - radial&#39;], SIMULACE$test[, &#39;SVM linear - discr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.2632162 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM linear - discr&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 2.025628e-09 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM poly - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 3.134111e-11 Code wilcox.test(SIMULACE$test[, &#39;LR_functional&#39;], SIMULACE$test[, &#39;SVM rbf - RKHS - radial&#39;], alternative = &#39;greater&#39;, paired = T)$p.value ## [1] 6.198434e-10 Finally, we can graphically display the computed values from the simulation for the individual classification methods using box plots, separately for test and training error rates. Code # Rename classification methods methods_names &lt;- c( &#39;$K$ nearest neighbors&#39;, &#39;Linear Discriminant Analysis&#39;, &#39;Quadratic Discriminant Analysis&#39;, &#39;Functional Logistic Regression&#39;, &#39;Logistic Regression with fPCA&#39;, &#39;Decision Tree -- discretization&#39;, &#39;Decision Tree -- fPCA&#39;, &#39;Decision Tree -- basis coefficients&#39;, &#39;Random Forest -- discretization&#39;, &#39;Random Forest -- fPCA&#39;, &#39;Random Forest -- basis coefficients&#39;, &#39;SVM (linear) -- discretization&#39;, &#39;SVM (poly) -- discretization&#39;, &#39;SVM (radial) -- discretization&#39;, &#39;SVM (linear) -- fPCA&#39;, &#39;SVM (poly) -- fPCA&#39;, &#39;SVM (radial) -- fPCA&#39;, &#39;SVM (linear) -- basis coefficients&#39;, &#39;SVM (poly) -- basis coefficients&#39;, &#39;SVM (radial) -- basis coefficients&#39;, &#39;SVM (linear) -- projection&#39;, &#39;SVM (poly) -- projection&#39;, &#39;SVM (radial) -- projection&#39;, &#39;RKHS (radial SVR) $+$ SVM (linear)&#39;, &#39;RKHS (radial SVR) $+$ SVM (poly)&#39;, &#39;RKHS (radial SVR) $+$ SVM (radial)&#39;, &#39;RKHS (poly SVR) $+$ SVM (linear)&#39;, &#39;RKHS (poly SVR) $+$ SVM (poly)&#39;, &#39;RKHS (poly SVR) $+$ SVM (radial)&#39;, &#39;RKHS (linear SVR) $+$ SVM (linear)&#39;, &#39;RKHS (linear SVR) $+$ SVM (poly)&#39;, &#39;RKHS (linear SVR) $+$ SVM (radial)&#39; ) # Colors for boxplots box_col &lt;- c(&#39;#4dd2ff&#39;, &#39;#0099cc&#39;, &#39;#00ace6&#39;, &#39;#00bfff&#39;, &#39;#1ac5ff&#39;, rep(&#39;#33ccff&#39;, 3), rep(&#39;#0086b3&#39;, 3), rep(&#39;#ff3814&#39;, 3), rep(&#39;#ff6347&#39;, 3), rep(&#39;#ff7961&#39;, 3), rep(&#39;#ff4d2e&#39;, 3), rep(&#39;#fa2600&#39;, 9)) # Alpha for boxplots box_alpha &lt;- c(0.9, 0.9, 0.8, 0.9, 0.8, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, 0.9, 0.8, 0.7, seq(0.9, 0.6, length = 9)) #- 0.3 Code sub_methods &lt;- methods[c(1:5, 9:32)] Code # for training data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 60, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Figure 6.18: Box plots of training error rates for 100 simulations separately for each classification method. The black symbols \\(+\\) denote the means. Code # for test data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; filter(method %in% sub_methods) |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_jitter(position = position_jitter(height = 0, width = 0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[test]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 45, hjust = 1)) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = 4, size = 2, color = &quot;black&quot;, alpha = 1) + scale_x_discrete(labels = methods_names1) + theme(plot.margin = unit(c(0.1, 0.1, 0.7, 0.5), &quot;cm&quot;)) + scale_fill_manual(values = box_col1) + scale_alpha_manual(values = box_alpha1) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;gray25&#39;, alpha = 0.8) Figure 6.19: Box plots of test error rates for 100 simulations separately for each classification method. The black symbols \\(+\\) denote the means. Code # ggsave(&quot;figures/results_tec_neder.tex&quot;, device = tikz, width = 7, height = 5) # ggsave(&quot;figures/kap7_tecator_box_test_neder_subset.tex&quot;, device = tikz, width = 6.5, height = 5) Finally, let’s take a look at which hyperparameter values were the most frequently chosen. Table 6.18: Medians of hyperparameter values for selected methods where some hyperparameter was determined using cross-validation. Median Hyperparameter Value KNN_K 1.0 nharm 1.0 LR_func_n_basis 10.0 SVM_d_Linear 6.0 SVM_d_Poly 6.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 3.7 SVM_RKHS_radial_gamma2 1.0 SVM_RKHS_radial_gamma3 1.0 SVM_RKHS_radial_d1 16.0 SVM_RKHS_radial_d2 16.0 SVM_RKHS_radial_d3 20.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 5.0 SVM_RKHS_poly_p3 5.0 SVM_RKHS_poly_d1 8.0 SVM_RKHS_poly_d2 6.0 SVM_RKHS_poly_d3 7.0 SVM_RKHS_linear_d1 17.0 SVM_RKHS_linear_d2 17.0 SVM_RKHS_linear_d3 13.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.20: Histograms of hyperparameter values for KNN, functional logistic regression, and also a histogram for the number of principal components. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.21: Histograms of hyperparameter values for SVM method with projection on B-spline basis. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter Values&#39;, y = &#39;Absolute Count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.22: Histograms of hyperparameter values for RKHS + SVM with radial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.23: Histograms of hyperparameter values for the RKHS + SVM with polynomial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.24: Histograms of hyperparameter values for the RKHS + SVM with linear kernel. 6.5.2 Simulation for derivations Now let’s finally take a look at the simulation of derived data (we determined the second derivative of the curves). Code # Setting the random number generator set.seed(41) # Number of simulations n.sim &lt;- 100 ## List to store error values # Columns will represent methods # Rows will represent individual repetitions # List has two items ... train and test methods &lt;- c(&#39;KNN&#39;, &#39;LDA&#39;, &#39;QDA&#39;, &#39;LR_functional&#39;, &#39;LR_score&#39;, &#39;Tree_discr&#39;, &#39;Tree_score&#39;, &#39;Tree_Bbasis&#39;, &#39;RF_discr&#39;, &#39;RF_score&#39;, &#39;RF_Bbasis&#39;, &#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;, &#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;, &#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;, &#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;, &#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;, &#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;, &#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;, &#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;) SIMULACE &lt;- list(train = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods))), test = as.data.frame(matrix(NA, ncol = length(methods), nrow = n.sim, dimnames = list(1:n.sim, methods)))) # Object to store optimal hyperparameter values determined using CV CV_RESULTS &lt;- data.frame(KNN_K = rep(NA, n.sim), nharm = NA, LR_func_n_basis = NA, SVM_d_Linear = NA, SVM_d_Poly = NA, SVM_d_Radial = NA, SVM_RKHS_radial_gamma1 = NA, SVM_RKHS_radial_gamma2 = NA, SVM_RKHS_radial_gamma3 = NA, SVM_RKHS_radial_d1 = NA, SVM_RKHS_radial_d2 = NA, SVM_RKHS_radial_d3 = NA, SVM_RKHS_poly_p1 = NA, SVM_RKHS_poly_p2 = NA, SVM_RKHS_poly_p3 = NA, SVM_RKHS_poly_d1 = NA, SVM_RKHS_poly_d2 = NA, SVM_RKHS_poly_d3 = NA, SVM_RKHS_linear_d1 = NA, SVM_RKHS_linear_d2 = NA, SVM_RKHS_linear_d3 = NA) Now we will repeat the entire previous section 100 times, storing the error values in the SIMULACE list. We will then store the optimal hyperparameter values in the CV_RESULTS data table—specifically for the \\(K\\)-nearest neighbors method and for SVM, the dimension \\(d\\) in the case of projection onto the B-spline basis. We will also save all hyperparameter values for the SVM + RKHS method. Code # nastaveni generatoru pseudonahodnych cisel set.seed(41) ## SIMULACE for(sim in 1:n.sim) { # rozdeleni na testovaci a trenovaci cast split &lt;- sample.split(XXder$fdnames$reps, SplitRatio = 0.7) # vytvoreni vektoru 0 a 1, 0 pro &lt; 20 a 1 pro &gt; 20 Y &lt;- ifelse(labels == &#39;large&#39;, 1, 0) X.train &lt;- subset(XXder, split == TRUE) X.test &lt;- subset(XXder, split == FALSE) Y.train &lt;- subset(Y, split == TRUE) Y.test &lt;- subset(Y, split == FALSE) x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(factor(Y.train)) ## 1) K nejbližších sousedů k_cv &lt;- 10 # k-fold CV neighbours &lt;- c(1:10) # pocet sousedu # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } CV.results &lt;- matrix(NA, nrow = length(neighbours), ncol = k_cv) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; factor() |&gt; as.numeric() # projdeme kazdou cast ... k-krat zopakujeme for(neighbour in neighbours) { # model pro konkretni volbu K neighb.model &lt;- classif.knn(group = y.train.cv, fdataobj = x.train.cv, knn = neighbour) # predikce na validacni casti model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = x.test.cv) # presnost na validacni casti presnost &lt;- table(y.test.cv, model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost vlozime na pozici pro dane K a fold CV.results[neighbour, index] &lt;- presnost } } # spocitame prumerne presnosti pro jednotliva K pres folds CV.results &lt;- apply(CV.results, 1, mean) K.opt &lt;- which.max(CV.results) CV_RESULTS$KNN_K[sim] &lt;- K.opt presnost.opt.cv &lt;- max(CV.results) CV.results &lt;- data.frame(K = neighbours, CV = CV.results) neighb.model &lt;- classif.knn(group = y.train, fdataobj = x.train, knn = K.opt) # predikce model.neighb.predict &lt;- predict(neighb.model, new.fdataobj = fdata(X.test)) presnost &lt;- table(as.numeric(factor(Y.test)), model.neighb.predict) |&gt; prop.table() |&gt; diag() |&gt; sum() RESULTS &lt;- data.frame(model = &#39;KNN&#39;, Err.train = 1 - neighb.model$max.prob, Err.test = 1 - presnost) ## 2) Lineární diskriminační analýza # analyza hlavnich komponent data.PCA &lt;- pca.fd(X.train, nharm = 10) # nharm - maximalni pocet HK nharm &lt;- which(cumsum(data.PCA$varprop) &gt;= 0.9)[1] # urceni p CV_RESULTS$nharm[sim] &lt;- nharm if(nharm == 1) nharm &lt;- 2 data.PCA &lt;- pca.fd(X.train, nharm = nharm) data.PCA.train &lt;- as.data.frame(data.PCA$scores) # skore prvnich p HK data.PCA.train$Y &lt;- factor(Y.train) # prislusnost do trid # vypocet skoru testovacich funkci scores &lt;- matrix(NA, ncol = nharm, nrow = length(Y.test)) # prazdna matice for(k in 1:dim(scores)[1]) { xfd = X.test[k] - data.PCA$meanfd[1] # k-te pozorovani - prumerna funkce scores[k, ] = inprod(xfd, data.PCA$harmonics) # skalarni soucin rezidua a vlastnich funkci rho (funkcionalni hlavni komponenty) } data.PCA.test &lt;- as.data.frame(scores) data.PCA.test$Y &lt;- factor(Y.test) colnames(data.PCA.test) &lt;- colnames(data.PCA.train) # model clf.LDA &lt;- lda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 3) Kvadratická diskriminační analýza # model clf.QDA &lt;- qda(Y ~ ., data = data.PCA.train) # presnost na trenovacich datech predictions.train &lt;- predict(clf.QDA, newdata = data.PCA.train) presnost.train &lt;- table(data.PCA.train$Y, predictions.train$class) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.QDA, newdata = data.PCA.test) presnost.test &lt;- table(data.PCA.test$Y, predictions.test$class) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;QDA&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 4) Logistická regrese ### 4.1) Funkcionální logistická regrese # vytvorime vhodne objekty x.train &lt;- fdata(X.train) y.train &lt;- as.numeric(Y.train) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze nbasis.x &lt;- 100 basis1 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = nbasis.x) ### 10-fold cross-validation n.basis.max &lt;- 25 n.basis &lt;- 4:n.basis.max k_cv &lt;- 10 # k-fold CV # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(X.train$fdnames$reps, k = k_cv, time = 1) } ## prvky, ktere se behem cyklu nemeni # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] rangeval &lt;- range(tt) # vztah f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) CV.results &lt;- matrix(NA, nrow = length(n.basis), ncol = k_cv, dimnames = list(n.basis, 1:k_cv)) for (index in 1:k_cv) { # definujeme danou indexovou mnozinu fold &lt;- folds[[index]] x.train.cv &lt;- subset(X.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.train.cv &lt;- subset(Y.train, c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() x.test.cv &lt;- subset(X.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; fdata() y.test.cv &lt;- subset(Y.train, !c(1:length(X.train$fdnames$reps)) %in% fold) |&gt; as.numeric() dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; for (i in n.basis) { # baze pro bety basis2 &lt;- create.bspline.basis(rangeval = rangeval, nbasis = i) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na validacni casti newldata = list(&quot;df&quot; = as.data.frame(y.test.cv), &quot;x&quot; = x.test.cv) predictions.valid &lt;- predict(model.glm, newx = newldata) predictions.valid &lt;- data.frame(Y.pred = ifelse(predictions.valid &lt; 1/2, 0, 1)) presnost.valid &lt;- table(y.test.cv, predictions.valid$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # vlozime do matice CV.results[as.character(i), as.character(index)] &lt;- presnost.valid } } # spocitame prumerne presnosti pro jednotliva n pres folds CV.results &lt;- apply(CV.results, 1, mean) n.basis.opt &lt;- n.basis[which.max(CV.results)] CV_RESULTS$LR_func_n_basis[sim] &lt;- n.basis.opt presnost.opt.cv &lt;- max(CV.results) # optimalni model basis2 &lt;- create.bspline.basis(rangeval = range(tt), nbasis = n.basis.opt) f &lt;- Y ~ x # baze pro x a bety basis.x &lt;- list(&quot;x&quot; = basis1) basis.b &lt;- list(&quot;x&quot; = basis2) # vstupni data do modelu dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) # binomicky model ... model logisticke regrese model.glm &lt;- fregre.glm(f, family = binomial(), data = ldata, basis.x = basis.x, basis.b = basis.b) # presnost na trenovacich datech predictions.train &lt;- predict(model.glm, newx = ldata) predictions.train &lt;- data.frame(Y.pred = ifelse(predictions.train &lt; 1/2, 0, 1)) presnost.train &lt;- table(Y.train, predictions.train$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech newldata = list(&quot;df&quot; = as.data.frame(Y.test), &quot;x&quot; = fdata(X.test)) predictions.test &lt;- predict(model.glm, newx = newldata) predictions.test &lt;- data.frame(Y.pred = ifelse(predictions.test &lt; 1/2, 0, 1)) presnost.test &lt;- table(Y.test, predictions.test$Y.pred) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_functional&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 4.2) Logistická regrese s analýzou hlavních komponent # model clf.LR &lt;- glm(Y ~ ., data = data.PCA.train, family = binomial) # presnost na trenovacich datech predictions.train &lt;- predict(clf.LR, newdata = data.PCA.train, type = &#39;response&#39;) predictions.train &lt;- ifelse(predictions.train &gt; 0.5, 1, 0) presnost.train &lt;- table(data.PCA.train$Y, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.LR, newdata = data.PCA.test, type = &#39;response&#39;) predictions.test &lt;- ifelse(predictions.test &gt; 0.5, 1, 0) presnost.test &lt;- table(data.PCA.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;LR_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 5) Rozhodovací stromy ### 5.1) Diskretizace intervalu # posloupnost bodu, ve kterych funkce vyhodnotime t.seq &lt;- seq(min(t), max(t), length = 101) grid.data &lt;- eval.fd(fdobj = X.train, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) # transpozice kvuli funkcim v radku grid.data$Y &lt;- Y.train |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test |&gt; factor() # sestrojeni modelu clf.tree &lt;- train(Y ~ ., data = grid.data, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.2) Skóre hlavních komponent # sestrojeni modelu clf.tree.PCA &lt;- train(Y ~ ., data = data.PCA.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 5.3) Bázové koeficienty # trenovaci dataset data.Bbasis.train &lt;- t(X.train$coefs) |&gt; as.data.frame() data.Bbasis.train$Y &lt;- factor(Y.train) # testovaci dataset data.Bbasis.test &lt;- t(X.test$coefs) |&gt; as.data.frame() data.Bbasis.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.tree.Bbasis &lt;- train(Y ~ ., data = data.Bbasis.train, method = &quot;rpart&quot;, trControl = trainControl(method = &quot;CV&quot;, number = 10), metric = &quot;Accuracy&quot;) # presnost na trenovacich datech predictions.train &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.tree.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;Tree_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 6) Náhodné lesy ### 6.1) Diskretizace intervalu # sestrojeni modelu clf.RF &lt;- randomForest(Y ~ ., data = grid.data, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF, newdata = grid.data) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF, newdata = grid.data.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_discr&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.2) Skóre hlavních komponent # sestrojeni modelu clf.RF.PCA &lt;- randomForest(Y ~ ., data = data.PCA.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.PCA, newdata = data.PCA.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.PCA, newdata = data.PCA.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_score&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ### 6.3) Bázové koeficienty # sestrojeni modelu clf.RF.Bbasis &lt;- randomForest(Y ~ ., data = data.Bbasis.train, ntree = 500, # pocet stromu importance = TRUE, nodesize = 5) # presnost na trenovacich datech predictions.train &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.RF.Bbasis, newdata = data.Bbasis.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = &#39;RF_Bbasis&#39;, Err.train = 1 - presnost.train, Err.test = 1 - presnost.test) RESULTS &lt;- rbind(RESULTS, Res) ## 7) SVM # rozdeleni na testovaci a trenovaci cast X.train_norm &lt;- subset(XXfd_norm_der, split == TRUE) X.test_norm &lt;- subset(XXfd_norm_der, split == FALSE) Y.train_norm &lt;- subset(Y, split == TRUE) Y.test_norm &lt;- subset(Y, split == FALSE) grid.data &lt;- eval.fd(fdobj = X.train_norm, evalarg = t.seq) grid.data &lt;- as.data.frame(t(grid.data)) grid.data$Y &lt;- Y.train_norm |&gt; factor() grid.data.test &lt;- eval.fd(fdobj = X.test_norm, evalarg = t.seq) grid.data.test &lt;- as.data.frame(t(grid.data.test)) grid.data.test$Y &lt;- Y.test_norm |&gt; factor() # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # kontrola, ze mame opravdu k = k_cv while (length(folds) != k_cv) { folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) } ### 7.0) SVM for functional data # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-5, 0, length = 5) C.cv &lt;- 10^seq(-2, 4, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (cost in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(X.train_norm$coefs)[2] %in% fold x.train.cv &lt;- fdata(subset(X.train_norm, cv_sample)) x.test.cv &lt;- fdata(subset(X.train_norm, !cv_sample)) y.train.cv &lt;- as.factor(subset(Y.train_norm, cv_sample)) y.test.cv &lt;- as.factor(subset(Y.train_norm, !cv_sample)) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train.cv[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train.cv) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze basis1 &lt;- create.bspline.basis(rangeval = range(tt), norder = 4, nbasis = 100) # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train.cv) ## LINEARNI JADRO # model SVM clf.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, cost = cost, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == cost], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, cost = cost, coef0 = coef0, degree = p, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == cost], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gam.cv in gamma.cv) { # sestrojeni modelu clf.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, cost = cost, gamma = gam.cv, type = &#39;C-classification&#39;, scale = TRUE) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = x.test.cv) predictions.test &lt;- predict(clf.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(y.test.cv == predictions.test) # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == cost], (1:length(gamma.cv))[gamma.cv == gam.cv], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # vytvorime vhodne objekty x.train &lt;- fdata(X.train_norm) y.train &lt;- as.factor(Y.train_norm) # body, ve kterych jsou funkce vyhodnoceny tt &lt;- x.train[[&quot;argvals&quot;]] dataf &lt;- as.data.frame(y.train) colnames(dataf) &lt;- &quot;Y&quot; # B-spline baze # basis1 &lt;- X.train_norm$basis # formula f &lt;- Y ~ x # baze pro x basis.x &lt;- list(&quot;x&quot; = basis1) # vstupni data do modelu ldata &lt;- list(&quot;df&quot; = dataf, &quot;x&quot; = x.train) model.svm.f_l &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;linear&#39;, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1]) model.svm.f_p &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;polynomial&#39;, type = &#39;C-classification&#39;, scale = TRUE, degree = p.opt, coef0 = coef0, cost = C.opt[2]) model.svm.f_r &lt;- classif.svm(formula = f, data = ldata, basis.x = basis.x, kernel = &#39;radial&#39;, type = &#39;C-classification&#39;, scale = TRUE, gamma = gamma.opt, cost = C.opt[3]) # presnost na trenovacich datech newdat &lt;- list(&quot;x&quot; = x.train) predictions.train.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.train.l &lt;- mean(factor(Y.train_norm) == predictions.train.l) predictions.train.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.train.p &lt;- mean(factor(Y.train_norm) == predictions.train.p) predictions.train.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.train.r &lt;- mean(factor(Y.train_norm) == predictions.train.r) # presnost na testovacich datech newdat &lt;- list(&quot;x&quot; = fdata(X.test_norm)) predictions.test.l &lt;- predict(model.svm.f_l, newdat, type = &#39;class&#39;) presnost.test.l &lt;- mean(factor(Y.test_norm) == predictions.test.l) predictions.test.p &lt;- predict(model.svm.f_p, newdat, type = &#39;class&#39;) presnost.test.p &lt;- mean(factor(Y.test_norm) == predictions.test.p) predictions.test.r &lt;- predict(model.svm.f_r, newdat, type = &#39;class&#39;) presnost.test.r &lt;- mean(factor(Y.test_norm) == predictions.test.r) Res &lt;- data.frame(model = c(&#39;SVM linear - func&#39;, &#39;SVM poly - func&#39;, &#39;SVM rbf - func&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.1) Diskretizace intervalu # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-4, -1, length = 5) C.cv &lt;- 10^seq(-4, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.grid.train.cv &lt;- as.data.frame(grid.data[cv_sample, ]) data.grid.test.cv &lt;- as.data.frame(grid.data[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.grid.test.cv) presnost.test.l &lt;- table(data.grid.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.grid.test.cv) presnost.test.p &lt;- table(data.grid.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.grid.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.grid.test.cv) presnost.test.r &lt;- table(data.grid.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r &lt;- svm(Y ~ ., data = grid.data, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l, newdata = grid.data) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p, newdata = grid.data) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r, newdata = grid.data) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = grid.data.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p, newdata = grid.data.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r, newdata = grid.data.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - diskr&#39;, &#39;SVM poly - diskr&#39;, &#39;SVM rbf - diskr&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.2) Skóre hlavních komponent # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-2, 2, length = 5) C.cv &lt;- 10^seq(-2, 5, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(data.PCA.train)[1] %in% fold data.PCA.train.cv &lt;- as.data.frame(data.PCA.train[cv_sample, ]) data.PCA.test.cv &lt;- as.data.frame(data.PCA.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.PCA.test.cv) presnost.test.l &lt;- table(data.PCA.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.PCA.test.cv) presnost.test.p &lt;- table(data.PCA.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.PCA.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.PCA.test.cv) presnost.test.r &lt;- table(data.PCA.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], coef0 = coef0, degree = p.opt, kernel = &#39;polynomial&#39;) clf.SVM.r.PCA &lt;- svm(Y ~ ., data = data.PCA.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.train) presnost.train.l &lt;- table(data.PCA.train$Y, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.train) presnost.train.p &lt;- table(data.PCA.train$Y, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.train) presnost.train.r &lt;- table(data.PCA.train$Y, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.PCA, newdata = data.PCA.test) presnost.test.l &lt;- table(data.PCA.test$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.PCA, newdata = data.PCA.test) presnost.test.p &lt;- table(data.PCA.test$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.PCA, newdata = data.PCA.test) presnost.test.r &lt;- table(data.PCA.test$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - PCA&#39;, &#39;SVM poly - PCA&#39;, &#39;SVM rbf - PCA&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.3) Bázové koeficienty # ktere hodnoty gamma chceme uvazovat gamma.cv &lt;- 10^seq(-4, 0, length = 5) C.cv &lt;- 10^seq(-4, 2, length = 5) p.cv &lt;- 3 coef0 &lt;- 1 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds CV.results &lt;- list( SVM.l = array(NA, dim = c(length(C.cv), k_cv)), SVM.p = array(NA, dim = c(length(C.cv), length(p.cv), k_cv)), SVM.r = array(NA, dim = c(length(C.cv), length(gamma.cv), k_cv)) ) # nejprve projdeme hodnoty C for (C in C.cv) { # projdeme jednotlive folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(grid.data)[1] %in% fold data.Bbasis.train.cv &lt;- as.data.frame(data.Bbasis.train[cv_sample, ]) data.Bbasis.test.cv &lt;- as.data.frame(data.Bbasis.train[!cv_sample, ]) ## LINEARNI JADRO # sestrojeni modelu clf.SVM.l &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, kernel = &#39;linear&#39;) # presnost na validacnich datech predictions.test.l &lt;- predict(clf.SVM.l, newdata = data.Bbasis.test.cv) presnost.test.l &lt;- table(data.Bbasis.test.cv$Y, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C a fold CV.results$SVM.l[(1:length(C.cv))[C.cv == C], index_cv] &lt;- presnost.test.l ## POLYNOMIALNI JADRO for (p in p.cv) { # sestrojeni modelu clf.SVM.p &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, degree = p, kernel = &#39;polynomial&#39;) # presnost na validacnich datech predictions.test.p &lt;- predict(clf.SVM.p, newdata = data.Bbasis.test.cv) presnost.test.p &lt;- table(data.Bbasis.test.cv$Y, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, p a fold CV.results$SVM.p[(1:length(C.cv))[C.cv == C], (1:length(p.cv))[p.cv == p], index_cv] &lt;- presnost.test.p } ## RADIALNI JADRO for (gamma in gamma.cv) { # sestrojeni modelu clf.SVM.r &lt;- svm(Y ~ ., data = data.Bbasis.train.cv, type = &#39;C-classification&#39;, scale = TRUE, cost = C, gamma = gamma, kernel = &#39;radial&#39;) # presnost na validacnich datech predictions.test.r &lt;- predict(clf.SVM.r, newdata = data.Bbasis.test.cv) presnost.test.r &lt;- table(data.Bbasis.test.cv$Y, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane C, gamma a fold CV.results$SVM.r[(1:length(C.cv))[C.cv == C], (1:length(gamma.cv))[gamma.cv == gamma], index_cv] &lt;- presnost.test.r } } } # spocitame prumerne presnosti pro jednotliva C pres folds ## Linearni jadro CV.results$SVM.l &lt;- apply(CV.results$SVM.l, 1, mean) ## Polynomialni jadro CV.results$SVM.p &lt;- apply(CV.results$SVM.p, c(1, 2), mean) ## Radialni jadro CV.results$SVM.r &lt;- apply(CV.results$SVM.r, c(1, 2), mean) C.opt &lt;- c(which.max(CV.results$SVM.l), which.max(CV.results$SVM.p) %% length(C.cv), which.max(CV.results$SVM.r) %% length(C.cv)) C.opt[C.opt == 0] &lt;- length(C.cv) C.opt &lt;- C.cv[C.opt] gamma.opt &lt;- which.max(t(CV.results$SVM.r)) %% length(gamma.cv) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] p.opt &lt;- which.max(t(CV.results$SVM.p)) %% length(p.cv) p.opt[p.opt == 0] &lt;- length(p.cv) p.opt &lt;- p.cv[p.opt] presnost.opt.cv &lt;- c(max(CV.results$SVM.l), max(CV.results$SVM.p), max(CV.results$SVM.r)) # sestrojeni modelu clf.SVM.l.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[1], kernel = &#39;linear&#39;) clf.SVM.p.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[2], degree = p.opt, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.Bbasis &lt;- svm(Y ~ ., data = data.Bbasis.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C.opt[3], gamma = gamma.opt, kernel = &#39;radial&#39;) # presnost na trenovacich datech predictions.train.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.train) presnost.train.l &lt;- table(Y.train, predictions.train.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.train) presnost.train.p &lt;- table(Y.train, predictions.train.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.train.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.train) presnost.train.r &lt;- table(Y.train, predictions.train.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test.l &lt;- predict(clf.SVM.l.Bbasis, newdata = data.Bbasis.test) presnost.test.l &lt;- table(Y.test, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.p &lt;- predict(clf.SVM.p.Bbasis, newdata = data.Bbasis.test) presnost.test.p &lt;- table(Y.test, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() predictions.test.r &lt;- predict(clf.SVM.r.Bbasis, newdata = data.Bbasis.test) presnost.test.r &lt;- table(Y.test, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() Res &lt;- data.frame(model = c(&#39;SVM linear - Bbasis&#39;, &#39;SVM poly - Bbasis&#39;, &#39;SVM rbf - Bbasis&#39;), Err.train = 1 - c(presnost.train.l, presnost.train.p, presnost.train.r), Err.test = 1 - c(presnost.test.l, presnost.test.p, presnost.test.r)) RESULTS &lt;- rbind(RESULTS, Res) ### 7.4) Projekce na B-splinovou bázi # hodnoty pro B-splinovou bazi rangeval &lt;- range(t) norder &lt;- 4 n_basis_min &lt;- norder n_basis_max &lt;- 20 dimensions &lt;- n_basis_min:n_basis_max folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) CV.results &lt;- list(SVM.l = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.p = matrix(NA, nrow = length(dimensions), ncol = k_cv), SVM.r = matrix(NA, nrow = length(dimensions), ncol = k_cv)) for (d in dimensions) { bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) for (index_cv in 1:k_cv) { fold &lt;- folds[[index_cv]] cv_sample &lt;- 1:dim(XX.train)[1] %in% fold data.projection.train.cv &lt;- as.data.frame(XX.train[cv_sample, ]) data.projection.train.cv$Y &lt;- factor(Y.train[cv_sample]) data.projection.test.cv &lt;- as.data.frame(XX.train[!cv_sample, ]) Y.test.cv &lt;- Y.train[!cv_sample] data.projection.test.cv$Y &lt;- factor(Y.test.cv) # sestrojeni modelu clf.SVM.l.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;linear&#39;) clf.SVM.p.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = &#39;polynomial&#39;) clf.SVM.r.projection &lt;- svm(Y ~ ., data = data.projection.train.cv, type = &#39;C-classification&#39;, scale = TRUE, kernel = &#39;radial&#39;) # presnost na validacnich datech ## linear kernel predictions.test.l &lt;- predict(clf.SVM.l.projection, newdata = data.projection.test.cv) presnost.test.l &lt;- table(Y.test.cv, predictions.test.l) |&gt; prop.table() |&gt; diag() |&gt; sum() ## polynomial kernel predictions.test.p &lt;- predict(clf.SVM.p.projection, newdata = data.projection.test.cv) presnost.test.p &lt;- table(Y.test.cv, predictions.test.p) |&gt; prop.table() |&gt; diag() |&gt; sum() ## radial kernel predictions.test.r &lt;- predict(clf.SVM.r.projection, newdata = data.projection.test.cv) presnost.test.r &lt;- table(Y.test.cv, predictions.test.r) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnosti vlozime na pozice pro dane d a fold CV.results$SVM.l[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.l CV.results$SVM.p[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.p CV.results$SVM.r[d - min(dimensions) + 1, index_cv] &lt;- presnost.test.r } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.max(CV.results$SVM.l) + n_basis_min - 1, which.max(CV.results$SVM.p) + n_basis_min - 1, which.max(CV.results$SVM.r) + n_basis_min - 1) # ulozime optimalni d do datove tabulky CV_RESULTS[sim, 4:6] &lt;- d.opt # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - projection&#39;, &#39;SVM poly - projection&#39;, &#39;SVM rbf - projection&#39;), Err.train = NA, Err.test = NA) for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] bbasis &lt;- create.bspline.basis(rangeval = rangeval, nbasis = d.opt[kernel_number]) Projection &lt;- project.basis(y = XX, argvals = t, basisobj = bbasis) XX.train &lt;- subset(t(Projection), split == TRUE) XX.test &lt;- subset(t(Projection), split == FALSE) data.projection.train &lt;- as.data.frame(XX.train) data.projection.train$Y &lt;- factor(Y.train) data.projection.test &lt;- as.data.frame(XX.test) data.projection.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.projection &lt;- svm(Y ~ ., data = data.projection.train, type = &#39;C-classification&#39;, scale = TRUE, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.projection, newdata = data.projection.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na trenovacich datech predictions.test &lt;- predict(clf.SVM.projection, newdata = data.projection.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## 7.5) SVM + RKHS C &lt;- 1 eps &lt;- 0.01 ### Gaussovo jadro # jadro a jadrova matice ... Gaussovske s parametrem gamma Gauss.kernel &lt;- function(x, y, gamma) { return(exp(-gamma * norm(c(x - y) |&gt; t(), type = &#39;F&#39;))) } Kernel.RKHS &lt;- function(x, gamma) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Gauss.kernel(x = x[i], y = x[j], gamma = gamma) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 30, by = 2) # rozumny rozsah hodnot d gamma.cv &lt;- 10^seq(-2, 2, length = 15) # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro danou gamma a vrstvy odpovidaji folds dim.names &lt;- list(gamma = paste0(&#39;gamma:&#39;, round(gamma.cv, 3)), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(gamma.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (gamma in gamma.cv) { K &lt;- Kernel.RKHS(t.seq, gamma = gamma) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;gamma:&#39;, round(gamma, 3)), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } gamma.opt &lt;- c(which.min(CV.results$SVM.l) %% length(gamma.cv), which.min(CV.results$SVM.p) %% length(gamma.cv), which.min(CV.results$SVM.r) %% length(gamma.cv)) gamma.opt[gamma.opt == 0] &lt;- length(gamma.cv) gamma.opt &lt;- gamma.cv[gamma.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, gamma = gamma.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 7:9] &lt;- gamma.opt CV_RESULTS[sim, 10:12] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - radial&#39;, &#39;SVM poly - RKHS - radial&#39;, &#39;SVM rbf - RKHS - radial&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K gamma &lt;- gamma.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, gamma = gamma) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;radial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, cost = C, gamma = gamma) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Polynomialni jadro # jadro a jadrova matice ... polynomialni s parametrem p Poly.kernel &lt;- function(x, y, p) { return((1 + x * y)^p) } Kernel.RKHS &lt;- function(x, p) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Poly.kernel(x = x[i], y = x[j], p) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(2, 10, by = 1) # rozumny rozsah hodnot d poly.cv &lt;- 2:5 # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane # v radcich budou hodnoty pro dane p a vrstvy odpovidaji folds dim.names &lt;- list(p = paste0(&#39;p:&#39;, poly.cv), d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(poly.cv), length(dimensions), k_cv), dimnames = dim.names)) # samotna CV for (p in poly.cv) { K &lt;- Kernel.RKHS(t.seq, p = p) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;p:&#39;, p), paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], c(1, 2), mean) } poly.opt &lt;- c(which.min(CV.results$SVM.l) %% length(poly.cv), which.min(CV.results$SVM.p) %% length(poly.cv), which.min(CV.results$SVM.r) %% length(poly.cv)) poly.opt[poly.opt == 0] &lt;- length(poly.cv) poly.opt &lt;- poly.cv[poly.opt] d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, p = poly.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 13:15] &lt;- poly.opt CV_RESULTS[sim, 16:18] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - poly&#39;, &#39;SVM poly - RKHS - poly&#39;, &#39;SVM rbf - RKHS - poly&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K p &lt;- poly.opt[kernel_number] # hodnota gamma pomoci CV K &lt;- Kernel.RKHS(t.seq, p = p) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;polynomial&#39;, type = &#39;eps-regression&#39;, epsilon = eps, coef0 = 1, gamma = 1, cost = C, degree = p) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, epsilon = eps, coef0 = 1, gamma = 1, cost = C, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ### Linearni jadro # jadro a jadrova matice ... polynomialni s parametrem p Linear.kernel &lt;- function(x, y) { return(x * y) } Kernel.RKHS &lt;- function(x) { K &lt;- matrix(NA, ncol = length(x), nrow = length(x)) for(i in 1:nrow(K)) { for(j in 1:ncol(K)) { K[i, j] &lt;- Linear.kernel(x = x[i], y = x[j]) } } return(K) } # rozdelime trenovaci data na k casti folds &lt;- createMultiFolds(1:sum(split), k = k_cv, time = 1) # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # hodnoty hyperparametru, ktere budeme prochazet dimensions &lt;- seq(3, 40, by = 2) # rozumny rozsah hodnot d # list se tremi slozkami ... array pro jednotlive jadra -&gt; linear, poly, radial # prazdna matice, do ktere vlozime jednotlive vysledky # ve sloupcich budou hodnoty presnosti pro dane d # v radcich budou hodnoty pro vrstvy odpovidaji folds dim.names &lt;- list(d = paste0(&#39;d:&#39;, dimensions), CV = paste0(&#39;cv:&#39;, 1:k_cv)) CV.results &lt;- list( SVM.l = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.p = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names), SVM.r = array(NA, dim = c(length(dimensions), k_cv), dimnames = dim.names)) # samotna CV K &lt;- Kernel.RKHS(t.seq) Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs } # projdeme dimenze for(d.RKHS in dimensions) { Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # projdeme folds for (index_cv in 1:k_cv) { # definice testovaci a trenovaci casti pro CV fold &lt;- folds[[index_cv]] # rozdeleni na trenovaci a validacni data XX.train &lt;- Lambda.RKHS[, fold] XX.test &lt;- Lambda.RKHS[, !(1:dim(Lambda.RKHS)[2] %in% fold)] # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS&#39;, &#39;SVM poly - RKHS&#39;, &#39;SVM rbf - RKHS&#39;), Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train[fold]) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.train[!(1:dim(Lambda.RKHS)[2] %in% fold)]) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na validacnich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(data.RKHS.test$Y, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, 2] &lt;- 1 - presnost.test } # presnosti vlozime na pozice pro dane d, gamma a fold CV.results$SVM.l[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[1, 2] CV.results$SVM.p[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[2, 2] CV.results$SVM.r[paste0(&#39;d:&#39;, d.RKHS), index_cv] &lt;- Res[3, 2] } } # spocitame prumerne presnosti pro jednotliva d pres folds for (n_method in 1:length(CV.results)) { CV.results[[n_method]] &lt;- apply(CV.results[[n_method]], 1, mean) } d.opt &lt;- c(which.min(t(CV.results$SVM.l)) %% length(dimensions), which.min(t(CV.results$SVM.p)) %% length(dimensions), which.min(t(CV.results$SVM.r)) %% length(dimensions)) d.opt[d.opt == 0] &lt;- length(dimensions) d.opt &lt;- dimensions[d.opt] err.opt.cv &lt;- c(min(CV.results$SVM.l), min(CV.results$SVM.p), min(CV.results$SVM.r)) df.RKHS.res &lt;- data.frame(d = d.opt, CV = err.opt.cv, Kernel = c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;) |&gt; factor(), row.names = c(&#39;linear&#39;, &#39;poly&#39;, &#39;radial&#39;)) CV_RESULTS[sim, 19:21] &lt;- d.opt # odstranime posledni sloupec, ve kterem jsou hodnoty Y data.RKHS &lt;- grid.data[, -dim(grid.data)[2]] |&gt; t() # pridame i testovaci data data.RKHS &lt;- cbind(data.RKHS, grid.data.test[, -dim(grid.data.test)[2]] |&gt; t()) # pripravime si datovou tabulku pro ulozeni vysledku Res &lt;- data.frame(model = c(&#39;SVM linear - RKHS - linear&#39;, &#39;SVM poly - RKHS - linear&#39;, &#39;SVM rbf - RKHS - linear&#39;), Err.train = NA, Err.test = NA) # projdeme jednotliva jadra for (kernel_number in 1:3) { # spocitame matici K K &lt;- Kernel.RKHS(t.seq) # urcime vlastni cisla a vektory Eig &lt;- eigen(K) eig.vals &lt;- Eig$values eig.vectors &lt;- Eig$vectors # urceni koeficientu alpha z SVM alpha.RKHS &lt;- matrix(0, nrow = dim(data.RKHS)[1], ncol = dim(data.RKHS)[2]) # prazdny objekt # model for(i in 1:dim(data.RKHS)[2]) { df.svm &lt;- data.frame(x = t.seq, y = data.RKHS[, i]) svm.RKHS &lt;- svm(y ~ x, data = df.svm, kernel = &#39;linear&#39;, type = &#39;eps-regression&#39;, cost = C, epsilon = eps) # urceni alpha alpha.RKHS[svm.RKHS$index, i] &lt;- svm.RKHS$coefs # nahrazeni nul koeficienty } # d d.RKHS &lt;- d.opt[kernel_number] # urceni vektoru lambda Lambda.RKHS &lt;- matrix(NA, ncol = dim(data.RKHS)[2], nrow = d.RKHS) # vytvoreni prazdneho objektu # vypocet reprezentace for(l in 1:dim(data.RKHS)[2]) { Lambda.RKHS[, l] &lt;- (t(eig.vectors[, 1:d.RKHS]) %*% alpha.RKHS[, l]) * eig.vals[1:d.RKHS] } # rozdeleni na trenovaci a testovaci data XX.train &lt;- Lambda.RKHS[, 1:dim(grid.data)[1]] XX.test &lt;- Lambda.RKHS[, (dim(grid.data)[1] + 1):dim(Lambda.RKHS)[2]] kernel_type &lt;- c(&#39;linear&#39;, &#39;polynomial&#39;, &#39;radial&#39;)[kernel_number] data.RKHS.train &lt;- as.data.frame(t(XX.train)) data.RKHS.train$Y &lt;- factor(Y.train) data.RKHS.test &lt;- as.data.frame(t(XX.test)) data.RKHS.test$Y &lt;- factor(Y.test) # sestrojeni modelu clf.SVM.RKHS &lt;- svm(Y ~ ., data = data.RKHS.train, type = &#39;C-classification&#39;, scale = TRUE, cost = C, coef0 = coef0, kernel = kernel_type) # presnost na trenovacich datech predictions.train &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.train) presnost.train &lt;- table(Y.train, predictions.train) |&gt; prop.table() |&gt; diag() |&gt; sum() # presnost na testovacich datech predictions.test &lt;- predict(clf.SVM.RKHS, newdata = data.RKHS.test) presnost.test &lt;- table(Y.test, predictions.test) |&gt; prop.table() |&gt; diag() |&gt; sum() # ulozeni vysledku Res[kernel_number, c(2, 3)] &lt;- 1 - c(presnost.train, presnost.test) } RESULTS &lt;- rbind(RESULTS, Res) ## pridame vysledky do objektu SIMULACE SIMULACE$train[sim, ] &lt;- RESULTS$Err.train SIMULACE$test[sim, ] &lt;- RESULTS$Err.test cat(&#39;\\r&#39;, sim) } # ulozime vysledne hodnoty save(SIMULACE, CV_RESULTS, file = &#39;RData/aplikace_03der.RData&#39;) Now we will calculate the average training and testing errors for the individual classification methods. Code # Insert into the final table SIMULACE.df &lt;- data.frame(Err.train = apply(SIMULACE$train, 2, mean), Err.test = apply(SIMULACE$test, 2, mean), SD.train = apply(SIMULACE$train, 2, sd), SD.test = apply(SIMULACE$test, 2, sd)) # Save the resulting values save(SIMULACE.df, file = &#39;RData/aplikace_03der_res.RData&#39;) 6.5.2.1 Results Table 6.19: Summary of the results of the methods used on simulated data. \\(\\widehat{Err}_{train}\\) denotes the estimate of training error, \\(\\widehat{Err}_{test}\\) the testing error, \\(\\widehat{SD}_{train}\\) the estimate of the standard deviation of training errors, and \\(\\widehat{SD}_{test}\\) the estimate of the standard deviation of testing errors. \\(\\widehat{Err}_{train}\\) \\(\\widehat{Err}_{test}\\) \\(\\widehat{SD}_{train}\\) \\(\\widehat{SD}_{test}\\) KNN 0.0153 0.0195 0.0067 0.0176 LDA 0.0575 0.0592 0.0107 0.0285 QDA 0.0115 0.0160 0.0062 0.0151 LR_functional 0.0001 0.0469 0.0007 0.0379 LR_score 0.0082 0.0166 0.0063 0.0143 Tree_discr 0.0109 0.0294 0.0454 0.0608 Tree_score 0.0169 0.0245 0.0065 0.0184 Tree_Bbasis 0.0107 0.0288 0.0455 0.0593 RF_discr 0.0003 0.0097 0.0015 0.0104 RF_score 0.0053 0.0166 0.0036 0.0159 RF_Bbasis 0.0002 0.0085 0.0011 0.0086 SVM linear - func 0.0011 0.0092 0.0027 0.0116 SVM poly - func 0.0019 0.0054 0.0032 0.0110 SVM rbf - func 0.0009 0.0069 0.0035 0.0110 SVM linear - diskr 0.0021 0.0117 0.0046 0.0140 SVM poly - diskr 0.0007 0.0126 0.0032 0.0157 SVM rbf - diskr 0.0012 0.0115 0.0036 0.0139 SVM linear - PCA 0.0101 0.0195 0.0065 0.0160 SVM poly - PCA 0.0086 0.0231 0.0062 0.0160 SVM rbf - PCA 0.0067 0.0217 0.0054 0.0196 SVM linear - Bbasis 0.0059 0.0254 0.0093 0.0191 SVM poly - Bbasis 0.0023 0.0223 0.0056 0.0195 SVM rbf - Bbasis 0.0025 0.0232 0.0053 0.0180 SVM linear - projection 0.0309 0.0420 0.0101 0.0251 SVM poly - projection 0.0354 0.0577 0.0158 0.0397 SVM rbf - projection 0.1415 0.1897 0.0327 0.0537 SVM linear - RKHS - radial 0.0015 0.0223 0.0030 0.0157 SVM poly - RKHS - radial 0.0028 0.0229 0.0037 0.0173 SVM rbf - RKHS - radial 0.0039 0.0212 0.0042 0.0157 SVM linear - RKHS - poly 0.0133 0.0442 0.0066 0.0217 SVM poly - RKHS - poly 0.0085 0.0502 0.0106 0.0251 SVM rbf - RKHS - poly 0.0125 0.0545 0.0101 0.0223 SVM linear - RKHS - linear 0.0077 0.0458 0.0100 0.0207 SVM poly - RKHS - linear 0.0049 0.0388 0.0069 0.0242 SVM rbf - RKHS - linear 0.0077 0.0429 0.0085 0.0252 The table above presents all computed characteristics. It also includes standard deviations to compare the stability or variability of the individual methods. We can also formally test whether some methods are more successful than others. Due to the violation of the normality assumption, we cannot use the classic paired t-test. Instead, we will use its non-parametric alternative - the Wilcoxon test. Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;RF_discr&#39;], alternative = &#39;less&#39;, paired = T)$p.value ## [1] 0.01762011 Code wilcox.test(SIMULACE$test[, &#39;RF_Bbasis&#39;], SIMULACE$test[, &#39;SVM linear - diskr&#39;], alternative = &#39;t&#39;, paired = T)$p.value ## [1] 0.01250668 Finally, we can graphically display the computed values from the simulation for individual classification methods using box plots, separately for testing and training errors. Code # for training data SIMULACE$train |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = 0.3)) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = expression(widehat(Err)[train])) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 40, hjust = 1)) + geom_jitter(position = position_jitter(0.15), alpha = 0.7, size = 1, pch = 21, colour = &#39;black&#39;) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = &#39;+&#39;, size = 4, color = &quot;black&quot;, alpha = 0.9)+ geom_hline(yintercept = min(SIMULACE.df$Err.train), linetype = &#39;dashed&#39;, colour = &#39;grey&#39;) Figure 6.25: Box plots of training errors for 100 simulations separately for each classification method. The black symbols \\(+\\) denote the means. Code # for testing data SIMULACE$test |&gt; pivot_longer(cols = methods, names_to = &#39;method&#39;, values_to = &#39;Err&#39;) |&gt; mutate(method = factor(method, levels = methods, labels = methods, ordered = TRUE)) |&gt; as.data.frame() |&gt; filter(method %in% sub_methods) |&gt; ggplot(aes(x = method, y = Err, fill = method, colour = method, alpha = method)) + geom_jitter(position = position_jitter(height = 0, width = 0.15), alpha = 0.6, size = 0.9, pch = 21, colour = &quot;black&quot;) + geom_boxplot(outlier.colour = &quot;white&quot;, outlier.shape = 16, outlier.size = 0, notch = FALSE, colour = &#39;black&#39;) + theme_bw() + labs(x = &#39;Classification Method&#39;, y = &quot;$\\\\widehat{\\\\textnormal{Err}}_{test}$&quot; # y = expression(widehat(Err)[train]) ) + theme(legend.position = &#39;none&#39;, axis.text.x = element_text(angle = 45, hjust = 1)) + stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, shape = 4, size = 2, color = &quot;black&quot;, alpha = 1) + scale_x_discrete(labels = methods_names1) + theme(plot.margin = unit(c(0.1, 0.1, 0.7, 0.5), &quot;cm&quot;)) + coord_cartesian(ylim = c(0, 0.15)) + scale_fill_manual(values = box_col1) + scale_alpha_manual(values = box_alpha1) + geom_hline(yintercept = min(SIMULACE.df$Err.test), linetype = &#39;dashed&#39;, colour = &#39;grey20&#39;, alpha = 0.8) Figure 6.26: Box plots of testing errors for 100 simulations separately for each classification method. The black symbols \\(+\\) denote the means. Code # ggsave(&quot;figures/results_tec_der.tex&quot;, device = tikz, width = 7, height = 5) # ggsave(&quot;figures/kap7_tecator_box_test_der_subset.tex&quot;, device = tikz, width = 6.5, height = 5) Finally, let’s take a look at which hyperparameter values were the most commonly chosen. Table 5.16: Medians of hyperparameter values for selected methods where a hyperparameter was determined using cross-validation. Median hyperparameter value KNN_K 4.0 nharm 2.0 LR_func_n_basis 8.0 SVM_d_Linear 6.0 SVM_d_Poly 6.0 SVM_d_Radial 6.0 SVM_RKHS_radial_gamma1 0.5 SVM_RKHS_radial_gamma2 0.3 SVM_RKHS_radial_gamma3 0.3 SVM_RKHS_radial_d1 14.0 SVM_RKHS_radial_d2 12.0 SVM_RKHS_radial_d3 8.0 SVM_RKHS_poly_p1 4.0 SVM_RKHS_poly_p2 4.0 SVM_RKHS_poly_p3 4.0 SVM_RKHS_poly_d1 6.0 SVM_RKHS_poly_d2 5.0 SVM_RKHS_poly_d3 4.0 SVM_RKHS_linear_d1 21.0 SVM_RKHS_linear_d2 21.0 SVM_RKHS_linear_d3 25.0 Code CV_res &lt;- CV_RESULTS |&gt; pivot_longer(cols = CV_RESULTS |&gt; colnames(), names_to = &#39;method&#39;, values_to = &#39;hyperparameter&#39;) |&gt; mutate(method = factor(method, levels = CV_RESULTS |&gt; colnames(), labels = CV_RESULTS |&gt; colnames(), ordered = TRUE)) |&gt; as.data.frame() CV_res |&gt; filter(method %in% c(&#39;KNN_K&#39;, &#39;nharm&#39;, &#39;LR_func_n_basis&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.41: Histograms of hyperparameter values for KNN, functional logistic regression, and also a histogram for the number of principal components. Code CV_res |&gt; filter(method %in% c(&#39;SVM_d_Linear&#39;, &#39;SVM_d_Poly&#39;, &#39;SVM_d_Radial&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_grid(~method, scales = &#39;free&#39;) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.27: Histograms of hyperparameter values for SVM with projection onto B-spline basis. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_radial_gamma1&#39;, &#39;SVM_RKHS_radial_gamma2&#39;, &#39;SVM_RKHS_radial_gamma3&#39;, &#39;SVM_RKHS_radial_d1&#39;, &#39;SVM_RKHS_radial_d2&#39;, &#39;SVM_RKHS_radial_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(bins = 10, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.42: Histograms of hyperparameter values for RKHS + SVM with radial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_poly_p1&#39;, &#39;SVM_RKHS_poly_p2&#39;, &#39;SVM_RKHS_poly_p3&#39;, &#39;SVM_RKHS_poly_d1&#39;, &#39;SVM_RKHS_poly_d2&#39;, &#39;SVM_RKHS_poly_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 1, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 6.28: Histograms of hyperparameter values for RKHS + SVM with polynomial kernel. Code CV_res |&gt; filter(method %in% c(&#39;SVM_RKHS_linear_d1&#39;, &#39;SVM_RKHS_linear_d2&#39;, &#39;SVM_RKHS_linear_d3&#39;)) |&gt; ggplot(aes(x = hyperparameter, #y = after_stat(density), fill = method, colour = method)) + geom_histogram(binwidth = 5, alpha = 0.6) + theme_bw() + facet_wrap(~method, scales = &#39;free&#39;, ncol = 3) + labs(x = &#39;Hyperparameter values&#39;, y = &#39;Absolute count&#39;) + theme(legend.position = &#39;none&#39;) Figure 5.43: Histograms of hyperparameter values for RKHS + SVM with linear kernel. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
